![kafka_logo](C:\Users\Administrator\Desktop\번역1\kafka_logo.png)  Apache Kafka(아파치 카프카)
=======
##### 높은 처리량을 분산한 메세징 시스템.

## Kafka 0.8.1 문서

현재 버전: [0.8.2-베타](http://kafka.apache.org/082/documentation.html) 이전 버전: [0.7.x,](http://kafka.apache.org/07/documentation.html)[0.8.0.](http://kafka.apache.org/08/documentation.html)

* [1.시작하기](http://kafka.apache.org/08/documentation.html#gettingStarted)
 * [1.1 소개](http://kafka.apache.org/08/documentation.html#introduction)
 * [1.2 사용 사례](http://kafka.apache.org/08/documentation.html#uses)
 * [1.3 빠른 시작](http://kafka.apache.org/documentation.html#quickstart)
 * [1.4 에코시스템](http://kafka.apache.org/documentation.html#ecosystem)
 * [1.5 업그레이드](http://kafka.apache.org/documentation.html#upgrade)
* [2.API](http://kafka.apache.org/documentation.html#api)
 * [2.1 프로듀서 API](http://kafka.apache.org/documentation.html#producerapi)
 * [2.2 고수준 컨슈머 API](http://kafka.apache.org/documentation.html#highlevelconsumerapi)
 * [2.3 단순 컨슈머 API](http://kafka.apache.org/documentation.html#simpleconsumerapi)
 * [2.4 카프카 하둡 컨슈머 API](http://kafka.apache.org/documentation.html#kafkahadoopconsumerapi)
* [3.환경설정](http://kafka.apache.org/documentation.html#configuration)
 * [3.1 브로커 설정](http://kafka.apache.org/documentation.html#brokerconfigs)
 * [3.2 컨슈머 설정](http://kafka.apache.org/documentation.html#consumerconfigs)
 * [3.3 프로듀서 설정](http://kafka.apache.org/documentation.html#producerconfigs)
 * [3.4 새 프로듀서 설정](http://kafka.apache.org/documentation.html#newproducerconfigs)
* [4.디자인](http://kafka.apache.org/documentation.html#design)
 * [4.1 동기부여](http://kafka.apache.org/documentation.html#majordesignelements)
 * [4.2 고집](http://kafka.apache.org/documentation.html#persistence)
 * [4.3 능률](http://kafka.apache.org/documentation.html#maximizingefficiency)
 * [4.4 프로듀서](http://kafka.apache.org/documentation.html#theproducer)
 * [4.5 컨슈머](http://kafka.apache.org/documentation.html#theconsumer)
 * [4.6 메세지 전달의 의미](http://kafka.apache.org/documentation.html#semantics)
 * [4.7 복제](http://kafka.apache.org/documentation.html#replication)
 * [4.8 로그 다짐](http://kafka.apache.org/documentation.html#compaction)
* [5.구현](http://kafka.apache.org/documentation.html#implementation)
 * [5.1 API 디자인](http://kafka.apache.org/documentation.html#apidesign)
 * [5.2 네트워크 계층](http://kafka.apache.org/documentation.html#networklayer)
 * [5.3 메시지](http://kafka.apache.org/documentation.html#messages)
 * [5.4 메세지 형식](http://kafka.apache.org/documentation.html#messageformat)
 * *[5.5 로그](http://kafka.apache.org/documentation.html#log)
 * [5.6 분산](http://kafka.apache.org/documentation.html#distributionimpl)
* [6.작동법](http://kafka.apache.org/documentation.html#operations)
 * [6.1 기본 카프카 작동법](http://kafka.apache.org/documentation.html#basic_ops)
   + [토픽 추가와 제거](http://kafka.apache.org/documentation.html#basic_ops_add_topic)
   + [토픽 수정](http://kafka.apache.org/documentation.html#basic_ops_modify_topic)
   + [정상종료](http://kafka.apache.org/documentation.html#basic_ops_restarting)
   + [균형의 리더십](http://kafka.apache.org/documentation.html#basic_ops_leader_balancing)
   + [컨슈머 위치 확인](http://kafka.apache.org/documentation.html#basic_ops_consumer_lag)
   + [클러스터 간의 데이터 미러링](http://kafka.apache.org/documentation.html#basic_ops_mirror_maker)
   + [클러스터 확장](http://kafka.apache.org/documentation.html#basic_ops_cluster_expansion)
   + [해체 브로커](http://kafka.apache.org/documentation.html#basic_ops_decommissioning_brokers)
   + [증가 복제 인자](http://kafka.apache.org/documentation.html#basic_ops_increase_replication_factor)
 * [6.2 데이타 센터](http://kafka.apache.org/documentation.html#datacenters)
 * [6.3 중요한 CONFIGS](http://kafka.apache.org/documentation.html#config)
   + [중요한 서버 CONFIGS](http://kafka.apache.org/documentation.html#serverconfig)
   + [중요한 클라이언트 CONFIGS](http://kafka.apache.org/documentation.html#clientconfig)
   + [운영서버 CONFIGS](http://kafka.apache.org/documentation.html#prodconfig)
 * [6.4 자바 버전](http://kafka.apache.org/documentation.html#java)
 * [6.5 하드웨어 및 OS](http://kafka.apache.org/documentation.html#hwandos)
    + [OS](http://kafka.apache.org/documentation.html#os)
    + [디스크 및 파일시스템](http://kafka.apache.org/documentation.html#diskandfs)
    + [응용프로그램 vs OS 세척 관리](http://kafka.apache.org/documentation.html#appvsosflush)
    + [리눅스 세척 행동](http://kafka.apache.org/documentation.html#linuxflush)
    + [Ext4 노트](http://kafka.apache.org/documentation.html#ext4)
 * [6.6 모니터링](http://kafka.apache.org/documentation.html#monitoring)
 * [6.7 주키퍼](http://kafka.apache.org/documentation.html#zk)
    + [안정버전](http://kafka.apache.org/documentation.html#zkversion)
    + [Operationalization](http://kafka.apache.org/documentation.html#zkops)

### 1.시작하기

#### 1.1 소개

카프카가 분산,분할,커밋 로그 서비스를 복제합니다. 이것은 메세징 시스템의 기능을 독특한 디자인으로 제공합니다.  

모두 무엇을 의미하는가?  

먼저, 몇 개의 기본적인 메세징 용어를 살펴보자:  

 - 카프카는 메세지의 피드를 _토픽_이라고 불리는 범주에 유지합니다.
 - 우리는 카프카 토픽 _프로듀서_에게 매세지를 게시하는 프로세스를 부를 것입니다.
 - 우리는 토픽에 등록하고 발행된 메세지 _컨슈머_의 공급을 처리하는 프로세르를 부를 것입니다.
 - 카프카는 _브로커_라고 불리는 각각 하나 이상의 서버로 구성된 클러스터로 실행합니다.  

그래서,고수준에서, 프로듀서들은 메세지들을 네트워크를 통해 결과적으로 컨슈머에게 그들을 제공하는 카프카 클러스터에게 보냅니다. 이와 같이:  

![producer_consumer.png](C:\Users\Administrator\Desktop\번역1\producer_consumer.png)  

클라이언트와 서버 간의 소통은 단순하고,고성능,언어 불가지론 [TCP 프로토콜](https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol)이다. 우리는 카프카를 위해 자바 클라이언트를 제공하지만, 클라이언트들은 [많은 언어](https://cwiki.apache.org/confluence/display/KAFKA/Clients)들을 이용가능하다.  

##### 토픽 및 로그

고수준의 추상적 개념 카프카가 제공하는 걸로 먼저 다이브-토픽.(Let's first dive into the high-level abstraction Kafka provides-the topic.)  
토픽은 카테고리 또는 메세지가 게시되는 피드 이름입니다. 각 토픽을 위해서, 카프카 클러스터는 분할 된 로그를 다음과 같이 유지:  

![log_anatomy.png](C:\Users\Administrator\Desktop\번역1\log_anatomy.png)

각 파티션은 지속적으로 커밋 로그에 추가하는 메시지의 불변의 순서를 정렬합니다. 파티션 안에 있는 메세지는 ,할당 된 각 순차 ID 번호고 고유 파티션 내에서 각 메세지를 식별하는 _오프셋_ 이라고 불립니다.

카프카 클러스터는 모든 발행 된 메세지를 그들이 소비되었는지 아닌지에 따라 구성 가능한 시간 기간동안 보유합니다. 예를 들어, 로그보존이 이틀로 설정된다면, 다음 이틀 동안 메세지가 게시 된 후 소비 할 수 있고, 그 후에는 공간을 확보하기 위해 삭제됩니다. 카프카의 수행은 능률적으로 데이터 크기에 대하여 일정해서 많은 양의 데이터를 보유하는 것에 문제가 없습니다.  

실제로 당 컨슈머 기준으로 유지하는 유일한 메타 데이터는 로그에 컨슈머의 위치를 "오프셋"으로 부릅니다. 컨슈머에 의해 제어된 오프셋: 보통 컨슈머는 메세지를 읽을 때 선형 오프셋을 진행시키지만, 실제로 위치는 컨슈머에 의해 제어되고 그것이 원하는 임의의 순서로 메세지를 사용 할 수 있습니다. 예를 들어 컨슈머는 재처리 이전 오프셋을 재설정 할 수 있습니다.  

이 기능들의 결합은 카프카 소비자들이 매우 저렴하다는 의미입니다. 즉, 그들은 클러스터나 다른 컨슈머들에 대한 큰 영향없이 드나들 수 있습니다. 예를 들어, 너는 우리의 명령 줄 도구들을 기존에 있는 어느 컨슈머들에 의해 소비되는 어떤 것의 변화없이 어떤 토픽의 내용인 "꼬리" 에 사용 할 수 있습니다.  

로그에 있는 파티션들은 몇몇의 목적들을 제공합니다. 첫째로, 그들은 로그를 하나의 서버에 딱 맞는 크기 이상으로 조정하도록 허락합니다. 각각의 개별 파티션은 이것을 진행하는 서버들에 적합해야 하지만, 토픽은 임의의 양의 데이터를 조절할 수 있어서 많은 파티션들을 가질 수 있습니다. 두 번째는 그들은 더 많은 비트에 대한 병행의 단위로써 행동합니다.  

##### 분산  

로그의 파티션들은 파티션의 공유요청과 데이터를 처리하는 각각의 서버와 카프카 클러스터 안에 있는 서버들을 걸쳐 분산합니다. 각 파티션은 고장 허용 범위를 위해 설정가능한 서버의 수를 걸쳐서 복제합니다.  

각각의 파티션은 "리더" 로써 일을 하는 하나의 서버와 "팔로워" 로써 일을 하는 제로 또는 더 많은 서버들을 가집니다. 리더는 팔로워들이 수동적으로 리더를 복사할 동안 파티션을 위한 모든 읽고 쓰는 요청들을 다룹니다. 만약 리더가 실패했을 때, 팔로워들 중 하나는 자동으로 새로운 리더가 될 수 있습니다. 각 서버는 리더로써 몇몇의 파티션들을 위해서 그리고 팔로워는 다른 것들을 위해서 수행해서 로드는 클러스터 내에 잘 균형 잡혀있습니다.  

##### 프로듀서

프로듀서들은 그들이 선택한 토픽에 데이터를 게시합니다. 프로듀서는  토픽내에서 어떤 파티션에 어떤 메세지를 지정할 지에 대한 선택에  책임이 있습니다. 이것은 단순히 로드 균형을 잡기 위해서 라운드-로빈 방식으로 수행 되거나 이것은 어떤 의미의 파티션 기능(메세지에서 어떤 키에 기초해서 말하는)에 따라 수행 될 수 있습니다. 두 번째에서 파티션의 사용에 대해서 더 자세히 있습니다.

##### 컨슈머

메세징은 전통적으로 두가지 모델이 있다: [대기행렬](http://en.wikipedia.org/wiki/Message_queue)과 [발행-구독](http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern). 대기행렬에서, 컨슈머의 풀은 서버로부터 읽을 수 있고, 각 메세지는 그들 중 하나에게 갑니다; 발행-구독에서는 메세지는 모든 컨슈머들에게 보도됩니다. 카프카는 이 소비자 그룹을 동시에 일반화시킨 한 컨슈머 추상화를 제공합니다.  

컨슈머는 스스로를 컨슈머 그룹 이름으로 라벨을 매기고, 토픽에 게시한 각각 메세지는 각 구독 컨슈머 그룹내에서 즉시 한 컨슈머에게 전달됩니다. 컨슈머 인스터스는 별도의 프로세스 또는 별도의 기계에 있을 수 있습니다.  

모든 컨슈머 인스턴스가 같은 컨슈머 그룹이라면, 이것은 컨슈머 이상 으로 전통적인 큐 밸런싱 로드처럼 작동합니다.  

모든 컨슈머 인스턴스가 다른 컨슈머 그룹이라면, 이것은 발행-구독처럼 작동하고 모든 메세지들은 모든 컨슈머들에게 보도됩니다.  

그러나,흔히,우리는 토픽들이 각각의 "논리적 구독자"에 대해서 적은 수의 컨슈머 그룹들을 가지고 있는 것을 발견할 수 있습니다. 각 그룹은 확장성과 내고장성에 대해 많은 컨슈머 인스턴스로 구성됩니다. 구독은 한 프로세스 대신 컨슈머의 클러스터인 곳에서 발행-구독의 의미가 다가 아닙니다.  

카프카도 전통적인 메세징 시스템보다 더 강한 순서 보장이 있습니다.  

전통적인 큐를 보유한 메세지-서버에 주문하고, 여러 컨슈머가 큐로 부터 소비하고 나서 서버가 그들이 저장한 순서로 메세지들을 나눠줍니다. 그러나, 비록 서버가 순서대로 메세지들을 나눠줄지라도, 메세지들은 컨슈머에게 비동기적으로 전달될 수 있습니다, 그래서 그들은 다른 컨슈머에게 순서에 맞지 않게 도달할 수 있습니다. 이것은 병렬 소비의 존재하에서 효과적으로 메세지의 순서가 틀렸다는 것을 의미합니다. 메세징 시스템은 자주 이것을 한 프로세스가 큐로 부터 소비하하도록 허락하는 "독점적인 컨슈머" 개념을 가짐으로써 이것을 해결합니다,하지만 물론 이것은 프로세스의 병행이 없다는 것을 의미합니다.  

![consumer-groups.png](C:\Users\Administrator\Desktop\번역1\consumer-groups.png)

카프카는 더 잘합니다. 토픽내에서 병렬 파티션의 개념을 가짐으로써, 카프카는 컨슈머 프로세스들의 풀 이상으로 순서보장과 로드 균형을 제공할 수 있습니다. 이것은 토픽안에서 컨슈머 그룹에 있는 컨슈머들에게 파티션을 지정함으로써 성취합니다.그래서 각 파티션은 정확히 그룹에 있는 한 컨슈머에 의해서 소비됩니다. 이렇게함으로써 우리는  컨슈머는 유일한 파티션의 리더고 순서대로 데이터를 소비하는 것을 증명합니다. 많은 파티션들이 있기 때문에 이것은 여전히 많은 컨슈머  인스턴스를 통해 로드를 균형 맞춥니다. 그러나 파티션 보다 더 많은 컨슈머 인스턴스는 없다는 것을 주목해야 합니다.  

카프카는 토픽안에 다른 파티션들 사이가 아닌 같은 파티션내에서 메세지를 통해 전체 순서를 제공합니다. 각 파티션 키에서 데이터를 분할하는 기능과 결합된 순서는 대부분의 어플리케이션에 충분합니다. 그러나,너가 메세지를 통해 전체 순서를 필요로 한다면 이것은 단지 하나의 컨슈머 프로세스를 의미할 것이지만 이것은 하나의 파티션이 있는 토픽으로 달성 할 수 있습니다.  

##### 보증

높은 수준에서 카프카는 다음과 같은 보장을 제공:

* 특정 토픽 파티션에 프로듀서가 보낸 메세지는 그들이 보낸 순서대로 첨부됩니다. 즉,메세지 M1은 M2 메세지와 같은 프로듀서에 의해 전송된 경우,M1이 먼저 전송되면 M1은 M2보다 더 낮은 오프셋을 가지고 로그에서 더 일찍 나타납니다.
* 컨슈머 인스턴스는 그들이 로그에 저장한 순서대로 메세지를 봅니다.
* 복제 계수 N이 있는 토픽을 들어, 우리는 로그에 커밋하는 메세지들을 잃지않고 N-1 서버 장애까지 허용할 수 있습니다.

이 보증에 대한 자세한 내용은 문서의 디자인 섹션에서 나옵니다.  

####1.2 사용사례

여기에서 아파치 카프카에 대한 인기있는 사용사례 몇개를 설명합니다. 기능에 대한 개요에 대해선 이 [블로그 게시물](For an overview of a number of these areas in action, see this blog post. )을 참조하세요.

##### 메세징

카프카는 더 전통적인 메세지 브로커에 대한 대체로 잘 작동합니다. 메세지 브로커들은 다양한 이유로 사용됩니다.(데이터 프로듀서로부터 프로세싱 분리,처리되지 않은 메시지를 버퍼에 저장,등등) 대부분의 메세징 시스템에 비해 카프카는 처리량,파티션닝 건설,복제,그리고 큰 규모의 메세지 프로세싱 응용프로그램을 위한 좋은 솔루션인 장애허용이 더 발달했습니다.  

우리의 경험에서 메세징 사용들이 종종 비교적 낮은 처리량이지만,낮은 엔드투엔드 지연을 요구할 수 있고 종종 카프카가 제공하는 강한 내구성 보증들에 의존합니다.  

이 도메인에서 카프카는 [ActiveMQ](In this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ. ) 또는 [RabbitMQ](https://www.rabbitmq.com/)와 같은 전통적인 메세징 시스템들과 맞먹습니다.

##### 웹사이트 활동 추적

카프카의 오리지널 사용사례는 실시간 publish-subscribe 피드의 세트로 유저 활동 추적 파이프라인을 재설정할 수 있습니다. 이것은 사이트 활동(페이지 뷰,검색,또는 다른 유저가 쓰는 활동들)이 활동유형 당  하나의 토픽을 중심 토픽에 게시하는것을 의미합니다. 이런 피드는 실시간 처리,실시간 모니터링,그리고 하둡 또는 오프라인 처리와 기록을 위한 오프라인 데이터 웨어하우싱 시스템으로 로딩하는 것들을 포함한 사용사례의 범위를 구독할 수 있습니다.  

활동 추적은 종종 많은 활동 메세지들을 각각의 유저 페이지 뷰에 발생시키는 매우 높은 볼륨입니다.

##### 통계

카프카는 종종 작동 모니터링 데이터를 위해 사용됩니다. 이것은 운영 데이터의 중앙 피드를 생성하는 분산 응용 프로그램에서 집계 통계가 포합됩니다.

##### 로그 집합

많은 사람들은 로그 통합 솔루션을 위해 대체로 카프카를 사용합니다. 로그 집합은 전형적으로 서버를 실제 로그 파일을 모으고 처리를 위해 그것들을 중심지(아마도 파일 서버 또는 HDFS)에 넣습니다. 카프카는 파일의 세부사항을 다르게 끌어내고 메세지의 스트림으로 이벤트 데이터나 로그의 클리너 추상화를 제공합니다. 이것은 낮은 지연처리와 다양한 데이터 소스들을 위해 더 쉽게 지원하고 데이터 소비를 분산하는 것을 허용합니다. 스크라이브(Scribe) 또는  플럼(Flume)과 같은 로그 중심의 시스템에 비해,카프카는 동등하게 좋은 성능,복제에 기인하는 더 강한 내구성 보장,그리고 더 낮은 엔드투엔드 지연을 제공합니다.

##### 스트림 프로세싱

많은 사용자는 데이터가 원시 데이터 토픽에서 소비된 후 집계,농축,또는 그렇지 않으면 더 소비를 위한 새로운 카프카 토픽에 변환된 데이터의 단계별 처리를 끝내버립니다. 예를 들어, 기사 추천에 대한 처리흐름은 RSS 피드에서 기사항목을 크롤하고 "기사" 토픽에 이것을 게시합니다; 추가의 프로세싱은 정규화시키는 것을 도와주거나 이 항목을 cleaned 기사 항목의 토픽에 중복을 제거할 수 있습니다; 마지막 단계는 유저에게 이 내용을 일치하도록 시도할 수 있습니다. 이것은 개별 토픽중 실시간 데이터 플로우의 그래프를 생성합니다.[스톰(Storm)](https://github.com/nathanmarz/storm)과 [삼자(Samza)](http://samza.apache.org/)는 변환의 이러한 종류의 구현을 위한 인기있는 프레임워크입니다.

##### 이벤트 소싱

[이벤트 소싱](http://martinfowler.com/eaaDev/EventSourcing.html)은 상태 변경이 기록의 순서대로 로그되는 어플리케이션 디자인의 스타일입니다. 매우 큰 저장된 로그 데이터를 위한 카프카의 지원은 이 스타일에 내장된 어플리케이션을 위해 이것을 훌륭한 백엔드로 만듭니다.

##### 커밋 로그

카프카는 분산 시스템에 대한 외부의 커밋로그의 일종으로 역할을 할 수 있습니다. 로그는 실패노드가 자신의 데이터를 복원하는 재동기 메커니즘으로 노드와 행동 사이의 데이터를 복제하는 것을 도와줍니다. 카프카에서 [로그 압축](http://kafka.apache.org/documentation.html#compaction)기능은이 사용을 지원하는데 도와줍니다. 이 사용에서 카프카는 [아파치 부기계](http://zookeeper.apache.org/bookkeeper/) 프로젝트와 비슷합니다.

#### 1.3 빠른 시작

이 튜토리얼은 당신이 새롭게 시작하고 기존의 카프카나 주키퍼 데이터에 존재하지 않는 것으로 가정합니다.

##### 1단계: 코드를 다운로드

0.8.1.1 버전및 un-tar를 [다운로드](https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz)

	> tar-xzf kafka_2.9.2-0.8.1.1.tgz
	> cd kafka_2.9.2-0.8.1.1

##### 2단계: 서버를 시작

카프카는 주키퍼를 사용해서 만약에 너가 이미 그것을 가지고 있지 않으면 너는 먼저 주키퍼 서버를 시작해야합니다. 너는 빠르고 간편한 단일노드 주키퍼 인스턴스를 얻기위해서 카프카와 함께 편리한 스크립트 패키지를 사용할 수 있습니다.

	> bin/zookeeper-server-start.sh config/zookeeper.properties
	[2013-04-22 15:01:37,495] INFO Reading configurtion from: config/zookeeper.properties(org.apache.zookeeper.serer.quorum.QuorumPeerConfig)

이제 카프카 서버를 시작:

	> bin/kafka-server-start.sh config/server.properties
	[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
    [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
    ...

##### 3단계: 토픽을 생성

하나의 파티션과 하나의 복제로 "테스트"라는 이름의 토픽을 만들어라:

	> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

만약에 우리가 토픽 명력 목록을 실행하면 우리는 이제 그 토픽을 볼 수 있습니다:

	> bin/kafka-topics.sh --list --zookeeper localhost: 2181
	test

대안적으로,존재하지 않는 토픽을 게시할때 수공으로 토픽을 만드는 것 대신에 너는 또한 너의 브로커들을 토픽 자동생성에 설정할 수 있습니다.

##### 4단계: 몇몇의 메세지들을 전송

카프카는 파일 또는 표준입력에서 입력을 하고 이것을 카프카 클러스터에 메세지로 발송하는 명령 행 클라이언트가 함께 제공됩니다. 자동적으로 각 라인은 별도의 메세지로 보내질 것입니다.  

프로듀서를 실행한 다음에 서버에 보내기 위한 몇 가지 메세지를 콘솔에 입력합니다.

	> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
	This is a message
    This is another message

##### 5단계: 컨슈머를 시작

카프카는 또한 표준출력에 메세지를 덤프아웃하는 명령 행 컨슈머가 있습니다.

	> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
	This is a message
    This is another message

만약 다른 터미널에서 각각의 위의 명령어를 실행한경우 너는 지금 메세지를 프로듀서 터미널에 입력하고 그들은 컨슈머 터미널에서 나타나는 것을 볼 수 있어야합니다.  

명령 행 도구의 모든 것은 추가적인 옵션이 있습니다; 인수없이 명령을 실행하는 것은 더 자세하게 그들을 기록하는 사용정보를 보여줍니다.

#####  6단계: 다수의 브로커 클러스터 설정

지금까지 우리는 하나의 브로커에 대해 실행했습니다,하지만 그건 재미가 없습니다. 카파카에서,하나의 브로커는 단지 크기 하나의 클러스터입니다,그래서 더 많은 몇 가지 브로커 인스턴스 시작 외에 큰 변화가 없습니다. 하지만 바로 그것을 느낄수 있습니다, 우리의 클러스터를 세 개의 노드로 확장하자.(여전히 우리의 로컬 기계에서)  

먼저 우리는 각각의 브로커들을 위한 설정파일을 만듭니다:

	> cp config/server.properties config/server-1.properties
	> cp config/server.properties config/server-2.properties

이제 이 새로운 파일들을 편집하고 다음에 따르는 속성을 설정합니다:

	config/server-1.properties:
    broker.id=1
    port=9093
    log.dir=/tmp/kafka-logs-1

	config/server-2.properties:
    broker.id=2
    port=9094
    log.dir=/tmp/kafka-logs-2

brkoer.id 특성은 클러스터안에 있는 각각의 노드들의 독특하고 영구적인 이름입니다. 우리는 포트를 중단시키고 우리는 이것들을 같은 기계에서 실행해야 하기때문에 디렉토리를 로그해야하고 우리는 같은 포트에 기록하거나 각각 다른 데이터를 덮어쓰기를 시도하는 브로커들을 보호하기를 원합니다.  

우리는 이미 주키퍼를 가지고 있고 우리의 단일 노트를 시작합니다,그래서 우리는 단지 두 개의 새로운 노드가 필요합니다:

	> bin/kafka-server-start.sh config/server-1.properties &
	...
    > bin/kafka-server-start.sh config/server-2.properties &
    ...

이제 세 개의 복제 인자로 새로운 토픽을 생성합니다:

	> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my replicate-topic

좋아,지금 우리는 클러스터를 가지고 있다 하지만 우리가 지금 어느 브로커가 무엇을 하는지 우리는 어떻게 알까? 그것을 보여주기 위해서 "describe 토픽" 명령을 실행시킨다:

	> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
	Topic:my-replicated-topic      PartitionCount:1    ReplicationFactor:3  Configs:
           Topic: my-replicated-topic        Partition: 0   Leader: 1      Replicas:  1,2,0  Isr: 1,2,0

출력에 대한 설명이 여기있다. 첫 번째 줄은 모든 파티션의 요약을 제공하고, 각각의 추가 라인은 하나의 파티션에 대한 정보를 제공합니다. 우리가 이 토픽에 대한 하나의 파티션을 가지고 있기 때문에 하나의 라인이 있습니다. 

* "리더"는 주어진 파티션에 대한 모든 읽고 쓰는 책임이 있는 노드이다. 각 노드는 파티션의 무작위로 선택된 부분에 대한 리더가 될 것입니다.
* "복제" 그들이 리더인지 또는 심지어 그들이 현재 살고있는지 여부에 관계없이 이 파티션의 로그 복제 노드의 목록입니다. 
* "ISR"은 "동기화" 복제본의 집합입니다. 이것은 현재 살아있고 리더에게 캐치업하는 복제본 목록의 부분집합입니다.

예에서 노드1은 토픽의 유일한 파티션의 리더입니다.  

우리는 우리가 어디에 그것이 있는지 보기위해 만든 원래의 토픽에 같은 명령을 실행할 수 있습니다:

	> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
	Topic:test      PartitionCount:1        ReplicationFactor:1     Configs:
          TopicL test      Partition: 0     Leader: 0     Replicas: 0       Isr: 0

그래서 저기에 놀라운 일이 없다- 원래의 토픽은 복제본이 없고 우리가 이것을 만들때 우리의 클러스터 안에 있는 유일한 서버인 서버 0에 있습니다.  

우리의 새로운 토픽에 몇 가지의 메세지를 게시하자:

	> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
	...
    my test message 1
    my test message 2
    ^C

이제 이 메세지를 사용하자:

	> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
	...
    my test message 1
    my test message 2
    ^C

장애허용능력을 테스트하자. 브로커 1은 리더로 행동합니다.그래서 이것을 죽이겠다:

	> ps | grep server -1.properties
 	7564 ttys002    0:15.91   /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java ...
     > kill -9 7564

리더쉽은 노예의 하나로 전환했고 노드 1은 동기화 복제 세트에 더 이상 있지 않는다:

	> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
	Topic:my-replicated-topic      PartitionCount:1    ReplicationFactor:3   Configs:
       Topic: my-replicated-topic     Partition: 0     Leaer: 2  Replicas:1,2,0  Isr:2,0

그러나  원래 쓰기를 했던 리더가 다운됬을지라도 메세지는 여전히 소비가 가능합니다.

	> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-begining --topic my-replicated-topic
	...
    my test message 1
    my test message 2
    ^C

#### 1.4 에코시스템

외부 카프카와 메인 분산을 통합 도구의 과잉이 있습니다. [에코시스템 페이지](https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem)는 스트림 처리 시스템,하둡 통합,모니터링,그리고 배치도구를 포함하는 이들 중 많은 것을 나열합니다.

#### 1.5 이전 버전으로 부터의 업그레이드

##### 0.8.0에서 부터 0.8.1까지 업그레이드

0.8.1은 0.8과 완전히 호환됩니다. 업그레이드는 단순히 그것을 중단시키고,코드를 업그레이드하고,그리고 이것을 재시작함으로써 한번에 한 브로커를 수행할 수 있습니다.

##### 0.7에서부터 업그레이드

0.8,복제본을 추가한 버전은,우리의 이전버전들과 호환되지 않는 첫번째 버전입니다: 주된 변화는 API,주키퍼 데이터 구조,그리고 프로토콜 및 환경설정에서 만들어졌습니다. 0.7에서 0.8.x까지의 업그레이드는 이송을 위한 [특별한 도구](https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8)가 필요합니다. 이 이송은 컴퓨터가 작동하지 않는 시간없이 수행됩니다.

### 2. API

#### 2.1 프로듀서 API

	/**
     *  V: 메세지의 타입
     *  K: 메세지와 연관된 옵션 키의 타입
    */
    class kafka.javaapi.producer.Producer<K,V>  {
      public Producer(ProducerConfig config);

    /**
     * 하나의 토픽에 데이터를 전송,키에 의한 분할,둘 중 하나 사용
     * 동기 또는 비동기 프로듀서
     * @param 메세지는 토픽,키 그리고 메세지 데이터를 요약한 프로듀서 데이터 객체 
     */
    public void send(KeyedMessage<K,V> message);

    /**
     * 이 API를 다수의 토픽에 데이터를 보내기 위해 사용
     * @param 메세지들은 토픽,키 그리고 메세지 데이터를 요약한 프로듀서 데이터 객체의 목록
     */
    public void send(List<KeyedMessage<K,V>> messages);

    /**
     * API 닫기는 모든 카프카 브로커에 프로듀서 풀(pool) 연결을 닫습니다
     * /
     public void close(); 
    }

너는 프로듀서 API를 사용하는 방법을 배울 수 있는 이 예를 따를 수 있습니다.

#### 2.2 고수준 컨슈머 API

    class Consumer  {
       /**
        * 컨슈머커넥터를 생성
        * 
        * @param 설정은 최소한으로,컨슈머와 주키퍼의 그룹피아이디를  명시해야한다.
        *  문자열 주키퍼.커넥트 연결
        */
    public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(ConsumerConfig config); 
    }

    /**
     *  V: 메세지의 타입
     *  K: 메세지와 연관된 옵션 키의 타입
     */
    public interface kafka.javaapi.consumer.ConsumerConnector {
    /**
     * 각 토픽에 대한 타입 T의 메세지 스트림의 목록을 작성
     * 
     * @param topicCountMap은  (topic, #streams) 쌍의 맵
     * @param 디코더는 메세지를 T로 바꿔주는 디코더이다.
     * @return은 (topic,list of KafkaStream) 쌍의 맵
     *      목록에 있는 항목의 수는 #streams 이다. 각 스트림은 지원한다.
     *      메세지/메타데이터 쌍의 반복자.
     */
    public <K,V> Map<String, List<KafkaStream<K,V>>>
     createMessageStreams(Map<String, Integer> topicCountMap, Decoder<K> keyDecoder, Decoder<V> valueDecoder);

    /**
     * 디폴트 디코더를 사용해서,각 토픽에 대한 타입 T의 메세지 스트림의 목록을 작성
     */
    public Map<String, List<KafkaStream<byte[], byte[]>>> createMessageStreams(Map<String, Integer> topicCountMap);

    /**
     * 와일드 카드와 일치하는 토픽에 대한 메세지 스트림의 목록을 작성
     * 
     * @param 토픽필터는 (화이트리스트 또는 블랙리스트 캡슐화)에 가입하는 어떤 토픽을 명시해야하는 토픽필터이다.
     * @param 넘스트름은 반화하는 메세지 스트림의 수
     * @param 키디코더는 메세지 키를 해독하는 디코더이다.
     * @param 밸류디코더는 메세지 자체를 해독하는 디코더이다
     * @return 카프카스트림의 목록. 각 스트림은 MessageAndMetadata 요소의 반복자를 지원합니다.
     */
    public <K,V> List<KafkaStream<K,V>>
     createMessageStreamByFilter(TopicFilter topicFilter, int numStreams, Decoder<K> keyDecoder, Decoder<V> valueDecoder);

    /**
     * 디폴트 디코더를 사용해서,와일드 카드와 일치하는 토픽에 대한 메세지 스트림의 목록을 작성
     */
    public List<KafkaStream<byte[], byte[]>> createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);

    /**
     * 하나의 스트림으로,디폴트 디코더를 사용해서,와일드 카드와 일치하는 토픽에 대한 메세지 스트림의 목록을 작성
     */
    public  List<KafkaStream<byte[], byte[]>> createMessageStreamByFilter(TopicFilter topicFilter);

    /**
     * 이 커넥터에 연결되는 모든 토픽/파티션의 오프셋을 커밋
     */
    public void commitOffsets();

    /**
     * 커넥터 종료
     */
    public void shutdown();
    }

너는 고수준 컨슈머 API를 어떻게 사용하는 지 배울 수 있는 [이 예](https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example)를 따를 수 있습니다.

#### 2.3 단순한 컨슈머 API

    class kafka.javaapi.consumer.SimpleConsumer {
    /**
     * 토픽에서 메세지의 세트를 가져옵니다.
     * 
     * @param 요청 토픽 이름,토픽 파티션,오프셋 바이트 시작,가져올 수 있는 최대 바이트 수를 명시합니다.
     * @retrn 가져온 메시지의 세트
     */
    public FetchResponse fetch(kafka.javaapi.FetchRequest request);

    /**
     * 주제의 순서에 대한 메타데이터 가져오기.
     * 
     * @param 요청 버전ID,클라이언트ID,토픽의 순서를 명시.
     * @return 요청에서 각 주제에 대한 메타데이터.
     */
    public kafka.javaapi.TopicMetadataResponse send(kafka.javaapi.TopicMetadataRequest request);

    /**
     * 주어진 시간전에 유효한 오프셋(최대사이즈 까지)의 목록을 가져오기.
     * 
     * @param [[kafka.javaapi.OffsetRequest]] 객체를 요청.
     * @return [[kafka.javaapi.OffsetResponse]] 객체.
     */
    public kafka.javaapi.OffsetResponse getOffsetBefore(OffsetRequest request);

    /**
     * 단순컨슈머 닫기.
     */
    public void colse();
    }

대부분의 응용프로그램에서, 고수준 컨슈머 API는 충분합니다. 몇몇의 응용프로그램은 아직 고수준 컨슈머에서 드러나지않은 특징을 원합니다.(예를 들어,컨슈머를 재시작할때 초기 오프셋을 설정) 그들은 대신에 낮은수준의 단순한 컨슈머 API를 사용할 수 있습니다. 논리는 좀 더 복잡하고 너는 [여기](https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example)에 있는 예를 따를 수 있습니다.

#### 2.4 카프카 하둡 컨슈머 API

하둡에 데이터를 집계하고 로딩에 대한 수평 확장가능한 솔루션을 제공하는 것은 우리의 기본 사용 사례중 하나입니다. 이 사용사례를 지원하기 위해서, 우리는 병렬 카프카 클러스터로 부터 데이터를 가져오는 많은 맵 작업을 생성하는 하둡에 기초한 컨슈머를 제공합니다. 이것은 매우 빠른 풀 기반의 하둡 데이터 로드 능력을 제공합니다.(우리는 몇 안되는 카프카 서버와 네트워크를 완전히 포화할 수 있습니다.)

하둡 컨슈머에 대한 사용정보는 [여기](https://github.com/linkedin/camus/)에서 찾을 수 있습니다.

### 3. 환경설정

카프카는 설정에 대한 [특성 파일 형식](http://en.wikipedia.org/wiki/.properties)으로 키-값(key-value) 쌍을 사용합니다. 이 값은 파일이나 프로그램에서 제공할 수 있습니다.

#### 3.1 브로커 설정 

필수적인 구성은 다음과 같다:

* broker.id
* log.dirs
* zookeeper.connect

토픽 레벌 설정 및 디폴트는 [아래](http://kafka.apache.org/documentation.html#topic-config)에서 더 자세히 논의됩니다.

속성|디폴트|서술
-----|-------------|--------------
broker.id |         |각각의 브로커는 음이 아닌 정수 ID에 의해 고유의 방법으로 식별됩니다. 이 ID는 브로커의 "이름"의 역할을 하고 브로커가  혼란스러워하는 컨슈머 없이 다른 호스트/포트로 이동할 수 있습니다. 너는 이것이 고유하는 한 너가 원하는 아무 숫자를 선택할 수 있습니다.
log.dirs | /tmp/kafka-logs | 카프카 데이터가 저장되는 하나 이상의 디렉토리들의 쉼표로 구분된 목록입니다. 생성된 각각의 새로운 파티션은 현재 가장 적은 파티션을 가진 디렉토리에 배치됩니다.
port | 6667 | 서버가 클라이언트 연결을 허용하는 포트입니다. 
zookeeper.connect | null | 호스트이름 형식에서 주키퍼 연결 스트링을 지정: 호스트이름과 포트가 너의 주키퍼 클러스터의 노드에 대한 호스트와 포트인 포트, 호스트가 다운되었을때 다른 주키퍼 노드들을 통해 연결할 수 있도록하려면 너는 또한 hostname1: port1,hostname2:port2,hostname3:port3 형태에서 다양한 호스트들을 지정할 수 있습니다. 주키퍼는 또한 너가 특정 경로 아래에서 이 클러스터를 보여주는 모든 카프카 데이터를 만들수 있는 "chroot" 경로를 추가하도록 허락합니다. 이것은 다수의 카프카 클러스터 또는 같은 주키퍼 클러스터에 있는 다른 응용프로그램을 설정하는 방법입니다. 이 작업을 수행하기위해서 path/chroot/path 아래에서 모든 이 클러스터의 데이터를 넣을수 있는hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 형태의 연결 스트링을 제공합니다. 너는 이 경로를 브로커를 시작하기 전에 스스로 만들 수 있어야 하고 컨슈머는 같은 연결 스트링을 사용해야합니다. 
message.max.bytes | 1000000 | 서버가 받을 수 있는 메세지의 최대 크기. 이 속성은 너의 컨슈머가 사용하는 최대 페치 사이즈 또는 다루기 힘든 프로듀서가 컨슈머가 사용하기에 너무 큰 메세지 게시와 협조 관계에 있다는 것이 중요합니다.
num.network.threads | 3 | 네트워크 요청들을 다루는 서버가 사용하는 네트워크 스레드의 수. 너는 아마도 이것을 바꿀 필요가 없습니다.
num.io.threads | 8 | 서버가 요청을 실행하기 위해서 사용하는 I/O 스레드의 수. 너는 적어도 너가 디스크를 가지고 있는 만큼 스레드를 가져야합니다.
background.threads | 4 | 파일 삭제와 같이 다양한 백그라운드 처리 작업을 위해 사용하는 스레드의 수. 이것을 바꿀 필요가 없습니다.
queued.max.requests | 500 | 네트워크 스레드 전에 I/O 스레드에 의해 처리를 위해 대기 할 수 있는 요청의 수는 새로운 요청에서 읽는 것을 멈춥니다.
host.name | null | 브로커의 호스트이름. 이것이 설정되면, 이것은 단지 이 주소에 바인드합니다. 이것이 설정이 안됬다면, 이것은 모든 인터페이스에 바인드하고,ZK에 하나를 게시합니다.
advertised.host.name | null | 이것이 설정되어있는 경우 이것은 연결하기 위해 생산자,소비자,그리고 다른 브로커에서 나와 주어진 호스트 이름입니다.
advertised.port | null | 연결 설정에 사용하는 프로듀서,컨슈머,그리고 다른 브로커에게 제공하는 포트.
socket.send.buffer.bytes | 100*1024 | 서버가 소켓 연결을 선호하는 SO_SNDBUFF 버퍼.
socket.receive.buffer.bytes | 100*1024 | 서버가 소켓 연결을 선호하는 SO_RCVBUFF 버퍼.
socket.request.max.bytes | 100x1024x1024 | 서버가 허용할수있는 최대 요청 크기. 이것은 메모리 부족으로부터 서버를 예방하고 자바 힙 크기보다 작아야합니다.
num.partitions | 1 | 파티션 카운트가 토픽 생성 시간에 주어지지않은 경우,토픽 당 파티션의 디폴트 수.
log.segment.bytes | 1024x1024x1024 | 토픽 파티션에 대한 로그는 세그먼트 파일의 디렉토리로 저장합니다. 이 설정은 새로운 세그먼트가 로그에 롤 오버되기 전에 세그머느 파일이 증가 할 수 있는 크기를 조절합니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config) 참조)에 오버라이딩할 수 있습니다.
log.roll.hours | 24*7 | 이 설정은 log.segment.bytes크기에 도달하지 않더라도 새로운 로그 세그먼트를 돌아가게 하라고 카프카를 강요할 것입니다. 이 설정은 토픽다 기초([토픽당 구성 섹션](http://kafka.apache.org/documentation.html#topic-config) 참조)에 오버라이딩할 수 있습니다.
log.clenup.policy | delete | 이것은 값을 지우거나 콤팩 둘 중 하나를 씁니다. 삭제를 설정한 경우 로그 세그먼트는 그들이 크기 제한 또는 설정된 시간에 도달할때 지울 것 입니다. 콤팩으로 설정한 경우 로그 콤팩션은 쓸모가 없는 기록들을 지우는 데 사용될 것입니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.retention.{minutes,hours} | 7 days | 그것이 지워지기 전에 로그 세그먼트를 유지하기 위한 시간의 양,즉 모든 토픽에 대한 디폴트 데이터 유지 윈도우. log.retention.minutes 와 log.retention.bytes 둘다 양쪽 모두 설정되어 있는 경우 우리는 어느 제한을 초과할때 세그먼트를 삭제합니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.retention.bytes | -1 | 각 토픽파티션에 대한 로그에서 보유하는 데이터의 양. 토픽에 대해 보존된 합계 데이터를 얻기 위해 파티션 수에 곱해서 이것은 파티션 당 제한을 합니다. (Note that this is the limit per-partition so multiply by the number of partitions to get the total data retained for the topic.) 또한 log.retention.hours 와 log.retention.bytes 둘다 설정된다면 우리는 제한이 초과될때 세그먼트를 삭제합니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.retention.check.interval.ms | 5 minutes | 우리가 아무 로그 세그먼트가 보유 기간을 충족시키는 삭제에 대한 자격이 있는지 체크하는 기간입니다.
log.cleaner.enable | false | 로그 콤팩션을 실행하기 위해서 이 설정은 사실로 설정해야 합니다.
log.cleaner.threads | 1 | 로그 컴팩션에서 로그를 청소하기 위해 사용하는 스리드의 수.
log.cleaner.io.max.bytes.per.second | None | I/O 로그 클리너의 최대 양은 로그 콤팩션을 수행하는 동안 할 수 있습니다. 이 설정은 라이브 요청 서빙에 영향주는 걸 피하기 위한 클리너에 대해 제한을 설정하는 걸 허용합니다.
log.cleaner.dedupe.buffer.size | 500x1024x1024 | 버퍼 로그 클러너의 크기는 청소하는동안 인덱싱 및 로그 중복제거를 위해서 사용합니다. 더 큰 더 나은 충분한 메모리가 제공됩니다.
log.cleaner.io.buffer.size | 512*1024 | 로그 청소하는 동안 사용되는 I/O 청크의 크기. 너는 아마도 이것을 바꿀 필요가 없습니다.
log.cleaner.io.buffer.load.factr | 0.9 | 로그 청소에 사용되는 해시 테이블의 로드 요소.너는 아마도 이것을 바꿀 필요가 없습니다.
log.cleaner.backoff.ms | 15000 | 어느 로그가 청소가 필요한 경우 검사 사이의 간격.
log.cleaner.min.cleanable.ratio | 0.5 | 이 설정은 얼마나 자주 로그 컴팩터가 로그를 청소하기 위해 시도 했는지 조절합니다.([로그 컴팩션](http://kafka.apache.org/documentation.html#compaction)이 가능한지 추정). 자동으로 우리는 콤팩트가 된 로그의 반이상인 곳에서 로그 청소하는 걸 피할 수 있습니다. 이 비율은 복제(50%에서 대부분의 로그 50%로 복제 될 수 있습니다.)에 의한 로그에서 소비되는 최대 공간 경계를 이룹니다. 높은 비율은 더 적고,보다 효율적으로 클리닝하는걸 의미하지만 로그에서 더 많은 공간을 쓰는 것도 의미합니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.cleaner.delete.retention.ms | 1 day | 보유하는 시간의 양은 컴팩트 된 토픽 로그에 대한 탐스톤 표시를 삭제합니다. 이 설정은 또한 그들이 마지막 단계의 유효한 스냅샷을 얻을 수 있도록 0 오프셋에서 시작하면 컨슈머가 읽기를 완료해야하는 시간에 바운드를 제공합니다.(그들이 그들의 스캔을 완료하기전에 그렇지 않으면 삭제 탐스톤이 수집될 수 있습니다.) 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.index.size.max.bytes | 10x1024x1024 | 각 로그 세그먼트의 오프셋 인덱스를 허용하는 바이트의 최대 크기. 우리는 항상 이 많은 공간과 함께 스파스 파일을 미리 할당할 수 있고 로그를 굴릴 때 이것을 아래로 감소할 수 있습니다. 인텍스가 채워지면 우리가 log.segment.bytes.limit 에 도달하지 않더라도 우리는 새로운 로그 세그먼트를 굴릴 수 있습니다. 이 설정은 토픽당 기초([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참조)에 오버라이딩할 수 있습니다.
log.index.interval.bytes | 4096 | 우리가 오프셋 인덱스에 항목을 추가하는 바이트 간격. 페치 요청을 실행할때 서버는 시작하고 페치를 종료하는 로그에 정확한 위치를 찾을 수 있는 이 많은 바이트들까지에 대한 선형 검색을 수행해야합니다. 그래서 이 값을 더 크게 설정하는 것은 더 큰 인덱스 파일(그리고 좀 더 많은 메모리 사용량)을 의미하지만 더 적은 스캐닝을 의미합니다. 그러나 서버가 로그 어펜드당 하나 이상의 인덱스 항목을 추가하지 않습니다.(log.index.interval 이상의 메세지의 가치를 추가할지라도) 일반적으로 너는 아마도 이 값을 엉망으로 만들 필요는 없습니다.
log.flush.interval.messages | None | 우리가 로그에 fsync를 강요하기전에 로그 파티션에 쓰여진 메세지의 수. 이것을 더 낮게 설정하는 것은 더 자주 데이터를 디스크에 싱크할수 있지만 성능에 주된 영향을 줍니다. 우리는 일반적으로 사람들이 단일 서버 fsync 보다 내구성을 위한 복제를 이용하는것을 권고합니다,그러나 이 설정은 특정 추가로 사용 할 수 있습니다.
log.flush.scheduler.interval.ms | 3000 | 로그 플러셔가 어느 로그가 디스크로 플러시할 자격이 있는지 체크하는 MS의 주파수.
log.flush.interval.ms | None | fsync 사이의 최대 시간은 로그를 부릅니다. log.flush.interval.messages와 결합해 사용하는 경우 어느 기준이 충촉될 때 로그는 플러시됩니다.
log.delete.delay.ms | 60000 | 우리가 그들이 메모리 안 세그먼트 인덱스로 부터 제거돈 후 로그파일주위를 가지고 있는 시간의 기간. 이 시간의 기간은 어느 진행중인 읽기를 잠금없이 방해하지 않고 완성할 수 있습니다. 너는 일반적으로 이것을 바꿀 필요가 없습니다
log.flush.offset.checkpoint.interval.ms | 60000 | 우리는 복구 로그에 대한 마지막 플러시 포인트인 체크 포인트가 있는 주파수. 너는 이것을 바꿀 필요가 없습니다.
auto.create.topics.enable | true | 서버에 있는 토픽의 자동생성을 가능하게 합니다. 이것이 true로 설정된 경우 생산,소비,또는 존재하지 않는 토픽에 대한 메타데이터를 가져오기 위한 시도는 자동으로 디폴트 복제 요소와 파티션의 번호와 함께 이것을 생성합니다.
controller.socket.timeout.ms | 30000 | 복제본에 파티션 관리 컨트롤러의 명령에 대한 소켓 타임아웃.
controller.message.queue.size | 10 | 컨트롤러와 브로커 간 채널에 대한 버퍼 사이즈.
default.replication.factor | 1 | 자동으로 생성되는 토픽에 대한 디폴트 복제 요소.
replica.lag.time.max.ms | 10000 | 팔로워가 시간의 윈도우에 대한 아무 페치 요청을 전송하지 않은 경우,리더는 ISR(복제본 동기화)로부터 팔로워를 제거할 수 있고 죽은걸로 취급할 수 있습니다.
replica.lag.max.messages | 4000 | 복제본이 리더 뒤에 이 많은 메세지보다 떨이질 경우,리더는 ISR로부터 팔로워를 제거할 수 있고 이것을 죽은걸로 취급할 수 있습니다.
replica.socket.timeout.ms | 30*1000 | 네트워크 소켓 타임아웃은 데이터를 복제하라고 리더에게 요청합니다.
replica.socket.receive.buffer.bytes | 64*1024 | 네트워크 소켓 리시브 버퍼는 데이터를 복제하라고 리더에게 요청합니다.
replica.fetch.max.bytes | 1024*1024 | 복제 요청 패치에 있는 각각의 파티션에 대해 가져오는 것을 시도하는 메세지 바이트의 수는 리더에게 전송됩니다.
replica.fetch.wait.max.ms | 500 | 리더 복제에 의해 전송되는 패치 요청에서 리더에게 도달하는 데이터에 대한 최대 대기 시간.
replica.fetch.min.bytes | 1 | 리더 복제에서 패치 요청에 대한 각 패치 응답을 기다리는 최소의 바이트. 바이트가 충분하지 않은 경우, 많은 바이트가 도착하는 이  replica.fetch.wait.max.ms까지 기다립니다.
num.replica.fetchers | 1 | 리더에서 메세지를 복제하는 데 사용되는 스레드의 수. 이 값을 늘리는 것은 팔로워 브로커에 I/O 병렬 처리 수준을 높일 수 있습니다.
replica.high.watermark.checkpoint.interval.ms | 5000 | 복구를 다루는 디스크에 이것의 높은 워터마크를 저장하는 각각의 복제본을 가진 주파수
fetch.purgatory.purge.interval.requests | 10000 | 퍼게토리 요청 패치의 제거 간격(요청 수).
producer.purgatory.purge.interval.requests | 10000 | 프로듀서 요청 패치의 제거 간격(요청 수).
zookeeper.session.timeout.ms | 6000 | 주키퍼 세션 타임 아웃. 서버가 이 시간내에 주키퍼에 하트비트 실패한경우 서버가 죽은걸로 취급합니다. 이것을 너무 낮게 설정하면 서버는 잘못하여 죽은 걸로 간주; 이것을 너무 높게 설정하면 이것은 진짜 죽은 서버를 인식하는데 너무 오랜 시간이 걸릴 수 있습니다.
zookeeper.connection.timeout.ms | 6000 | 클라이언트가 주키퍼 연결을 설정하기 위해 기다리는 최대 시간.
zookeeper.sync.time.ms | 2000 | 얼마나 멀리 ZK팔로워가 ZK리더 뒤에 있는지.
controlled.shutdown.enable | false | 브로커의 통제된 중단을 활성화합니다. 활성화된경우, 브로커는 자기를 중단하기전에 모든 리더들을 다른 몇몇의 브로커들로 움직일 수 있습니다. 이것은 중단하는동안 윈도우 비가동률을 줄입니다.
controlled.shutdown.max.retries | 3 | 부정 종료를 실행하기 전에 성공적으로 제어된 종료를 완료하는 재시도 횟수
controlled.shutdown.retry.backoff.ms | 5000 | 종료 시도 사이의 백오프 시간.
auto.leader.rebalance.enable | false | 이것을 활성화한경우 컨트롤러는 자동으로 이것이 가능하면 주기적으로 각 파티션에 대한 "선호"복제에 리더쉽을 반환함으로써 브로커 간 파티션에 대한 리더쉽을 맞추려고 시도할 것입니다.
leader.imbalance.per.broker.percentage | 10 | 브로커당 허용하는 리더 불균형의 퍼센트. 컨트롤러는 이 비율이 브로커당 설정된 값을 넘어가는 경우 리더쉽을 재균형합니다.
leader.imbalance.check.interval.seconds | 300 | 리더 불균형을 확인하는 주파수.
offset.metadata.max.bytes | 1024 | 클라이언트가 그들의 오프셋에 저장할 수 있는 메타데이터의 최대 양.

브로커 설정에 대한 자세한 내용은 스칼라 클래스 kafka.server.KafkaConfig에서 찾을 수 있습니다.

##### 토픽수준의 설정

주제에 대한 적절한 설정은 글로벌 디폴트와 조절 토픽당 오버라이드 둘 다 가지고있습니다. 토픽당 설정이 주어지지않으면 글로벌 디폴트가 사용됩니다. 오버라이드는 하나이상의 --config 옵션을 제공하면서 토픽생성시간에 설정할 수 있습니다. 이 예는 *my-topic*라는 토픽을 사용자최대메세지크기와 플러시비율과 함께 생성:

	> bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my-topic --partitions 1
	      --replication-factor 1 --config max.message.bytes=6400 --config flush.messages=1

또 오버라이드는 토픽 명령은 바꾸거나 대안토픽명령을 사용하여 늦게 설정할 수 있습니다. 이 예는 *my-topic*에 대한 최대 메세지 크기 업데이트:

	> bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic
	   --config max.message.bytes=12800

오버라이드를 제거하는 것도 할 수 있다.

	> bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic
	   --deleteConfig max.message.bytes

다음은 토픽수준의 설정입니다. 서버 디폴트 속성(Server Default Property) 제목, 서버 설정에 이 디폴트를 설정하는 아래에 제공된 이 속성에 대한 서버의 디폴트 설정은 너가 명시된 오버라이드가 업는 토픽에 주어진 디폴트를 바꾸는 걸 허용합니다.

속성 | 디폴트 | 서버 디폴트 속성 |      묘사      |
----|-------|----------------------------------------|---------------|
cleanup.policy | delete | log.cleanup.policy | "컴팩트" 또는 "삭제" 중 하나인 스트링입니다. 이 스트링은 이전 로그 세그먼트에서 사용하는 보존정책을 지정합니다. 디폴트 정책("삭제")은 그들의 보존시간 또는 제한시간에 도달하면 이전 세그먼트를 버립니다. "컴팩트"설정은 토픽에 [로그 컴팩션](http://kafka.apache.org/documentation.html#compaction)을 가능하게 합니다.
delete.retention.ms | 86400000(24 hours) | log.cleaner.delete.retention.ms | [로그 압축된](http://kafka.apache.org/documentation.html#compaction) 토픽에 대해 삭제 표시 마커들을 유지하는 시간. 또 이 설정은 그들이 마지막 단계의 유효한 스냅샷을 얻을 수 있도록 오프셋0에서 부터 시작하면 컨슈머가 읽기를 완료해야하는 시간에 바운드를 제공합니다.(그렇지 않으면 그들이 자신의 스캔을 완료하기전에 삭제 표시가 수집될 수 있습니다.)
flush.messages | None | log.flush.interval.messages | 이 설정은 우리가 로그에 기록된 데이터의 fsync를 강제로하는 간격을 지정할 수 있습니다. 예를들어 이것이 1로 설정되면 우리는 모든 메세지후에 fsync로 할 것입니다; 이것이 5로 설정되면 우리는 모든 다섯개 메세지 후에 fsync로 할 것입니다. 일반적으로 우리는 너에게 이것을 설정하지 말고 내구성을 위해 복제를 사용하고 더 효과적이기 때문에 운영체제의 백그라운드 플러시 능력을 허용하라고 추천합니다. 이 설정은 토픽당 기초에 오버라이딩 할 수 있습니다.([토픽당 설정 섹션](http://kafka.apache.org/documentation.html#topic-config)을 참고)
flush.ms | None | log.flush.interval.ms | 이 설정은 로그에 기록된 데이터의 fsync를 강제로 하는 시간 간격을 지정할 수 있습니다. 예를 들어, 이것이 1000으로 설정된다면 우리는 1000ms를 지난 후에 fsync를 할 것입니다. 일반적으로 우리는 너가 이것을 설정하지 않고 내구성을 위해서 복제를 사용하고 더 효과적이기 때문에 운영체제의 백그라운드 플러시 능력을 허용하라고 추천합니다.
index.interval.bytes | 4096 | log.index.interval.bytes | 이 설정은 카프카가 오프셋 인덱스에 인덱스 항목을 추가하는 빈도를 조절합니다. 디폴트 설정은 우리가 메세지를 거의 모든 4096바이트를 인덱스한다고 보장합니다. 더 인덱싱하는 것은 로그안에 정확한 위치에 더 가깝게 이동해서 읽는걸 허용하지만 인덱스가 커집니다. 너는 아마도 이것을 바꿀 필요가 없습니다.
max.message.bytes | 1,000,000 | message.max.bytes | 이것은 가장 큰 메세지 크기입니다. 카프카는 이 토픽에 추가할 수 있습니다. 너가 사이즈를 증가시키면 그들이 이 크기에 메세지를 페치할 수 있도록 너는 또한 너의 컨슈머의 페치사이즈도 증가시켜야 한다는걸 주의해야합니다.
min.cleanable.dirty.ratio | 0.5 | log.cleaner.min.cleanable.ratio | 이 설정은 로그 컴팩터가 로그를 청소하는 시도의 빈도를 조절합니다([로그 컴팩션](http://kafka.apache.org/documentation.html#compaction)이 사용가능하다고 가정). 기본적으로 우리는 50%이상의 로그가 압축되있는 로그를 청소하는걸 피해야 합니다. 이 비율은 복제에 의한 로그에 낭비하는 최대 공간을 바운드합니다(50%에서 로그의 대부분의 50%는 복제될 수 있습니다). 더 높은 비율은 더 낮고,더 효과적인 청소를 의미하지만 로그에 공간을 더 낭비하는 것도 의미합니다.
retention.bytes | None | log.retention.bytes | 이 설정은 우리가 "삭제"보유정책을 사용하는 경우 우리가 공간 확보를 위해서 이전 로그 세그먼트를 버리기전에 로그가 성장할 수 있는 최대 크기를 제어합니다. 기본적으로 시간제한만 크기제한이 없습니다.
retention.ms | 7 days | log.retention.minutes | 이 설정은 우리가 "삭제"보유정책을 사용하는 경우 우리가 공간 확보를 위해서 이전 로그 세그먼트를 버리기전에 로그가 성장할 수 있는 최대 크기를 제어합니다. 이것은 얼마나 빨리 컨슈머가 그들의 데이터를 읽을 수 있는지를 나타낸 SLA를 나타냅니다.
segment.bytes | 1 GB | log.segment.bytes | 이 설정은 로그에 대한 세그먼트 파일을 제어합니다. 유지와 청소는 항상 한번에 파일을 수행해서 더 큰 세그먼트 크기는 더 적은 파일을 의미하지만 유지를 통해 덜 세분화된 것을 제어합니다.
segment.index.bytes | 10 MB | log.index.size.max.bytes | 이 설정은 오프셋을 파일 위치로 맵핑하는 인덱스의 사이즈를 조절합니다. 우리는 이 인덱스 파일을 미리 할당하고 이것을 로그 롤 후에 수축합니다. 너는 일반적으로 이 설정을 바꿀 필요가 없습니다.
segment.ms | 7 days | log.roll.hours | 이 설정은 세그먼트 파일이 기억이 삭제되거나 오래된 데이터를 압축하는 걸 완전히 보장하지 않더라도 어떤 카프카가 롤에 로그를 강요한 후 시간기간을 조절합니다.

#### 3.2 컨슈머 설정

필수 컨슈머 설정은 다음과 같습니다:

* group.id
* zookeeper.connect

속성 | 디폴트 | 서술
----|-------|------
group.id | | 유일하게 컨슈머가 속해있는 소비자프로세스의 그룹을 식별하는 스트링. 같은 그룹id를 설정함으로써 다수의 프로세스들은 그들이 같은 컨슈머 그릅의 일부분이라는걸 나타냅니다.
zookeeper.connect | | 호스트와 포트가 주키퍼 서버의 호스트와 포트인 형식 hostname:port에 있는 주키퍼 연결 스트링을 명시합니다. 주키퍼 기계가 다운될 때 다른 주키퍼를 통해 연결하기 위해서 너는 또한 형식 hostname1:port1,hostname2:port2,hostname3:port3에 있는 다수의 호스트를 명시해야합니다. 또 서버는 글로벌 주키퍼 명칭공간에 어떤 경로아래에 데이터를 넣는 주키퍼 연결 스트링의 부분으로 주키퍼 chroot 경로를 가질 수 있습니다. 그렇다면 컨슈머는 연결 스트링에서 같은 chroot 경로를 사용해야 합니다. 예를들어 /chroot/path 의 chroot 경로를 주기위해 너는 hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 같은 연결 스트링을 제공해야합니다.
consumer.id | null | 설정되지 않앗으면 자동으로 생성됩니다.
socket.timeout.ms | 30*1000 | 네트워크 요청을 위한 소켓 타임아웃. 실제 타임아웃 세트는  max.fetch.wait + socket.timeout.ms 될 것입니다.
socket.receive.buffer.bytes | 64*1024 | 네트워크 요청을 위한 소켓 리시브 버퍼
fetch.message.max.bytes | 1024*1024 | 각 페치 요청의 각 토픽파티션에 대한 페치를 시도하는 메세지 바이트의 수. 이런 바이트는 각 파티션에 대한 메모리로 읽혀질 것입니다, 그래서 이것은 컨슈머에 의해 사용되어지는 메모리를 조절하는데 도움을 줍니다. 페치 요청 크기는 적어도 서버가 허용하거나 프로듀서를 컨슈머가 페치할 수 있는 것보다 더 크게 메세지를 전송하는 것이 가능한 최대 메세지크기만큼 커야합니다.
auto.commit.enable | true | 사실이면,주기적으로 이미 컨슈머가 가져온 메세지의 주키퍼 오프셋 커밋. 이 커밋된 오프셋은 새로운 컨슈머가 시작되는 위치로서 프로세스가 실패할때 사용되어질 것입니다.
auto.commit.interval.ms | 60*1000 | 컨슈머 오프셋을 주키퍼에 커밋하는 MS에 주파수.
queued.max.message.chunks | 10 | 소비에 대해 버퍼하는 메세지 청크의 최대 수. 각 청크는 fetch.message.max.bytes 까지 할 수 있습니다.
rebalance.max.retries | 4 | 새로운 컨슈머가 컨슈머 그룹에 합류할때 컨슈머의 세트가 각 컨슈머에 파티션을 할당하는 "재균형" 로드를 시도합니다. 컨슈머의 세트가 바뀔때 이 임무가 일어나는 동안 재균형은 실패하고 다시 시도합니다. 이 설정은 포기하기전에 시도의 최대수를 조절합니다.
fetch.min.bytes | 1 | 페치 요청에 대한 서버가 반환할 수 있는 데이터의 최소양. 사용할 수 있는 데이터가 부족하면 요청은 요청에 답하기전에 많은 데이터를 축적하는 것을 기다릴 것입니다.
fetch.wait.max.ms | 100 | 즉시 fetch.min.bytes를 만족하는 데이터가 부족할때 페치 요청에 답하기전에 서버가 막는 최대시간.
rebalance.backoff.ms | 2000 | 재균형하는 동안 재시도 간의 백오프 시간.
refresh.leader.backoff.ms | 200 | 이것의 리더를 잃은 파티션의 리더를 결정하기전에 대기하는 시간이 백오프 시간.
auto.offset.reset | largest | 오프셋이 범위를 벗어나거나 주키퍼에 초기화 오프셋이 없을 때 해야 할 일:     최소 : 자동으로 오프셋을 가장 작은 오프셋으로 리셋.  최대 : 자동으로 오프셋을 가장 큰 오프셋으로 리셋  다른것 : 예외를 컨슈머에게 던지다.
consumer.timeout.ms | -1 | 지정한 간격 후 메세지가 소비를 불가능한 경우 컨슈머에게 타임아웃 예외를 던지다.
client.id | group id value | 클라이언트ID는 호출 추적을 도와주기 위해 각 요청으로 보내지는 사용자가 지정한 스트링입니다. 이것은 논리적으로 요청하는 어플리케이션을 구별합니다.
zookeeper.session.timeout.ms | 6000 | 주키퍼 세션 타임아웃. 소비자가 이 기간동안 주키퍼에 하트비트를 실패하면 이것은 죽은걸로 취급되고 재균형이 발생할 것 입니다.
zookeeper.connection.timeout.ms | 6000 | 주키퍼에 연결하는 동안 클라이언트가 기다리는 최대시간.
zookeeper.sync.time.ms | 2000 | ZK팔로워가 얼마나 멀리 ZK지도자 뒤에 있을 수 있는지

컨슈머 설정에 대한 자세한 내용은 스칼라 클래스 kafka.consumer.ConsumerConfig에서 찾을 수 있습니다.

#### 3.3 프로듀서 설정

프로듀서를 위한 필수 구성 특성은 다음과 같습니다:

* metadata.broker.list
* request.required.acks
* producer.type
* serializer.class

속성 | 디폴트 | 서술 |
----|-------|------|
metadata.broker.list | | 이것은 부트스트래핑을 위하고 프로듀서는 이것을 메타데이터(토픽,파티션,그리고 복제본)를 얻기위해서 사용합니다. 실제 데이터를 전송하는 소켓 연결은 메타데이터에서 반환된 브로커 정보에 기반하여 설정합니다. 형식은 host1:port1,host2:port2이고, 목록은 브로커의 일부 또는 브로커의 일부를 가리키는 VIP가 될 수 있습니다.
request.required.acks|  0 | 이 값은 생산요청이 완성됫다고 취급될 때 조절합니다. 특히,얼마나 다른 브로커가 그들의 로그에 데이터를 커밋하고 이것을 리더에게 알릴 수 있을까? 전형적인 값은   0, 프로듀서가 절대 브로커 확인을 기다리지 않는 것을 의미합니다(0.7과 같은 행동).이 옵션은 가장 낮은 지연을 제공하지만 가장 약한 내구성 보장을 제공하기도 합니다(어떤 데이터는 서버가 실패할때 손실될 수도 있습니다).  1, 리더 복제본이 데이터를 수신한 후 프로듀서가 확인을 얻는 것을 의미합니다. 이 옵션은 서버가 성공적으로 요청을 승인할때까지 클라이언트가 기다리는 더 나은 내구성을 제공합니다(지금은 죽은 리더에 기록되어있지만 아직 복제되지 않은 메시지만 손실됩니다).  -1, 모든 동기화 복제본이 데이터를 받은 후에 프로듀서가 확인을 얻는 것을 의미합니다. 이 옵션은 최상의 내구성을 제공하고,우리는 적어도 동기 복제에 하나가 남아있는 한 어떤 메세지도 손실되지 않음을 보장합니다.
request.timeout.ms | 10000 | 클라이언트에게 에러를 다시 보내기전에 request.required.acks 요건을 충족시키는 시도를 기다리는 시간.
producer.type | sync | 이 파라미터는 메세지가 백그라운드 스러드에 비동기로 보내지는지 를 명시합니다. 유효한 값은 비동기 전송을 위한(1) 비동기통신이고 동기 전송을 위한 (2) 동기통신입니다. 프로듀서가 비동기로 설정함으로써 우리는 요청(처리량이 좋은)을 같이 배치할 수 있지만 보내지 않은 데이터를 떨어뜨리는 클라이언트 기계의 가능성을 엽니다.
serializer.class | kafka.serializer.DefaultEncoder | 메세지의 직렬 클래스. 디폴트 인코더는 byte[]를 받고 같은 byte[]를 반환합니다.
key.serializer.class | | 키의 시리얼 클래스(아무것도 주어지지 않은 경우 메세지와 같은 디폴트).
partitioner.class | kafka.producer.DefaultPartitioner | 하위 토픽 사이에 메세지를 구획하는 파티션 클래스. 디폴트 파티셔너는 키의 해시에 기초합니다.
compression.codec | none | 이 파라미터는 이 프로듀서에 의해 발생된 모든 데이터에 대한 압축코덱을 명시할 수 있습니다. 유효한 값은 "none","gzip" 그리고 "snappy"입니다.
compressed.topics | null | 이 파라미터는 압축이 특정 토픽에 대해 변해야 하는지를 설정할 수 있습니다. 압축 코덱이 NoCompressionCodec 이외만 있는 경우 명시된 토픽에 대한 압축을 할 수 있습니다. 압축된 토픽의 목록이 비어있다면, 모든 토픽에 명시된 압축 코덱을 가능하게 해야합니다. 압축 코덱이 NoCompressionCodec인 경우, 압축은 모든 토픽에 대해 사용할 수 없습니다.
message.send.max.retries | 3 | 이 속성은 생산자가 자동으로 실패한 전송 요청을 재시도하게 합니다. 이 속성은 이런 실패가 발생할때 재시도 횟수를 명시합니다. 0이 아닌 값을 여기에 설정하는 것은 메세지가 보내지지만 확인 응답이 손실되게 하는 네트워크 에러의 경우에 중복을 초래할 수 있는 걸 주목.
retry.backoff.ms | 100 | 각 재시도 전에, 프로듀서는 새로운 리더가 뽑혀 있는지 확인하기 위해서 관정한련된 토픽의 메타데이터를 새로 고칩니다. 리더 선거가 다소 시간이 걸리니까, 이 속성은 메타데이터를 새로 고치기전에 프로듀서가 기다리는 시간을 명시해야 합니다.
topic.metadata.refresh.interval.ms | 600*1000 | 실패(파티션 잃음,리더 사용불가...등등)할때 일반적으로 프로듀서는 브로커로부터 토픽 메타데이터를 새로 고칩니다. 이것은 또한 정기적으로 조사합니다(기본:매 10분마다 600000ms). 너가 이 음의 값으로 설정하면, 메타데이터는 실패만 얻을 것입니다. 너가 이것을 0으로 설정하면, 메타데이터는 각 메세지가 보내진후(권장하지 않음) 새로 고쳐 얻을 것입니다. 중요 노트: 새로 고침 메세지가 전송된 후에만 일어납니다,그래서 프로듀서가 메세지를 보내지 않은 경우 메타데이터는 새로 고쳐지지 않습니다.
queue.buffering.max.ms | 5000 | 비동기 모드일때 데이터를 버퍼하는 최대 시간. 예를 들어 100의 설정은 한번에 보내는 메세지의 100ms와 함께 배치를 시도합니다. 이것은 처리량을 향상시키지만 버퍼링 때문에 메세지 배달 지연을 추가합니다.
queue.buffering.max.messages | 10000 | 프로듀서를 차단해야하거나 데이터를 중단시키는 어느 쪽이 발생하기 전에 비동기모드를 사용할 때 프로듀서를 줄지어 보내지 않은 메세지의 최대 수.
queue.enqueue.timeout.ms | -1 | 비동기모드로 실행할때 메세지를 떨어트리기 전에 막는 시간과 버퍼는 queue.buffering.max.messages에 도달합니다. 큐가 찬 경우 0으로 설정하면 0이벤트가 즉시 인큐하거나 삭제됩니다(프로듀서 전송 요청은 차단하지 않습니다). -1로 설정하면 프로듀서는 무기한으로 차단하고 기꺼이 전송을 삭제하지 않습니다.
batch.num.messages | 200 | 비동기모드를 사용할때 한 배치에서 보내지는 메세지의 수. 프로듀서는 이 메세지의 수가 보낼 준비 또는 queue.buffer.max.ms에 도달 할때까지 기다릴 것입니다.
send.buffer.bytes | 100*1024 | 소켓 쓰기 버퍼 크기
client.id | "" | 클라이언트 id는 호출추적을 도와주기 위해 각 요청에 전송 유저가 지정한 스트링입니다. 이것은 논리적으로 요청하는 어플리케이션을 구별합니다.

프로듀서 설정에 대한 자세한 내용은 스칼라 클래스 kafka.producer.ProducerConfig 에서 볼 수 있습니다.

#### 3.4 새 프로듀서 설정

우리는 기존에 존재하는 프로듀서 대체에 노력하고 있습니다. 이제 코드는 트렁크에서 사용가능하고 베타 품질로 취급될 수 있습니다. 다음은 새로운 프로듀서를 위한 설정입니다.

이름 | 타입 | 디폴트 | 중요성 | 서술
----|------|-------|-------|------
bootstrap.servers | list |  | high | 카프카 클러스터에 초기연결설정을 위해 사용한 호스트/포트 쌍의 목록. 데이터는 부트스트래핑에 대해 여기에 지정되는 서비에 관계없이 모든 서버를 통해 균형있게 로드될 것입니다-이 목록은 단지 서버의 풀세트를 발견하기 위해 사용한 초기 호스트에만 영향을 줍니다. 이 목록은 형식 host1:port1,host2:port2에 있어야 합니다. 이런 서버가 단지 전체 클러스터 멤버쉽(동적으로 변화할 수 있는)을 발견하기 위한 초기 연결에만 사용되기 때문에, 이 목록은 전체 서버의 세트를 포함할 필요는 없습니다(하지만 서버가 다운된 경우,너는 하나이상을 원할지도 모릅니다). 목록에 이용가능한 서버가 없는 경우 데이터를 보내는 것은 이용이 가능할때까지 실패할 것입니다. 
acks | string | 1 | high | 완료 요청을 고려하기전에 프로듀서는 리더에게 받으라고 요청하는게 확인 횟수 입니다. 이것은 전송된 기록의 내구성을 조절합니다. 다음 설정은 일반적이다: acks = 0 0으로 설정한 경우 프로듀서는 모든 서버에서의 승인을 기다리지 않습니다. 기록은 즉시 소켓버퍼에 추가하거나 전송을 고려합니다. 보장은 서버가 이 경우에 기록을 받았다고 할 수 없고 재시도 설정이 적용되지 않습니다(일반적으로 클라이언트는 모든 실패를 알지못해서). 각 레코드에 다시 주어진 오프셋은 항상 -1로 설정됩니다. acks = 1 이것은 리더가 자신의 로컬 로그에 기록을 작성할 수 있지만 모든 팔로워에서 전체 확인을 기다리지 않고 응답한다는 의미입니다. 이 경우 리더는 기록을 확인후에 즉시 실패하지만 팔로워가 다음 복제전에 기록은 삭제됩니다. acks = all 이것은 리더가 기록을 확인하기 위해 동기 복제의 전체 세트를 기다린다는 의미입니다. 이것은 기록이 적어도 동기복제가 남아있는한 손실되지 않는다는 것을 보장합니다. 이것은 가장 강한 이용가능한 보장입니다.  acks=2와 같은 다른 설정들은 또한 가능하고, 주어진 확인의 수를 요구하지만 일반적으로 이것은 덜 유용합니다. 
buffer.memory | long | 33554432 | high | 프로듀서 메모리의 전체 바이트는 서버로 전송 대기중인 버퍼 기록을 사용할 수 있습니다. 기록이 서버에 그들을 보내는 것보다 더 빠르게 보내면 프로듀서는 막거나 block.on.buffer.full에 의해 명시한 선호에 기반한 예외를 쓰로우 합니다. 이 설정은 프로듀서가 사용하는 전체 메모리에 대략 일치해야하지만, 버퍼링을 위해 프로듀서가 모든 메모리를 사용하지 않기 때문에 하드 바인딩하지 않습니다. 일부 추가 메모리는 기내 요청 유지뿐만 아니라 압축(압축이 가능한 경우)에 사용되어집니다. 
compression.type | string | none | high | 프로듀서에 의해 생선된 모든 데이터에 대한 압축 타입. 디폴트는 none 입니다(즉,압축 없음). 유효한 값은 none,gzip,또는 snappy입니다. 압축은 데이터의 전체 배치라서, 배치의 효능은 또한 압축 비율에 영향을 줍니다(더 배치하는 것은 더 나은 압축을 의미). 
retries | int | 0 | high | 0보다 큰 값을 설정하는 것은 클라이언트가 잠재적으로 일시적인 에러와 함께 실패를 보내는 누군가의 아무 기록을 다시 보내게 합니다. 이 재시도는 클라이언트가 에러를 받을 때 기록을 다시 보내는 경우와 다르지 않습니다. 두개의 기록이 하나의 파티션에 보내는 경우 때문에 재시도를 허용하는 것은 잠재적으로 기록의 순서를 바꾸고, 첫 번째는 실패하고 재시도하지만, 두 번째는 성공하고나서,두번째 기록이 첫번째로 나타날 수 있습니다. 
batch.size | int | 16384 | medium | 프로듀서는 다수의 기록이 같은 파티션에 보내질때마다 더 적은 수의 요청으로 배치 기록을 시도합니다. 이것은 클라이언트와 서버 모두의 성능을 도와줍니다. 이 설정은 바이트에 디폴트 배치 크기를 조절합니다. 어떠한 시도도 이 크기보다 더 큰 배치 기록을 만들 수 없습니다. 브로커에 보낸 요청은 다수의 배치, 전송 될 수 있는 데이터와 각 파티션에 하나를 포함합니다. 작은 배치 크기는 덜 일반적이게 배칭을 만들고 처리량을 줄일 것입니다(제로의 배치크기는 완전히 배치 해제됩니다). 매우 큰 배치 크기는 우리가 항상 추가기록의 예상에 지정된 배치 크기의 버퍼를 할당하는 것과 같이 좀 더 쓸데없이 메모리를 사용합니다. 
client.id | string | | medium | 요청을 할 때 서버에 전달하는 id 스트링. 이것의 목적은  요구에 포함되는 논리 어플리케이션 이름을 허용함으로써 단지 아이피/포트 이후 요청의 소스를 추적할 수 있어야 합니다. 그 어플리케이션은 아무 로깅과 통계 이외의 기능적인 목적이 없어서 이것이 원하는 스트링으로 설정합니다. 
linger.ms | long | 0 | medium | 하나의 일괄 요청에 대한 전송요청 사이에 도착한 아무 기록과 결합한 프로듀서 그룹. 일반적으로 이것은 레코드가 발송될 수 있는 것보다 더 빨리 도착할때 로드아래에서만 발생합니다. 그러나 어떤 경우에는 클라이언트는 심지어 보통의 로드아래에서 요청횟수를 줄이기를 원합니다. 이 설정은 이것을 인공 지연의 적은양을 추가하면서 해냅니다-즉,바로 기록을 보내기 보다는 프로듀서가 전송이 같이 배치되도록 다른 기록들이 전송되기 위해 주어진 지연까지 기다릴 것 입니다. 이것은 TCP에 네이글의 알고리즘과 유사하다고 생각할 수 있습니다. 이 설정은 배치에 대한 지연에 상한을 제공 : 우리가 파티션에 대한 기록의 배치 크기 가치를 얻을 때 이것은 바로 설정에 관계없이 보내질 것입니다,하지만 이 파티션에 대한 축적된 많은 바이트들이 우리가 가진 것보다 적으면 우리는 지정된 시간동안 보여주기 위한 더 많은 기록을 기다리기 위해 '링거(linger)'할 것입니다. 이 설정은 0으로 기본값을 설정합니다(즉,지연 없음). 예를들어, linger.ms=5로 설정하는 것은 전송요청횟수를 감소시키는 효과를 얻을수 있지만 로드의 absense에 전송기록 지연의 5ms까지 추가할 수 있습니다. 
max.request.size | int | 1048576 | medium | 요청의 최대크기. 또한 이것은 효과적으로 최대 기록 크기에 덮어씌웁니다. 이것과 다른 기록 크기에 서버는 자신의 캡을 가지고 있습니다. 이 설정은 생산자가 큰 요청을 전송하지 않기 위해 한 요청에 전송할 수 있는 기록 배치의 수를 제한합니다. 
receie.buffer.bytes | int | 32768 | medium | 데이터를 읽을 때 쓰는 TCP 리시브 버퍼의 크기.  send.buffer.bytes | int | 131072 | medium | 데이터를 전송할 때 쓰는 TCP 센드 버퍼의 크기. 
timeout.ms | int | 30000 | medium | 이 설정은 프로듀서가 acks 설정으로 지정한 승인 요청을 만족시키기 위해 서버가 팔로워로부터 승인을 기다리는 최대시간을 조절합니다. 타임아웃이 지날 때 요청된 승인수가 충분하지 않으면 에러가 반환됩니다. 이 타임아웃은 서버측에서 측정되고, 요청 네트워크 지연에 포함되지 않습니다. 
block.on.buffer.full | boolean | true | low | 우리의 메모리 버퍼가 소진될 때 우리는 새 기록(블록)을 받는 걸 멈추거나 에러를 보내야 합니다. 기본적으로 이 설정은 true이고 우리는 차단합니다,하지만 일부 시나리오에서는 막는 것은 바람직하지 않고 바로 에러를 보내는 것이 더 좋습니다. 이것을 false로 설정하는 것은 그것을 이뤄낼 것입니다 : 기록이 전송되고 버퍼공간이 가득 차면 프로듀서가 BufferExhaustedException을 보냅니다. 
metadata.fetch.timeout.ms | long | 60000 | low | 처음 데이터는 우리가 어느 서버 호스트가 토픽의 파티션인지 알기 위해서 그 토픽에 대한 메타데이터를 패치해야하는 토픽에 전송됩니다. 이 설정은 클라이언트에게 다시 예외를 보내기 전에 우리가 성공 패치 메타데이터 를 기다리는 것을 차단하는 최대시간을 조절합니다. 
metadata.max.age.ms | long | 300000 | low | 비록 우리가 앞서는 어떤 파티션 리더십 변화를 못 볼지라도 우리가 메타데이터의 새로고침을 강요받은 이후의 밀리초 시간은 새로운 아무 브로커나 파티션들을 발견합니다. 
metric.reporters | list | [] | low | 메트릭 리포터로 사용하는 클래스의 목록. MetricReporter 인터페이스를 구현하는 것은 새로운 메트릭 생성을 알려주는 클래스에 연결할 수 있습니다. JMxReporter는 항상 JMX통계등록을 포함합니다.
metrics.num.samples | int | 2 | low | 메트릭을 계산하기 위해 유지한 샘플의 수. 
metrics.sample.window.ms | long | 30000 | low | 메트릭스 시스템은 고정된 윈도우 크기를 넘어 변경가능한 샘플의 수를 유지합니다. 이 설정은 윈도우의 크기를 조절합니다. 예를 들어 우리는 각각 30초 동안 측정된 두 개의 샘플을 유지할 수 있습니다. 윈도우가 만료될 때 우리는 지우고 가장 오래된 윈도우를 덮어 씁니다. 
reconnect.backoff.ms | long | 10 | low | 연결이 실패할때 주어진 호스트를 재연결하는 시도전 기다리는 시간. 이것은 클라이언트가 반복적으로 타이트 루프에서 호스트에 연결을 시도하는 경우를 방지합니다. 
retry.backoff.ms | long | 100 | low | 주어진 토픽파티션에 실패한 생산 요청을 재시도하기 전에 기다리는 시간. 이것은 타이프 루프에서 전송-실패 반복을 방지합니다. 

### 4. 디자인

#### 4.1 동기부여

우리는 카프카를 [큰 회사가 가지고 있는](http://kafka.apache.org/documentation.html#introduction) 모든 실시간 데이터 피드를 다루는 통합된 플랫폼으로서 활동할 수 있도록 디자인합니다. 이것을 하기 위해서 우리는 사용 사례의 상당히 광범위한 세트를 통해 생각했습니다.  

이것은 실시간 로그 집계와 같은 높은 볼륨 이벤트 스트림을 지원하기 위해 높은 처리량을 가져야 합니다.  


이것은 오프라인 시스템으로 부터 정기적인 데이터 로드를 지원할 수 있는 큰 데이터 백로그로 적절하게 처리해야합니다.  

이것은 또한 시스템이 더 전통적인 메세징 사용사례를 다루기 위해 낮은 지연 배달을 처리하는 것을 의미합니다.  

우리는 파티션,분산,이 새로운 것을 생성하는 피드들의 실시간 처리,파생 피드를 지원해주길 원합니다. 이것은 우리의 파티셔닝 과 컨슈머 모델에 동기부여합니다.  

마지막 스트림이 일하기 위해 다른 데이터 시스템에 공급되는 경우에,우리는 기계 고장의 존재에 고장허용범위를 새로운 시스템으로 보장해야합니다.  

마지막 스트림이 일하기 위해 다른 데이터 시스템에 공급되는 경우에,우리는 기계 고장의 존재에 고장허용범위를 새로운 시스템으로 보장해야합니다. 우리는 다음 섹션에서 디자인의 몇가지 요소를 소개할 것입니다.  

#### 4.2 고집

##### 파일 시스템을 두려워 하지 마세요!

카프카는 크게 저장 및 메세지를 캐싱 하는 파일시스템에 의존합니다. 집요한 구조가 경쟁력 있는 성능을 제공할 수 있는 의심많은 사람들이 만든 "디스크는 느리다"라는 일반적인 인식이 있습니다. 사실 디스크는 사용하는 방법에 따라 사람들이 기대한 것보다 더 빠르거나 더 느릴수 있습니다; 그리고 적절히 디자인한 디스크 구조는 종종 네트워크처럼 빠르게 할 수 있습니다.  

디스크 성능에 대한 중요한 사실은 하드드라이브의 처리량이 지난 십년동안 디스크 시크의 지연으로 나뉜 것입니다. 결과적으로 선형성능은 여섯 개의 7200rpm SATA RAID-5 배열은 약 600MB/sec 이지만 임의의 쓰기의 성능은 오직 약 100k/sec-6000X이상의 차이와 함께 [JBOD](http://en.wikipedia.org/wiki/Non-RAID_drive_architectures)설정에 기록됩니다. 이 선형 읽기와 쓰기는 예측가능한 모든 사용패턴의 대부분이고,운영체제에 의해 활용됩니다. 최신 운영체제는 미리 읽기와 큰 피지컬 쓰기에 큰 블록 배수와 더 작은 그릅 논리적 쓰기에 데이터를 프리패치하는 나중에쓰기 기술들을 제공합니다. 이 문제에 대한 더 자세한 내용은 [ACM Queue article](http://queue.acm.org/detail.cfm?id=1563874)에서 찾을 수 있습니다; 그들은 실제로 [어떤 경우에는 순차적인 디스크 접근이 무작위 메모리 접근보다 더 빠를 수 있는 것을](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg) 발견합니다.  

이 성능 차이의 보상을 위해서 최신 운영체제는 그들의 디스크 캐싱을 위한 메인 메모리의 사용에 점점더 적극적이되었습니다. 메모리가 복구될 때 최신 OS는 적절히 *모든* 사용가능한 메모리를 적은 성능 패널티를 가진 디스크 캐시로 전환합니다. 모든 디스크 읽기와 쓰기는 이 통합된 캐시를 통해 이동합니다. 이 특징은 다이렉트 I/O 사용없이 쉽게 해제할 수 없어서,프로세스가 데이터의 처리중인 캐시를 유지하고 있어도,이 데이터는 효과적으로 모든 것을 두 번 저장하는 OS 페이지 캐시에 중복됩니다.  

게다가 우리는 JVM의 상단에 구축하고,자바 메모리 사용에 아무때나 보낸 누구나 두 가지를 압니다:

1. 객체의 메모리 오버헤드는 종종 매우 높은,저장된 데이터의 크기를 두 배로 합니다(나쁘거나).
2. 자바 가비지 콜렉션은 점점 힙 데이터가 증가함에 따라 더 성가시고 느려집니다.

이런 요소의 결과로서 페이지캐시에 의존하고 파일시스템을 사용하는 것은 다른 구조 또는 메모리 내부 캐시를 유지하는 것보다 더 우수합니다-우리는 적어도 모든 사용가능한 메모리에 자동으로 접근함으로써 이용가능한 캐시를 더블하고,개인 객체보다 콤팩트 바이트 구조를 저장함으로써 다시 더블합니다. 그렇게하는것은 GC패널티없이 32GB 기계에 28-30GB까지하는 최대캐시가 발생합니다. 게다가 이 캐시는 서비스가 다시시작하더라도,따뜻하게 유지하는 반면에 처리중인 캐시가 메모리에 다시 구축(10GB캐시에 10분이 걸리는)하거나 이것은 완전히 차가운 캐시로 시작해야합니다(심각한 초기 성능을 비슷하게 의미하는). 이것은 또한 크게 단 한번의 처리중인 시도보다 더 효과적이고 더 정확하게 하는 경향이 있는 OS에 지금 있는 캐시와 파일시스템 사이의 일관성을 유지하기 위한 모든 로직으로써 코드를 간단하게 합니다. 너의 디스크 사용 호의 선형 읽기 즉 미리읽기는 효과적으로 각 디스크 읽기에 유용한 데이터와 함께 이 캐시를 미리 덧붙입니다.  

이것은 매우 간단한 디자인을 제공: 메모리 내 가능한 한 유지하고 우리가 공간을 다 썼을 때 패닉에 파일시스템을 모두 보내는것 보다는 우리가 그것을 바꿉니다. 모든 데이터는 바로 디스크에 플러시를 하지 않고 파일시스템에 대한 지속적인 로그에 기록됩니다.실제로 이것은 커널의 페이지캐시로 전환한다는 걸 의미합니다. 페이지 캐시 중심 디자인의 이 스타일은 여기에 바니시 디자인 [문서](https://www.varnish-cache.org/trac/wiki/ArchitectNotes)에 설명됩니다(오만의 많은 양과 함께).  

##### 일정 시간 충분

메세징 시스템에서 사용하는 지속적인 데이터 구조는 종종 메세지에 대한 메타데이터를 유지하기 위해https://translate.google.co.kr/서 연관된 BTree 또는 다른 범용 랜덤 접근 데이터 구조를 갖는 컨슈머당 큐이다. BTree는 가장 다양한 데이터 구조를 사용하고, 메세징 시스템에서 트랜잭션과 비트랜잭션 의미론의 다양하게 지원하도록 만듭니다. 하지만 그들은 꽤 높은 비용이 든다: BTree 옵션은 O(로그 N). 보통 O(로그 N)은 일정시간에 본질적으로 동등한 것으로 간주되지만,이것은 디스크 작동에 대해 사실이 아닙니다. 디스크 탐색은 come at 10ms a pop,그리고 각 디스크는 병렬화가 제한적이라서 한번에 오직 한 탐색만 수행합니다. 이런 이유로 디스크의 적은 경우에도 매우 높은 오버헤드를 초래합니다. 매우 느린 피지컬 디스크 작동으로 저장 시스템이 매우 빠르게 캐시된 작업을 섞기 때문에, 트리구조의 관찰 성능은 종종 고정된 캐시 데이터 증가에 따라 초선형됩니다.-즉, 너의 데이터를 두 배로 하는 것은 그것을 두 배나 더 느리게 더 악화시킵니다.  

직관적으로 지속적인 큐는 단순한 읽기에 구축되고 흔한 로그 해결책의 경우와 같이 파일에 추가합니다. 이 구조는 모든 작업이 O(1)이고 읽기는 기록 또는 서로를 막지않는 이점을 가집니다. 성능이 데이타 크기로 부터 완전히 분리되어 있기 때문에 이것은 명백한 성능 이점을 가집니다.-한 서버는 지금 완전 싸고,낮은 회전 속도 1+TB SATA 드라이브의 수를 이용합니다. 그들이 낮은 탐색성능을 가지고 있지만,이 드라이브는 큰 읽기와 쓰기에 대한 허용할 수 있는 성능을 가지고 있고, 3/1가격과 3배의 용량을 가져옵니다.  

어떤 성능 패널티없이 거의 무제한 디스크 공간에 접근하는 것은 우리가 메세징 시스템에서 흔히 발견되지 않는 어떤 특징을 제공하는 것을 의미합니다. 예를 들어,카프카에서,그들이 사용하자마자 메세지를 지우는 시도 대신에 우리는 메세지를 상대적으로 긴 시간(예를 들어 한 주 같이)동안 보유할 수 있습니다. 우리가 설명하는 동안 이것은 컨슈머에 대한 다량의 유연성으로 이어집니다.

#### 4.3 효율성

우리는 효율성에 대해 상당한 노력을 했습니다. 우리의 주된 사용사례 중 하나는 매우 높은 용량의 웹 활동 데이터를 다루는 것입니다: 각 페이지 뷰는 많은 쓰기를 생성합니다. 게다가 우리는 적어도 한 컨슈머에 의해(가끔 많은) 게시된 각 메세지가 준비되었다고 추정합니다,이런 이유로 우리는 가능한 싸게 소비하기 위해 노력합니다.  

우리는 또한 비슷한 시스템 수를 경험 구축과 실행하면서 효율이 효과적인 멀티 테넌트 작업의 핵심이라는걸 발견합니다. 다운스트림 인프라 서비스가 어플리케이션 사용에 의해 작은 범프에 병목이 쉽게 될 수 있다면,그런 작은 변화는 종종 문제를 일으킵니다. 매우 빠른 것을 통해 우리는 인프라 전 로드아래 어플리케이션이 전도된다고 보장하는 걸 도와줍니다. 이것은 수십,수백 개의 어플리케이션을 사용패턴이 거의 매일 일어나는 변화로 중앙집권된 클러스터에 지원하는 중앙집권된 서비스를 운영할 때 특히 중요합니다.  

우리는 이전 버전에서 디스크 효율성을 얘기했습니다. 안좋은 디스크 접근 패턴이 제거될 때, 이 시스템의 유형에 두 개 비효율성의 일반적인 원인이 있습니다: 너무 많은 소규모 I/O 작업,그리고 과도한 바이트 복사.  

소규모 I/O 문제는 클라이언트와 서버사이와 서버의 고유 지속적인 작동에 모두 일어납니다.  

이를 피하기 위해,프로토콜은 자연스럽게 그룹 메세지를 함께 "메세지 세트"추상화를 중심으로 구축합니다. 이것은 그룹 메세지에 대한 네트워크 요청을 함께 할 수 있어 한번에 한 메세지를 보내는 것보다 네트워크 왕복 오버헤드를 상환합니다. 순차적으로 서버가 1회에 로그 메세지 청크를 추가하고, 컨슈머는 큰 선형 청크를 한 번에 가져옵니다.  

이 단순한 최적화는 속도 증가 수를 생성합니다. 배칭은 카프카를 컨슈머에게 흐르는 선형 쓰기로 임의 메세지 쓰기의 버스티 스트림으로 바꾸는 더 큰 네트워크 패킷,더 큰 순차 디스크 작업, 인접한 메모리 블록 등등에 이르게 합니다.  

다른 비효율성은 바이트 복사에 있습니다. 낮은 메세지 속도에 이것은 문제가 아니라, 로드아래 영향이 중요합니다. 이것을 피하기 위해 우리는 프로듀서,브로커,컨슈머에 의해 공유된 기준화 된 바이너리 메세지 형식을 사용합니다(그래서 데이터 청크는 그들 사이의 개조없이 전송할 수 있습니다).  

브로커에 의해 유지된 메세지 로그는 프로듀서와 컨슈머가 사용하는 것과 같은 형식으로 디스크에 기록되는 메세지 세트의 순서로 덧붙여지는 자체 파일 디렉토리일뿐입니다. 이 흔한 포맷을 유지하는 것이 가장 중요한 작업의 최적화를 가능: 지속적인 로그 청크의 네트워크 전송. 최신 유닉스운영체제가 소켓에 페이지캐시에서 데이터를 전송하기 위한 몹시 최적화된 코드경로를 제공합니다; 리눅스에서 이것은 [sendfile 시스템 호출](http://man7.org/linux/man-pages/man2/sendfile.2.html)에서 수행됩니다.  

전송파일의 영향을 이해하기 위해서,파일에서 소켓으로 데이터를 전송하는 일반적인 데이터 경로를 이해하는 것은 매우 중요합니다:

1. 운영시스템은 커널 공간에서 페이지 캐시 디스크에서 데이터를 읽습니다.
2. 응용프로그램은 유저 공간 버퍼 커널 공간에서 데이터를 읽습니다.
3. 응용프로그램은 소켓 버퍼에 커널 공간으로 다시 데이터를 작성합니다.
4. 운영 시스템은 네트워크를 통해 전송되는 소켓 버퍼에서 NIC버퍼로 데이터를 복사합니다.  

이것은 분명히 비효율적이고, 4개의 복사본과 2개의 시스템 호출이 있습니다. 전송파일을 사용해서,재복사는 OS가 데이터를 페이지캐시에서 네트워크로 즉시 데이터를 전송하게함으로써 예방합니다. 그래서 이 최적화된 경로에서, NIC버퍼에 유일한 최종복사는 필요합니다.  

우리는 일반적인 사용사례는 주제에 다수의 컨슈머를 기대합니다. 위의 제로 복사 최적화를 사용하면서,데이터는 정확히 한번 페이지캐시에 복사되고 메모리에 저장하고 읽을 때마다 커널 공간에 복사하는 대신에 각 사용할때마다 재사용됩니다. 이것은 메세지가 네트워크 연결의 제한에 접근하는 비율로 써지도록 허용합니다.  

페이지캐시와 전송파일의 조합은 컨슈머가 많이 있는 카프카클러스터에서 너가 캐시에서 완전한 데이터를 제공하도록 디스크에 완전 읽기 작업을 볼 수 없다는 것을 의미합니다.  

자바에서 전송파일과 제로복사지원에 대한 자세한 배경정보는 이 [기사](https://www.ibm.com/developerworks/linux/library/j-zerocopy/)를 참조하세요.  

##### 엔드-투-엔드 배치 표현

어떤 경우에는 병목 현상은 실제로 CPU나 디스크에 없지만 네트워크 대역폭에는 있습니다. 이것은 특히 광역 네트워크를 통한 데이터 센터 사이에서 메세지를 보낼 필요가 있는 데이터 파이프라인에 적용됩니다. 물론 유저가 카프카에서 필요한 어느 지원없이 항상 한번에 하나의 메세지를 압축할 수 있지만,이것은 같은 유형(예를 들어, 웹 로그 또는 흐한 스트링 값에 JSON이라는 필드 이름 또는 유저 에이전트)의 메세지 사이의 중복때문에 많은 감소로 매우 안좋은 압축률을 초래합니다. 효율적인 압축은 각 메세지를 개별로 압축하는 것보다 다양한 메세지를 압축하는 것을 요구합니다.  

카프카는 재귀 메세지 세트를 허용함으로써 이것을 지원합니다. 이 형식에서 메세지의 배치는 압축하고 서버에 보내는 것을 같이 모읍니다. 이 메세지의 배치는 압축 형식으로 기록된 로그에 압축 된 상태로 남아있는 유일한 소비자로 압축됩니다.  

카프카는 GZIP과 Snappy 압축 프로토콜을 지원합니다. 자세한 내용은 [여기](https://cwiki.apache.org/confluence/display/KAFKA/Compression)에서 찾을 수 있습니다.

### 4.4 프로듀서

##### 로드 밸런싱

프로듀서는 데이터를 어떤 내정간섭 라우팅 타이어 없이 파티션의 리더인 브로커에 바로 데이터를 전송합니다. 프로듀서가 이것을 하는 걸 도와주기 위해서 모든 카프카 노드들은 서버들이 살아있고 토픽의 파티션 리더가 언제든지 요청에 적절하게 지시할 수 있는 메타데이터에 대한 요청에 답할 수 있습니다.  

클라이언트는 메세지를 게시하는 파티션을 제어합니다. 이것은 랜덤 로드 밸런싱 같은 걸 구현하면서 무작위로 수행되거나 이것은 몇몇 의미론적 분할 기능에 의해 수행됩니다. 우리는 유저가 키를 파티션에 명시하는 걸 허용하고 이것을 파티션 해시에 사용함으로써 의미론적 분할에 대한 인터페이스를 공개합니다(또 필요한 경우 파티션 기능을 오버라이드하는 작동이 있습니다). 예를 들어 선택된 키가 유저id인 경우 주어진 유저에 대한 모든 데이터는 같은 파티션에 보내질 수 있습니다. 결국 이것은 컨슈머가 그들의 소비에 대한 일부를 장악할 수 있습니다. 이 분산스타일은 명백하게 컨슈머에게 일부의 민감한 처리를 가능하도록 설계되었습니다.  

##### 비동기 전송

배칭은 효율의 큰 요인 중 하나이며, 카프카 프로듀서를 배칭하는 걸 가능하게 하는 것은 메모리에 데이터를 축적하고 단일 요청에 더 큰 배치들을 전송하는 비동기 모드입니다. 배칭은 고정된 메세지의 수보다 크지않게 축적하고 몇몇의 고정된 지연 바운드보다 길지 않은 시간을 기다리도록 설정되어집니다(예를 들어100메시지들 또는 5초). 이것은 서버에 몇몇의 더 큰 I/O 작동과 더 많은 바이트의 축적을 보내는 걸 가능하게 합니다. 클라이언트에서 이 버퍼링이 발생하면 이것은 어느 데이터가 메모리에 버퍼되고 아직 보내지지 않은 것이 프로듀서 충돌상황에서 손실되므로 분명히 내구성을 줄입니다.  

카프카 0.8.1에서 비동기 프로듀서는 전송 에러를 잡기 위한 레지스터 핸들러에 사용되어지는 회신을 가지고 있지 않습니다. 이런 회신 기능의 추가는 카프카 0.9에 제안, [제안된 프로듀서 API](https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite#ClientRewrite-ProposedProducerAPI)를 참조

#### 4.5 컨슈머

카프카 소비자는 소비하길 원하는 파티션을 이끄는 브로커에 "fetch" 요청을 발표함으로써 작동합니다. 소비자는 각 요청에 대한 로그에 그것의 오프셋을 지정하고 그 위치에서 시작하는 로그의 청크를 다시 받습니다. 그래서 컨슈머는 중요한 위치를 통해 제어하고 필요할 때 데이터를 다시쓰기 위해서 되감을 수 있습니다.  

##### 푸시 VS 풀  

우리가 고려한 초기 조건은 소비자가 데이터를 브로커에서 당기거나 브로커가 소비자에게 데이터를 보내는지 여부입니다. 이런 점에서 카프카는 데이터가 프로듀서로 부터 브로커에 밀고, 소비자에 의해 브로커에서 당기는 대부분의 메세징 시스템에서 공유된 더 전통적인 디자인을 따릅니다. [스크라이브(Scribe)](https://github.com/facebookarchive/scribe)와 [아파치 플럼(Apache Flume)](http://flume.apache.org/)과 같은 몇몇의 로깅 중심적인 시스템은 하류로 데이터가 보내지는 경로에 기초한 매우 다른 푸시를 따릅니다. 이런 처리방법은 장점과 단점이 있습니다. 그러나 푸시 기반 시스템은 브로커가 데이터를 전송하는 속도를 제어하는 동안에 다양한 컨슈머를 다루는 데 어려움이 있습니다. 목표는 소비자가 최대가능속도로 소비 할 수 있도록 하는 일반적인 것입니다; 불행히도 푸시시스템에서 소비속도가 생산속도(본질적으로 서비스 공격 거부)보다 떨어질때 이것은 컨슈머가 압도하는 경향이 있다는 걸 의미합니다. 풀 기반 시스템은 소비자가 뒤쳐질때 캐치업하는 더 나은 특징이 있습니다. 이것은 소비자가 이것이 압도하고 있다고 가리키는 백오프 프로토콜의 몇가지로 줄일 수 있지만, 소비자에게 충분히 활용(활용을 오버하지않는)할 수 있는 전송속도를 얻는 것은 보이는 것보다 어렵습니다. 이 방식으로 구축시스템에서의 이전 시도는 우리를 더 전통적인 풀모델로 이끕니다.  

풀기반 시스템의 다른 이점은 그것이 소비자에게 전송되는 데이터의 배치에 적극적인 것입니다. 푸시기반 시스템은 바로 요청을 전송하거나 더 데이터를 모으고나서 그것을 하류 소비자가 바로 처리할 수 있는지에 대한 인식없이 나중에 전송하는 것 중 하나를 선택해야합니다. 낮은 지연을 위해 조절하는 경우 이것은 낭비하는 곳에 버퍼링되기 때문에 전송을 위해 한번에 한 메세지를 보낼 수 있습니다. 풀기반 디자인은 이것을 로그의 현재위치 다음(또는 일부 설정 가능한 최대 크기까지)에 소비자가 항상 메세지를 당기는걸 가능하도록 고칩니다. 그래서 하나는 불필요한 지연 도입없이 최상의 배칭을 얻습니다.  

소박한 풀기반 시스템의 결함은 브로커가 데이터가 없을 때 소비자가 타이트루프에 효과적으로 도착하기를 기다리는 데이터에 대한 폴링을 그만둔다는 것입니다. 이것을 피하기 위해서 우리는 소바자 요구가 데이터가 도착할 때까지 기다리는 "롱 폴"에서 차단할 수 있도록 풀 요청에 대한 파라미터를 가집니다(그리고 주어진 바이트의 수가 큰 전송 크기를 확보하는 데 사용할 수 있을때까지 선택적으로 대기).  

너는 유일하게 엔드-투-엔드 풀을 할 수 있는 다른 가능한 디자인을 상상할 수 있습니다. 프로듀서는 로컬 로그에 작성하고,브로커는 소비자가 그들로부터 당긴걸 당길 것이다. "저장 및 전달"프로듀서와 비슷한 유형은 종종 제안됩니다. 이것은 흥미롭지만 우리는 수천의 프로듀서를 가지는 우리의 목표 사용사례에 적절하지 않다고 느꼈습니다. 스케일에 지속적인 데이터 시스템을 운영하는 우리의 경험은 우리를 많은 어플리케이션을 통한 시스템에 있는 수천의 디스크를 포함하는 것이 실제로 그것을 더 믿지 못하게 하고 작동에 대한 악몽을 느끼도록 이끕니다. 그리고 실제로 우리는 우리가 프로듀서 지속성의 필요없이 크고 강한 SLAs와 파이프라인을 운영하는 걸 발견할 수 있습니다.  

##### 컨슈머 입장

무엇이 사용되었는지 기록하는 것은 놀랍게도 메세징 시스템의 중요한 성능 요점입니다.  

대부분의 메세징 시스템은 어떤 메세지가 브로커에 사용되었는지에 대한 메타데이터를 유지합니다. 즉, 메세지가 소비자에게 배포될때, 브로커는 근처에 바로 기록하거나 소비자의 승인을 기다릴 것입니다. 이것은 꽤 이해하기 쉬운 선택이고,실제로 단일 기계 서버를 위해 이것은 이 상태가 갈 수 있는 곳이 명백하지 않습니다. 많은 메세징 시스템에 저장하는데 사용되는 데이터구조가 덜 변경되기 때문에,이것은 또한 실용적인 선택입니다--브로커가 작은 데이터 크기를 유지하면서 어떤 것이 바로 이것을 제거하는 데 사용되는지 알고 있기 때문입니다.  

아마도 어떤 것이 분명하지 않다는 것은 사소한 문제가 아닌 것이 소비되었는지에 대한 협정에 작용하는 브로커와 컨슈머를 얻는 것입니다.(What is peraps not obvious,is that getting the broker and consumer to come into agreement about what has been consumed is not a trivial problem.) 브로커가 메세지를 **쓸때**마다 바로 기록하는 경우 이것은 네트워크를 통해 나눠지고 나서 소비자가 메세지(예를 들어 그것이 충돌하거나 요청시간이 끝나거나 등등 때문에)처리를 실패하면 메세지는 손실됩니다. 이런 문제를 해결하기 위해서,많은 메세징 시스템들은 메세지를 보낼때 **사용**되지 않고 **보냄**으로만 메세지가 표시되는 것을 의미하는 응답 기능을 추가합니다; 브로커는 **소비되었다고** 메세지를 기록하기 위해 소비자로부터 특정 승인을 기다립니다. 이 전략은 손실메세지의 문제를 고치지만, 새로운 문제를 만듭니다. 먼저, 이것이 인식을 보내기전에 소비자가 메세지를 처리하면 메세지는 두번 사용합니다. 두번째 문제는 성능주위에서,지금 브로커는 모든 단일 메세지의 여러상태를 유지해야합니다(처음으로 이것을 잠그면 두번째 시간을 제공하지않고,이것을 제거하도록 이것을 영구적으로 사용할 수 있게 표시합니다).곤란한 문제는 전송되지만 인식되지않는 메세지를 어떻게 하는지와 같이 다뤄져야 합니다.  

카프카가 이것을 다르게 다룹니다. 우리의 토픽은 어떤 주어진 시간에 한 소비자에 의해 소비되는 각각의 완전히 정렬된 파티션의 세트루 나눠집니다. 이것은 각 파티션에 소비자의 위치가 다음 메세지의 오프셋을 소비하는 단지 하나의 정수인것을 의미합니다. 이것은 각 파티션에 아주 작은 하나의 수를 소비되었는지에 대한 상태를 만듭니다. 이 상태를 주기적으로 점검합니다. 이것은 메세지 응답의 동등한 것들을 싸게 만듭니다.  

결정의 측면 혜택이 있습니다. 소비자는 고의로 오래된 오프셋에 다시 *되감기* 할 수 있고 데이터를 다시 사용할 수 있습니다. 이것은 큐의 계약에 위반하지만,많은 소비자를 위한 기본적인 특징인 것으로 나타납니다. 예를 들어, 소비자 코드가 버그를 가지고 있고,몇개의 메세지들이 사용된 후에 발견되면,소비자는 그 메세지들을 버그가 고쳐지면 다시 사용할 수 있습니다.  

##### 오프라인 데이터 로드

확장 가능한 지속성은 하둡 또는 상관있는 데이터 웨어하우스와 같은 주기적으로 대부분 데이터를 오프라인 시스템으로 로드하는 배치 데이터 로드와 같은 주기적으로 소비만 하는 소비자의 가능성을 허용합니다.  

하둡의 경우 우리는 로드에 풀 병렬화를 허용하면서 각 노드/토픽/파티션 조합의 한개와 같은 개인 맵 작업을 통해 로드를 쪼갬으로써 데이터 로드를 평행하게 합니다. 하둡은 업무 관리를 제공하고 중복 데이터의 위험 없이 실패한 업무를 다시 시작할 수 있습니다.-그들은 단순히 그들의 원래 위치에서 재시작합니다.

##### 4.6 메세지 전달 의미론

이제 우리는 생산자와 소비자가 어떻게 작동하는지에 대해 조금은 이해할 수 있습니다,생산자와 소비자 사이의 카프카가 제공하는 의미론적 보장을 알아보자. 분명히 제공할 수 있는 여러 가능한 메세지 전달 보장이 있습니다:  

* 최대 한번-메세지들은 손상되지만 절대 다시 전달되지않습니다.
* 적어도 한번-메세지들은 절대 손상되지 않지만 다시 전달될 수도 있습니다.
* 정확히 한번-이것은 사람들이 실제로 무엇을 원하는지,각 메세지는 한번만 전달송됩니다.

이것은 두 가지 문제로 나뉩니다: 게시한 메세지의 내구성 보장과 메세지를 사용할때의 보장.  

많은 시스템은 "정확히 한번"전송 의미론을 제공한다고 주장하지만,그것은 작은 글씨를 읽는 것이 중요하고,이런 주장의 대부분은 잘못인도합니다(예를 들어 그들은 소비자 또는 생성자가 실패하는 경우 또는 다수의 소비자 프로세스들이 있는 경우,또는 디스크에 기록된 데이터가 손상되는 경우에 변환되지 않습니다).  

카프카의 의미는 직진(straight-forward)입니다. 메세지가 게시될 때 우리는 메세지의 개념이 로그에 "커밋"합니다. 게시한 메세지가 커밋되면 이것은 "살아있는"으로 쓰여진 메세지에 파티션을 복제한 브로커가 있는 한 손상되지 않을것입니다. 살아있는것의 정의뿐만 아니라 우리가 다루기 위해 시도한 실패의 어느 타입의 서술에 대한 자세한 내용은 다음 섹션에 설명할 것입니다. 지금은 완벽한 무손실 브로커를 추정하고 생산자와 소비자에 대한 보증을 이해하라. 프로듀서가 메세지를 게시하기 위해 시도하고 네트워크 에러를 경험한다면 이것은 이 에러가 메세지가 커밋된 전후로 발생한다면 확실하지 않습니다. 이것은 자동생성된 키로 데이터베이스 테이블에 입력하는 의미와 비슷합니다.  

이것들은 게시자들을 위한 강한 가능한 의미는 아닙니다. 비록 우리가 네트워크 에러의 경우 무슨일이 일어날지 확실히 할 수 없을지라도, 이것은 생산자가 생산 요구 멱등을 다시시도하는 "기본 키"의 종류를 생성시킬 수 있습니다. 이 특징은 서버 실패의 경우에조차(또는 특히) 작동해야하는 과정 때문에 중복된 시스템에 대해 쉬운일이 아닙니다. 이 특징을 이용해서 우리가 메세지를 정확히 한번 게시된 보증을 하는 그 시점에서 성공적으로 커밋된 메세지의 응답을 받을때까지 다시 시도하도록 하는 생산자에 대해 충족시킬수 있습니다. 우리는 미래 카프카 버전에 이것이 추가되기를 바랍니다.  

모든 사용사례는 아니지만 그런 강한 보증을 요구합니다. 지연에 민감한 사용을 위해서 우리는 생산자가 바람직한 내구성 수준을 지정하는 걸 허용합니다. 프로듀서가 커밋된 메세지에 기다리는 것을 원한다고 지정한다면 이것은 10ms의 주문에 걸릴 수 있습니다. 그러나 프로듀서는 또한 완전히 비동기로 전송하거나 리더(팔로워가 필수적이지는 않음)가 메세지를 가질때까지만 기다리도록 지정할 수 있습니다.  

이제 소비자의 관점에서 의미론을 설명하자. 모든 복사본은 동일한 오프셋과 동일한 로그를 가집니다. 소비자는 이 로그의 위치를 제어합니다. 소비자가 충돌하지 않으면 이것은 단지 메모리에 이 위치를 저장하지만 ,소비자가 실패하고 우리가 다른 프로세스에 인수받기 위해 이 토픽 파티션을 원하면 새로운 프로세스는 처리를 시작하는 적절한 위치를 선택해야할 필요가 있습니다. 소비자가 어떤 메세지를 읽는 것을 얘기하자-이것은 메세지를 처리하고 그 위치를 업데이트하기 위한 몇 가지 옵션이 있습니다.  

1. 이것은 메세지를 읽을 수 있고, 로그에 그것의 위치를 저장하고, 최종적으로 메세지를 처리합니다. 이 경우에 이것의 위치를 저장한 뒤 소비자가 이것의 메세지 처리의 결과를 저장하기 전에 충돌을 처리하는 가능성이 있습니다. 이 경우에 처리를 인수받은 프로세스는 적은 메세지가 그 위치에 앞서 처리되지 않았더라도 저장된 위치에서 시작해야합니다.(In this case the process that took over processing would start at the saved position even though a few messages prior to that position had not been processed.) 이것은 "최대 한번"의미에 소비자 실패 메세지가 처리되지 않은 경우로 일치합니다.
2. 이것은 메세지를 읽을 수 있고,메세지를 처리하고,마지막으로 이것의 위치를 저장합니다. 이 경우에 메세지 처리 후 소비자가 이것의 위치를 저장하기 전에 충돌을 처리하는 가능성이 있습니다. 이 경우에 새 프로세스가 첫번째 적은 메세지를 인수받을때 이것은 이미 처리되어 있습니다.(In this case when the new process takes over the first few messages it receives will already have been processed.) 이것은 소비자 실패의 경우에 "적어도 한번"의미와 일치합니다. 많은 경우에 메세지는 업데이트가 멱등(같은 메세지를 두번 받는것은 단지 기록에 다른 복사본을 가지고 오버라이트합니다)이라서 기본 키를 가지고 있습니다.
3. 그래서 정확히 무엇에 하나 의미하나(예를 들어 너가 실제로 원하는 것)? 여기에서의 제한은 실제로 메세징 시스템의 특징이 아니라 실제로 출력으로 저장되는 것으로 소비자의 입장을 조정 할 필요가 없습니다. 이것을 얻는 고전적인 방법은 소비자 위치에 대한 저장과 소비자 결과에 대한 저장 사이의 두개의 면을 도입하기 위해 커밋합니다. 하지만 이것은 단순히 출력과 같은 위치에 오프셋 소비자 상점을 허용함으로써 더 단순하고 일반적으로 다뤄질 수 있습니다. 이것은 두 단계 커밋을 지원하지 않는 소비자가 쓰기 원하는 많은 출력 시스템 때문에 더 낫습니다.(This is better because many of the output systems a consumer might want to write to will not support a two-phase commit.) 이것의 예로,우리의 HDFS에 데이터를 모으는 하둡 ETL은 HDFS에 이것의 오프셋을 데이터와 오프셋을 동시에 업데이트하거나 안하거나 중 하나를 보장하도록 이것을 읽는 데이터와 같이 저장합니다. 우리는 이 강한 의미를 요구하는 많은 다른 데이터 시스템과 기본키를 가지지 않는 메세지를 중복제거를 위해 허용하는 비슷한 패턴을 따릅니다.

그래서 기본적으로 효과적으로 카프카는 적어도 한번 전송을 보장하고 사용자를 최대 한번전송에 프로듀서에 재시도를 불가능하게 하고 메세지 배치 처리보다 앞서 이 오프셋을 커밋함으로써 시행합니다. 정확히 한번전송은 도착 장소 시스템과의 조율을 요구하지만 카프카는 이 직선을 시행하게 만드는 오프셋을 제공합니다.

#### 4.7 복제

카프카는 서버의 설정 가능한 수(너가 이 토픽 대 토픽 기반(topic-by-topic basis)에 복제 요소를 설정할 수 있습니다.)를 넘어 각 토픽의 파티션에 대한 로그를 복제합니다. 이것은 메세지들이 실패의 존재에 남아있기 때문에 클러스터의 서버가 실패할때 이 복사본에 대한 자동 페일오버를 가능하게 합니다.(This allows automatic failover to these replicas when a server in the cluster fails so messages remain available in the presence of failures).  

다른 메세징 시스템은 몇개의 복사관련 기능을 제공하지만,우리의 의견으로는(완전 선입견있는),이것은 큰 단점과 많이 사용되지 않게 첨가된 것을 표현: 종속장치 비활성,처리량이 크게 영향되는,이것은 성가신 수동 설정을 요구,등등. 카프카는 기본적으로 복제를 사용한다는 것을 의미합니다-사실 우리는 복제요소가 하나인 복제된 토픽으로 복제되지 않은 토픽을 시행합니다.  

복제단위는 토픽파티션입니다. 비장애상태아래서,카프카의 각 파티션은 하나의 리더와 0또는 더 많은 팔로워를 가집니다. 리더를 포함한 복사본 총 수는 복제 인자를 구성합니다. 모든 읽고 쓰는 것은 파티션의 리더에 갑니다. 일반적으로, 브로커보다 더 많은 파티션들이 있고 리더가 고르게 브로커에 분산되어있습니다. 팔로워의 로그는 리더의 로그와 동일합니다-모두가 같은 순서에 같은 오프셋과 메세지를 가집니다.(그러나,물론,아무때나 리더는 로그의 끝에 약간 아직 복제되지 않은 메세지를 가지고있을수도 있습니다.)  

팔로워는 보통 카프카 소비자가 하는 것처럼 리더의 메세지를 소비하고 그들을 자신의 로그에 적용합니다. 리더에서 끌어당긴 팔로워를 가지는 것은 소비자가 자연스럽게 그들의 로그에 적용되는 로그 항목을 같이 배치하는 걸 허용하는 좋은 특성을 가집니다.  

대부분의 분산 시스템처럼 자동으로 오류를 다루는 것은 "살아있는" 노드에 대해 무엇을 의미하는지에 대한 정확한 정의를 요구합니다. 카프카에서 노드살아있음은 두가지 조건을 가집니다  

1. 노드는 주키퍼 섹션을 유지할 수 있어야 합니다(주키퍼의 하트비트 매커니즘을 통해).
2. 이것이 노예(slave)라면 이것은 반드시 리더에서 일어나는 읽기를 복제해야하고 "너무 먼"뒤에 떨어지지 않아야 합니다.

우리는 "살아있는" 또는 "실패"의 혼동을 막기 위해 "동기화"하는걸로 이 두 조건을 만족시키는 노드를 나타냅니다. 리더는 "동기화"노드들의 세트를 기억합니다. 팔로워가 죽거나,꼼짝못하거나,뒤로 떨어지면 리더는 이것을 동기 복제의 목록에서 제거할 것입니다. 얼마나 멀리 뒤에 있는지에 대한 정의는 replica.lag.max.messages 설정에 의해 제어되고 무엇을 할지 모르는 복제의 정의는 replica.lag.time.max.ms 설정에 의해 제어됩니다.  

분산 시스템 용어에서 우리는 단지 노드가 갑자기 작업을 중단한 오류의 "실패/회복" 모델을 다루기 위해 시도하고 후에 회복합니다(아마 그들이 죽었는지에 인식없이). 카프카는 소위 노드가 임의로 또는 악의적인 응답을 만드는(아마 버그 또는 파울 플레이 때문에) "Byzantine" 오류를 다루지 않습니다.  

파티션에 대한 모든 동기화 복제가 이것을 자신의 로그에 적용할때 메세지는 "커밋"으로 간주합니다. 오직 커밋된 메세지들은 항상 소비자에게 배부되었습니다. 이것은 소비자가 잠재적으로 리더가 실패할때 손실되는 메세지를 보는 걸 걱정할 필요가 없다는 것을 의미합니다. 반면에 생산자는 지연과 내구성 사이에 교환을 위한 그들의 선호도에 따라 메세지가 커밋되거나 안되는 것 중 하나를 기다리는 옵션을 가집니다. 이 성능은 생산자가 사용하는 request.required.acks 설정에 의해 제어됩니다.  

카프카가 제공하는 보증은 항상 살아있는 동기 복제에 적어도 한 개가 있는 한 커밋된 메세지가 손실되지 않는다는 것입니다.  

카프카는 짧은 페일오버 기간 후에 노드실패의 존재에 남아 이용할 수 있지만, 네트워크 파티션의 존재에 남아 이용할수는 없습니다.  

##### 복제된 로그: 쿼럼(Quorums),ISRs,그리고 기계(Machines)

그 중심에서 카프카 파티션은 복제된 로그입니다. 복제된 로그는 분산 데이터 시스템에 가장 기초적인 것 중 하나이고,이것을 시행하기 위한 많은 접근이 있습니다. 복제된 로그는 [상태 머신 스타일](http://en.wikipedia.org/wiki/State_machine_replication)에 다른 분산시스템을 시행하기 위해 원시적으로 다른 시스템에 의해 사용될 수 있습니다.  

복제된 로그 모델은 일련의 값의 순서에 일치에 들어가는 과정(일반적으로 로그 항목 0,1,2...을 번호매기기). 이것을 시행하기 위한 많은 방법이 있지만,가장 간단하고 빠른 것은 이것이 제공되는 값의 순서를 선택하는 리더와 같이하는 것입니다. 리더가 살아 남아있는한,모든 팔로워들은 리더가 선택한 순서와 값을 복사해야만 합니다.  

물론 지도자가 실패하지 않으면 우리는 팔로워가 필요없습니다! 리더가 죽을때 우리는 팔로워 중에서 새로운 리더를 선택해야 합니다. 하지만 팔로워 스스로 뒤에 떨어지거나 충돌할 수 있어서 우리는 최신의 팔로워를 선택하는 것을 확인할 필요가 있습니다. 로그 복제 알고리즘이 제공하는 기본적인 보증은 우리가 클라이언트에게 메세지가 커밋됬고 리더가 실패했다고 말하는 경우,우리가 뽑은 새로운 리더 또한 그 메세지를 가지고 있다는 것입니다. 이것은 트레이드오프를 생산: 그때 커밋된 이것을 선언하기 전에 리더가 메세지를 승인하기 위한 많은 팔로워를 기다린다면 더 많은 잠재적으로 선출될 수 있는 리더들이 있습니다.  

너가 필요한 승인수와 리더를 뽑기 위해 중복에 대한 보장과 같은 것을 비교해야 하는 로그수를 고른다면,이것은 쿼럼(Quorum)이라고 합니다.  

이 트레이드오프의 흔한 방법은 커밋결정과 리더선거 둘다 과반수 투표를 사용하는 것입니다. 이것은 카프카가 무엇을 할 것인가가 아니고,트레이드오프를 이해하기 위해서 어쨋든 이것을 분석하자. 우리는 2F+1 복사본을 가지고 있다고 말하자. 리더에 의해 선언된 커밋에 앞서 F+1 복사본이 메세지를 받는 경우,그리고 적어도 F+1 복사본에서,또,F 실패가 더이상 없는 곳에서 우리가 가장 완벽한 로그 팔로워를 뽑음으로써 새로운 리더를 뽑은 경우,리더는 모든 커밋된 메세지들을 가지는 보증이 됩니다. 아무 F+1 복사본 중에서,커밋된 모든 메세지를 포함하는 적어도 하나의 복사본이 존재해야하기 때문입니다. 복제본의 로그가 가장 완벽하고 그래서 새로운 리더로 뽑힐수 있습니다. 각 알고리즘이 다루는 많은 세부사항(어떤것이 로그를 더 완벽하게 만드는지에 대한 정확한 정의,리더 실패동안 로그 지속성을 보장,또는 복사 세트에서 서버의 세트를 교환과 같은)이 있지만 우리는 지금은 이것을 무시할것입니다.  

이 다수결 방식은 매우 좋은 특징이 있습니다: 지연은 오직 가장 빠른 서버에 의존하고 있습니다. 즉,복사 요소가 세 개라면,지연은 더 느린것이 아닌 더 빠른 종속장치에 의해 결정됩니다.  

주키퍼의 [Zab](http://web.stanford.edu/class/cs347/reading/zab.pdf),[Raft]()그리고 [Viewstamped 복제](http://pmg.csail.mit.edu/papers/vr-revisited.pdf)를 포함하는 이 가족에 알고리즘의 많은 종류가 있습니다. 우리가 카프카의 실제 구현에 대해 인식하고 있는 가장 비슷한 학술 출판물에는 마이크로소프트에서 [퍼시피카(PacificA)](http://research.microsoft.com/apps/pubs/default.aspx?id=66814)입니다.  

다수결의 단점은 이것이 뽑을만한 리더 없이 당신을 떠나기 위한 많은 실패를 겪지 않는다는것입니다. 하나의 고장을 견디기 위해 데이터의 세개의 복사본을 필요로하고,두개의 고장을 견디기 위해 데이터의 다섯개의 복사본을 필요로합니다. 우리의 경험에서 오직 단일 고장을 견디기위해 충분한 중복성을 가지는 것은 실용적인 시스템을 위해 충분하지 않지만,5배 디스크 공간 요구와 1/5의 처리량과 함께 매 5번 쓰기는 큰 용량 데이터 문제에 대해 매우 실용적이지 않습니다. 이것은 왜 주키퍼 같은 공유 클러스터 설정을 위한 쿼럼(quorum) 알고리즘에 더 많이 흔하게 나타나지만 기초적인 데이터 저장소에는 일반적이지 않는지 나타냅니다. HDFS의 예에서 네임노드의 고가용성 기능은 [과반수 투표에 기반한 저널](http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1/)에 구축되어 있지만,더 비싼 이 방법은 데이터 자체에 사용되지 않습니다.  

카프카는 그것의 쿼럼 세트를 선택하기 위해 약간 다른 방법을 취합니다. 다수결 투표대신에,카프카는 동적으로 리더에 따라잡힌 동기화 복제(ISR) 세트를 유지합니다. 이 세트의 멤버만 리더로서 선거의 대상이 됩니다. 카프카 파티션에 쓰는 것은 모든 동기화 복제가 쓰기를 받을 때까지 커밋으로 취급되지 않습니다. 이 ISR 세트는 변경될때마다 주키퍼로 유지되고 있습니다. 이것때문에,ISR에 모든 복사본은 리더로 뽑힐 수 있습니다. 이것은 많은 파티션이 있는 카프카의 사용 모델을 위한 중요한 요소이고 리더쉽 균형을 확보하는 것이 중요합니다. 이 ISR 모델과 F+1복제본과 함께 카프카 토픽은 커밋된 메세지를 손실하지 않고 F 실패를 견딜수 있습니다.  

우리가 다루기 바라는 대부분의 사용사례를 위해,우리는 이 트레이드오프가 타당한 것이라고 생각합니다. 사실, F 실패를 견디기 위해서,다수결과 ISR방법 모두 메세지를 커밋하기 전에 확인하기 위한 복제와 같은수를 기다립니다(예를 들어 한 실패를 구하기 위해서 대다수의 쿼럼은 세개의 복사본과 하나의 확인을 요구하고 ISR방법은 두개의 복사본과 하나의 확인을 요구합니다). 가장 느린 서버없이 커밋하는 능력은 다수결 방식의 장점입니다. 그러나,우리는 클라이언트가 그들이 메세지 커밋을 막거나 안 막거나 중 하나를 선택하도록 허락함으로써 개선 될 수 있다고 생각하고,추가적인 처리량과 디스크 공간은 요구한 복제요소보다 더 작기때문에 이것을 할 만한 가치가 있습니다.  

또 다른 중요한 디자인 차이는 카프카가 충돌한 노드들이 모든 데이터를 온전하게 복구하는 걸 요구하지 않는 것입니다. 잠재적인 일관성 위반 없이 아무 실패회복 상황에 손실되지 않는 "안정된 저장소"의 존재를 믿는 이 공간에서 복제 알고리즘은 드문 일이 아닙니다. 이 추정에 두개의 기본적인 문제가 있습니다. 먼저, 디스크 오류는 우리가 지속적인 데이터 시스템의 실제 운영에서 관찰할 수 있는 가장 흔한 문제이고,그들은 종종 데이터 온전함을 떠나지 않았습니다. 두번째로, 이것이 문제가 아닐지라도,우리는 2-3 자리로 인해 성능이 저하될 수 있기때문에 우리의 일관성을 보장하기 위한 모든 쓰기에서 fsync의 사용을 요구할 필요가 없습니다. ISR과 재조합하기 위한 복제본을 허용하기 위한 우리의 프로토콜은 재조합하기전에 이것이 충돌에 플러시되지 않은 데이터가 손실될지라도 완전히 재동기화해야함을 보장합니다.  

##### 부정한 리더 선거: 그들이 모두 죽는다면 어떻게 됩니까?

데이터 손실에 관한 카프카의 보장은 적어도 동기화에 나머지 복제본을 전제로 하고 있다는 점을 주목하세요. 모든 노드가 파티션 다이를 복제하면,이 보증은 더이상 유지하지 않습니다.  

그러나 실제적인 시스템은 모든 복제본이 죽었을때 합리적인 무언가를 해야합니다. 이것이 발생하는 충분한 불운이라면,이것은 무슨일이 일어나는가를 검토하는 것이 중요합니다. 구현할 수 있는 두 가지 행동이 있습니다:

1. 일상으로 돌아와서 ISR에 복제를 기다리고 이 복제를 리더로서 선택합니다(바라건대 이것은 여전히 그것의 모든 데이터를 가지고 있습니다).
2. 리더로써 일상으로 돌아오는 첫번째 복제(ISR에 필수적이지 않은)를 선택합니다.

이것은 가용성과 일관성 사이의 단순한 트레이드오프입니다. 우리가 ISR에 복제를 기다린다면,우리는 그 복제가 다운되는 한 사용할 수 없는 상태로 유지됩니다. 이런 복제들이 파괴되거나 그들의 데이터가 손실된 경우,우리는 영구적으로 다운되어 있습니다. 반면에 비동기화 복제가 일상으로 돌아오고 우리가 그것이 리더가 되는걸 허락한 경우,해당 로그는 모든 커밋된 메세지를 갖는다는 보증이 없더라도 진실의 근원이 됩니다. 우리의 최신 버전에서 우리는 두번째 전략을 선택하고 ISR에 모든 복제본이 죽을때 잠재적으로 모순된 복제복을 선택할 수 있습니다. 향후에, 우리는 다운타임이 모순되는 것이 나은 사용사례를 더 좋게 지원하기 위해 이 설정가능한 것을 만들 수 있습니다.  

이 딜레마는 카프카에 명시되지 않습니다. 이것은 모든 쿼럼기반의 체계에 존재합니다. 예를 들어 과반수 체계에서,서버의 대다수가 영구적인 실패를 겪는다면,너는 너의 데이터의 100프로를 잃거나 너의 새로운 사실소스로 존재하는 서버에 남아있는 어떤 것을 취함으로써 일관성에 위반하는 것중 하나를 선택해야합니다.

##### 복제 관리

복제된 로그에 위의 논의는 정말 유일한 단일 로그,즉 하나의 토픽 파티션을 다룹니다. 그러나 카프카 클러스터는 수백 또는 수천의 이 파티션들을 관리할 수 있습니다. 우리는 노드의 적은 수에 높은 양 토픽을 위한 모든 파티션 클러스팅을 피하기 위해 라운드-로빈 방식에 클러스터 이내에 파티션을 균형맟추려고 시도합니다. 마찬가지로,우리는 각 노드가 파티션의 비례하는 공유를 위한 리더가 되도록 파티션의 리더쉽을 균형 맟추기위해 노력합니다.  

이것은 또한 사용할 수 없는 중요한 창으로 리더쉽 선출 처리를 최적화하는 것이 중요합니다. 리더선거의 소박한 구현은 노드가 실패할때 노드가 호스트되는 모든 파티션의 파티션마다 선거를 실행하고 끝날 것 입니다. 대신에,우리는 "컨트롤러"로 브로커 중에 하나를 선태합니다. 이 컨트롤러는 브로커 수준에서 실패를 발견하고 실패한 브로커에 영향을 받는 모든 파티션의 리더를 변경하기 위한 책임이 있습니다. 결과는 우리가 훨씬 더 저렴하고 더 빠르게 많은 파티션수를 위한 선거 과정을 만드는 데 필요한 많은 리더쉽의 변경 통지를 같이 배치할 수 있다는 것입니다.  

#### 4.8 로그 압축

로그 압축은 카프카가 항상 단일 토픽 파티션에 데이터의 로그이내로 각 메세지 키에 대한 적어도 최근에 알려진 값을 유지하는 걸 보장합니다. 이것은 어플리케이션 충돌 또는 시스템 실패후에 상태를 복구하는거나 운영보수동안에 어플리케이션을 다시 시작한 후 캐시를 리로딩하는 것과 같은 사용사례와 상황에 대응합니다. 좀 더 자세히 이러한 사용사례와 다음 압축이 어떻게 작업하는지 살펴보자.(Let's dive into these use cases in more detail and then describe how compaction works)  

여기까지 우리는 고정된 기간 후 또는 미리결정한 몇몇의 크키에 로그가 도달했을때 오래된 데이터가 버려지는 데이터 유지에 간단한 접근만 설명했습니다. 이 작업은 각 기록이 혼자 서있는 로깅과 같은 일시적인 이벤트 데이터에 적합합니다. 그러나 데이터 스트림의 중요한 클래스는 잘 변하는 데이터를 입력한 변화의 로그입니다(예를 들어, 데이터베이스 테이블 변화).  

그러면 그런 스트림의 구체적인 예를 살펴보자. 우리는 유저메일주소를 포함하는 토픽을 가지고 있다고하자;사용자가 자신의 메일주소를 업데이트할때마다 우리는 기본키로 그들의 유저 아이디를 사용하는 이 토픽에 메세지를 보냅니다. 이제 우리는 id123을 가진 유저를 위한 약간의 시간을 통해 다음 메세지를 전송한다고하자,각 메세지는 바뀐 이메일 주소에 대응(다른 아이디 메세지가 생략됬습니다):


	123 => bill@microsoft.com
            .
	        .
	        .
	123 => bill@gatesfoundation.org
    		.
   			.
  			.
	123 => bill@gmail.com

로그 압축은 우리가 각 기본키(예: bill@gmail.com)에 대한 적어도 마지막 업데이트를 유지하는 것이 보장되도록 더 많은 보유 메커니즘을 제공합니다. 이렇게함으로써 우리는 로그가 최근에 변경된 키가 아닌 모든 키의 최종 값의 풀 스냅샷을 포함하는 것을 보장합니다. 이것은 다운스트림 소비자가 모든 변화의 완벽히 로그를 유지하지 않아도 토픽을 벗어난 그들 자신의 상태를 복구할 것을 의미합니다.  

이것이 유용한 적은 사용사례를 보면서 시작,그리고나서 우리는 어떻게 이것이 사용되는 지 보자.

1. 데이터베이스 변경 구독. 이것은 종종 다수의 데이터 시스템에 데이터 세트를 가질 필요가 있고,종종 이러한 시스템의 하나는 어떤 종류의 데이터베이스입니다(RDBMs 또는 아마 새로운 유행 키 값 저장 중 하나).
2. 이벤트 소싱. 이것은 어플리케이션 디자인과 어플리케이션을 위한 기초적인 저장으로 변화의 로그를 사용하는 쿼리 처리와 함께 배치된 어플리케이션 디자인의 스타일입니다.
3. 고 가용성 저널링(Journaling). 지역 계산하는 프로세스는 이것이 실패한 경우 다른 프로세스가 이 변화를 재배치하고 움직이기 때문에 이것의 지역 상태를 만드는 변경을 로그아웃함으로써 고장 방지할 수 있습니다. 이것의 구체적인 예는 계산,집계,그리고 스트림 쿼리 시스템 처리와 같은 다른 "그룹에 의한"것을 처리합니다. 실시간 스트림 처리 프레임워크인 Samza는 이 목적을 위해 [이 특징을 사용](http://samza.apache.org/learn/documentation/0.7.0/container/state-management.html)합니다.  

이러한 경우의 각각에서 하나는 주로 실시간 변화의 피드를 다루기 위해 필요하지만,가끔 기계가 충돌하거나 데이터가 재배치하거나 재처리해야할 필요가 있을때,풀 로딩해야할 필요가 있습니다. 로그 압축은 이 사용사례 모두가 동일한 보조 토픽을 얻을 수 있습니다. 로그사용의 이 스타일은 [이 블로그 포스트](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)에 더 자세히 나와있습니다.  

일반적인 생각은 매우 간단합니다. 우리가 무한 로그 유지를 가지고 있고 위의 예에 각 변화를 기록했다면 우리는 이것이 처음시작했을때부터 각 시점에서 시스템 상태를 캡쳐해야합니다. 이 완전한 로그를 사용하면서 우리는 로그에 첫번째 N 기록을 대신함으로써 언제든지 복원할 수 있습니다. 이 가상의 완전한 로그는 로그가 안정적인 데이터세트에 바인딩 되지않고 커지면서 단일 기록을 여러번 업데이트 하는 시스템을 위해 매우 실용적이지 않습니다. 오래된 업데이트를 버리는 간단한 로그 유지 메커니즘은 공간을 바운딩하지만 더이상 로그는 현재 상태를 복원하는 방법이 아닙니다-이제 로그의 시작부터 복구하는 것은 오래된 업데이트가 전혀 포착되지 않기 때문에 더 이상 현재 상태를 재현하지 않습니다.  

로그 압축은 시간에 기반한 보존 보다 기록마다 보존하는 메커니즘입니다. 아이디어는 선택적으로 우리가 동일한 기본키를 가진 더 최근 업데이트 기록을 지우는 것입니다. 이 방법에서 로그는 적어도 각 키에 대한 최신 상태를 가지고 있다는 걸 보장합니다.  

이 보존 정책은 토픽당 설정할 수 있어서, 단일 클러스터는 유지가 크기 또는 시간에 의해 실행되는 몇 가지 토픽과 유지가 압축에 의해 실행되는 다른 토픽들을 가질 수 있습니다.  

이 기능은 인프라 링크드인의 가장 오래되고 가장 성공적인 작품 중 하나에 의해 영감됩니다-데이터베이스 체인지로그 캐싱 서비스는 [데이터 버스](https://github.com/linkedin/databus)라고 불립니다. 대부분의 로그 구조의 저장 시스템과 달리 카프카는 구독을 위해 구축하고 데이터를 빠른 선형 읽기와 쓰기를 위해 체계화합니다. 데이터버스와 달리, 카프카는 심지어 업스트림 데이터 소스가 재생가능하지 않은 상황에 유용하도록 사실 소스(source-of-truth)저장소를 수행합니다.  

##### 로그 압축 기초

각 메세지에 대한 오프셋을 가진 카프카 로그의 논리적인 구조를 보여주는 고수준 사진이 여기에 있습니다.  

![log_cleaner_anatomy](C:\Users\Administrator\Desktop\번역1\log_cleaner_anatomy.png)

로그의 헤드는 전통적인 카프카 로그와 동일합니다. 이것은 조밀한 순차 오프셋을 가지고 모든 메세지를 보유합니다. 로그 압축은 로그의 끝을 처리하기 위한 옵션을 추가합니다. 위에 있는 사진은 압축된 끝(compacted tail)을 가진 로그를 보여줍니다. 로그의 마지막 메세지는 그들이 먼저 작성한-절대 변화지 않는 지정된 오프셋 원본을 유지합니다. 또한 모든 오프셋들은 그 오프셋을 가진 메세지가 떨어져 압축되어 있더라도 로그에 유효한 위치를 유지합니다; 이 경우에 이 위치는 로그에 나타나는 다음 최고 오프셋과 구분이 안됩니다. 예를 들어, 위의 사진에서 오프셋 36 , 37 그리고 38은 모두 동등한 위치와 이 오프셋에 아무때나 시작하는 읽기는 38로 시작하는 메세지 세트를 반환합니다.  

압축은 또한 제거할 수 있습니다. 키와 널 페이로드를 가진 메세지는 로그에서 제거하도록 처리될 수 있습니다. 이 삭제표시는 제거(그 키를 가진 어떤 새로운 메세지처럼)하기 위해 그 키를 가진 어떤 메세지를 미리 일으킬 수 있지만, 삭제 표시들은 그들이 스스로 공간을 확보하기 위해 일정시간 후에 로그 외에서 제거된다는 것이 특별합니다. 삭제가 더 이상 유지되지 않는 그 시점은 위 다이아그램에서 "삭제 유지 시점" 으로 표시됩니다.  

압축은 정기적으로 로그 세그먼트를 다시 복사함으로써 백그라운드에서 실행됩니다. 청소는 읽기를 막지 않고 생산자와 소비자에 영향을 주지 않도록 설정가능한 I/O 처리량의 양 보다 더 적게 사용하기 위해 좁힐 수 있습니다. 로그 세그먼트를 압축하는 실제 프로세스는 다음과 같이 됩니다:

![log_compaction](C:\Users\Administrator\Desktop\번역1\log_compaction.png)

##### 로그 압축은 어떤 보장을 제공합니까?

로그 압축은 다음을 보장:

1. 로그의 헤드에 밀린 상태인 어떤 소비자는 작성된 모든 메세지를 볼 수 있습니다; 이 메세지는 순차적 오프셋을 가질 수 있습니다.
2. 메세지의 순서는 항상 유지됩니다. 압축은 절대 재주문 메세지가 아닙니다,단순히 일부를 제거하는 것입니다.
3. 절대 변화지 않는 메세지에 대한 오프셋입니다. 이것은 로그의 위치에 대한 영구적인 식별자입니다.
4. 오프셋 0부터 처리하는 어떤 읽기는 적어도 그들이 적은 순서에 모든 기록의 최종 상태를 볼 수 있습니다. 삭제된 기록에 대한 모든 삭제표시는 독자가 토픽의 delete.retention.ms 설정(기본값은 24시간입니다.)보다 더 적은 기간에 로그의 헤드에 도달하는 걸 제공하는 것을 볼 수 있습니다. 이것은 삭제 표시 제거가 읽기와 동시에 일어나기 때문에 중요합니다(따라서 이것은 독자가 이것을 읽기에 앞서 어떤 삭제 마커를 제거하는 것이 중요합니다.).
5. 로그의 시작에서 처리하는 어떤 소비자는 적어도 그들이 적은 순서에 모든 기록의 *최종*상태를 볼 수 있습니다. 삭제된 기록에 대한 모든 삭제 마커는 소비자가 토픽의 delete.retention.ms 설정(기본값은 24시간입니다.)보다 더 적은 기간에 로그의 헤드에 도달하는 걸 제공하는 것을 볼 수 있습니다. 이것은 삭제 표시 제거가 읽기와 동시에 일어나기 때문에 중요하고 그래서 이것은 소비자가 보기에 앞서 우리가 아무 삭제 표시를 제거할 수 없다는 것이 중요합니다.  

##### 로그 압축 세부사항

로그 압축은 로그의 헤드에 누구의 키를 나타내는 기록을 지우면서 로그 세그먼트 파일을 다시 복사하는 백그라운드 스레드의 풀인 로그 클리너에 의해 처리됩니다. 다음과 같이 각 압축기 스레드가 작업합니다:

1. 이것은 로그끝에 향하는 로그의 최고 비율을 가지는 로그를 선택합니다.
2. 이것은 로그의 헤드에 각 키에 대한 마지막 오프셋의 간결한 요약을 작성합니다.
3. 이것은 로그를 처음부터 끝까지 로그에 마지막 발생을 가지는 키를 제거하면서 다시 복사합니다. 새로운,청소 세그먼트를 필요한 추가 디스크 공간이 오직 하나의 추가 로그 세그먼트(로그의 완전한 복사가 아닌)이도록 즉시 로그로 교체됩니다.
4. 로그 헤드의 요약은 기본적으로 오직 공간 압축 해시 테이블입니다. 이것은 정확히 목록 당 24바이트를 사용합니다. 결과적으로 클리너 버퍼의 8GB를 가진 하나의 클리너 반복은 약 로그헤드의 366GB 를 청소할 수 있습니다(1K 메세지로 추정).  

##### 로그 클리너 구성

0.8.1.에서 로그 클리너는 기본적으로 비활성화되어 있습니다. 이것을 활성화하기 위해서 서버 설정을 설정해야합니다.

	log.cleaner.enable = true

이것은 클리너 쓰레드의 풀(the pool of cleaner thread)을 시작할 수 있습니다. 특정 토픽에 대한 로그 청소를 활성화하기 위해서 너는 로그 고유의 속성을 추가할 수 있습니다.

	log.cleanup.policy = compact

이것은 토픽 생성 시간 또는 대안 토픽 명령을 사용하는 것 중 하나에 수행할 수 있습니다.  

게다가 클리너 설정은 [여기서](http://kafka.apache.org/documentation.html#brokerconfigs) 설명되고 있습니다.  

##### 로그 압축 제한

1. 너는 얼마나 많은 로그가 압축(로그의 "헤드")없이 유지되는지에 대해 아직 설정할 수 없습니다. 현재 모든 세그먼트가 마지막 세그먼트를 제외하고 대상이 됩니다,예를 들어 현재 기록되고 있는 하나.
2. 로그 압축은 아직 압축된 토픽과 호환되지 않습니다.  

### 5. 구현

#### 5.1 API 디자인

##### 프로듀서 APIs

2개의 낮은 수준의 프로듀서를 두른 프로듀서 API-kafka.producer.SyncProducer 과 kafka.producer.async.AsyncProducer.

	class Producer {
    
    /* 동기 또는 비동기 프로듀서 중 하나를 사용하여 토픽에 키에 의해 분산되는 데이터를 전송 */
	public void send(kafka.javaapi.producer.ProducerData<K,V> producerData);

	/* 동기 또는 비동기 프로듀서 중 하나를 사용하여 토픽에 키에 의해 분산된 데이터의 목록을 전송 */
    public void send(java.util.List<kafka.javaapi.producer.ProducerData<K,V>> producerData);
	
    /* 프로듀서를 닫고 치웁니다 */
	public void close();
    
	}

목표는 클라이언트에 단일 API를 통해 모든 프로듀서 기능성을 드러내는 것입니다. 새로운 프로듀서는-

* 다수의 프로듀서 요청과 배치된 데이터의 비동기 디스패치의 쿼닝/버퍼링을 다룰 수 있습니다-

kafka.producer.Producer는 직렬화와 그들을 적절한 카프카 브로커 파티션에 보내기 전에 다수의 생산 요청(producer.type = async)을 배치하는 능력을 제공합니다. 배치의 크기는 몇 가지 설정 매개변수로 제어할 수 있습니다. 이벤트가 큐에 들어가면,queue.time 또는 batch.size 중 하나가 도달할 때까지, 그들은 큐에 버퍼됩니다. 백그라운드 스레드(kafka.producer.async.ProducerSendThread)는 데이터의 배치를 디큐하고 kafka.producer.EventHandler를 직렬화하고 적절한 카프카 브로커 파티션에 데이터를 전송합니다. 커스텀 이벤트 핸들러는 event.handler 설정 매겨변수를 통해 연결할 수 있습니다. 이 프로듀서 큐 파이프라인의 다양한 단계에서,이것은 커스텀 로깅/트레이싱 코드 또는 커스텀 모니터링 논리 둘 중 한 곳에 연결하기 위한 콜백을 주입할 수 있도록 도움이 됩니다. 이것은 kafka.producer.async.CallbackHandler 인터페이스를 구현하고 그 클래스에 callback.handler 설정 매개변수를 설정함으로써 가능합니다.  

* 유저 지정 인코더를 통해 데이터의 직렬화를 처리합니다-


	interface Encoder < T > {
       public Message toMessage(T data);
    }

기본적으로 노 오퍼레이션(no-op)은 kafka.srializer.DefaultEncoder 입니다.

* 마음대로 유저 지정 파티셔너를 통해 소프트웨어 로드 밸런싱을 제공합니다-

 라우팅 결정은 kafka.producer.Partitioner에 의해 영향을 받습니다.


	interface Partitioner< T> {
      int partition(T key, int numPartitions);
    }

파티션 API는 키와 파티션 id를 반환하기 위해 사용가능한 브로커 파티션의 수를 사용합니다. 이 id는 프로듀서 요청에 대한 브로커 파티션을 정하기 위해 broker_ids 와 파티션의 정렬된 목록으로 인덱스를 사용할 수 있습니다. 기본 파티셔닝 전략은 해시(키) % numPartitions 입니다. 키가 비어있으면, 임의로 브로커 파티션이 선택됩니다. 커스텀 파티셔니 전략은 또한 partitioner.class 설정 매겨변수를 사용하여 연결할 수 있습니다.

##### 컨슈머 APIs

우리는 소비자 APIs의 두 개의 수준을 가집니다. 저수준의 "단순한" API는 단일 브로커에 연결을 유지하고 서버에 전송되는 네트워크 요청에 밀접한 관련이 있습니다. 이 API는 그러나 그들이 선택한 이 메타데이터를 유지하기 위해 사용자를 허용하면서 모든 요청에 통과하는 오프셋을 가진 완전히 무상태입니다.  

고수준의 API는 소비자 브로커의 세부정보를 숨기고 근본적인 토폴로지에 대한 생각없이 기계의 클러스터 해제를 사용할 수 있습니다. 이것은 또한 어떤것이 사용됬는지에 대한 상태를 유지합니다. 고수준 API는 또한 필터 표현(즉, 화이트리스트 또는 블랙리스트 정규 표현 방식 중 하나)에 일치하는 토픽을 등록하는 기능을 제공합니다.  

**저수준의 API**

	class SimpleConsumer {

	/* 브로커에 페치 요청을 전송하고 메시지의 세트를 되찾습니다. */
    public ByteBufferMessageSet fetch(FetchRequest request);
	
    /* 페치요청의 목록을 브로커에 전송하고 응답세트를 되찾습니다. */
	public MultiFetchResponse multifetch(List<FetchRequest> fetches);

	/**
     * 주어진 시간전에 유효한 오프셋(최대크기까지)의 목록을 얻습니다.
     * 내림차순으로, 결과는 오프셋의 목록입니다.
     * @param time: 밀리세컨드 시간
     *              OffsetRequest$.MODULE$.LATIEST_TIME()로 설정하면, 사용가능한 최신 오프셋에서 얻습니다.
     *              OffsetRequest$.MODULE$.EARLIEST_TIME()로 설정하면, 사용가능한 가장 빠른 오프셋에서 얻습니다.
     */
	public long[] getOffsetsBefore(String topic, int partition, long time, int maxNumOffsets);
    
    }

저수준 API는 상태를 유지하는 주위에 특정 요구를 가지는 우리의 오프라인 소비자(하둡 소비자와 같은)의 일부를 위해 직접 사용하는 것 뿐만 아니라 고수준 API 구현에도 사용됩니다.

**고수준 API**

	/* 클러스터에 연결을 생성 */
    ConsumerConnector connector = Consumer.create(consumerConfig);
	
    interface ConsumerConnector {

	/**
     * 이 방법은 너가 메세지를 얻을 수 있고 메타데이터(현재는 토픽만)와 연관된 MessageAndMetadata 객체의 반복자인 카프카스트림(KafkaStreams)의 목록을 얻기위해 사용됩니다.
     * 입력: <topic, #streams>의 맵
     * 출력: <topic, list of message streams>의 맵
     */
	public Map<String,List<KafkaStream>> createMessageStreams(Map<String,Int> topicCountMap);

	/**
     * 너는 또한 TopicFilter와 일치하는 토픽에서 메세지를 통해 반복하는 카프카스트림(KafkaStreams)의 목록을 얻을 수 있습니다.(TopicFilter는 화이트리스트 또는 표준 자바 정규 표현에 있는 블랙리스트를 캡슐화합니다.)
     */
	public List<KafkaStream> createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);

	/* 지금까지 소비된 모든 메세지의 오프셋을 커밋 */
    public commitOffsets()
	
    /* 연결기 차단 */
	public shutdown()
    
    }

이 API는 카프카스트림(KafkaStream) 클래스에 의해 구현하는 반복자를 중심으로 합니다. 각 카프카스트림(KafkaStream)은 하나 이상의 서버에 하나 이상의 파티션에서 메세지의 스트림을 나타냅니다. 각 스트림은 클라이언트가 생성한 콜에서 원하는 스트림의 수를 제공할 수 있도록 단일 스레드된 처리를 위해 사용합니다. 따라서 스트림은 다수의 서버 파티션(처리 스레드의 수에 상응하는)의 합병을 나타내지만, 각 파티션은 한 스트림에만 갑니다.  

createMessageStreams 콜은 소비자/브로커 할당을 재조정하는 토픽을 위한 소비자를 등록합니다. API는 단일 콜에 이 재조정을 최소화하기 위해서 많은 토픽 스트림을 만드는 걸 권장합니다. createMessageStreamByFilter 콜(추가로)은 이것의 필터와 일치하는 새로운 토픽을 발견하는 관찰자를 등록합니다. createMessageStreamsByFilter을 반환하는 각 스트림은 다수의 토픽에서 메세지를 통해 반복합니다(즉, 다수의 토픽은 필터에 의해 허락됩니다.).

#### 5.2 네트워크 계층

네트워크 계층은 매우 직선의 NIO 서버이고, 매우 구체적으로 설명하지 않습니다. 전송파일 구현은 writeTo 방법의 MessageSet 인터페이스을 제공함으로써 수행합니다. 이것은 처리중인 버퍼링된 쓰기 대신에 더 효율적인 transferTo 구현을 사용하기 위한 메세지 세트를 백파일 하는 걸 허용합니다. 쓰레딩 모델은 각 연결의 고정된 수를 처리하는 단일 어셉터 쓰레드와 N 프로세서 쓰레드입니다. 이 디자인은 상당히 철저하게 [다른곳에](http://data.linkedin.com/blog/2009/08/introducing-the-nio-socketserver-implementation) 시험되고 구현이 쉽고 빠른 걸 발견합니다. 프로토콜은 다른 언어로 클라이언트의 미래 구현을 가능하게 하기 위해 아주 간단하게 유지합니다.

#### 5.3 메세지

메세지는 고정된 크기의 헤더와 다양한 길이의 불투명한 바이트 배열 페이로드로 구성됩니다. 헤더는 포맷버전과 변형이나 절단을 감지하기 위한 CRC32 체크섬이 포함됩니다. 페이로드를 불투명하게 두는 것은 올바른 결정입니다: 지금 직렬화 라이브러리에 일어나는 다량의 프로세스가 있고, 어떤 특정 선택은 모든 사용에 적합하지 않습니다. 말할 필요도 없이 카프카를 사용하는 특정 어플리케이션은 그 사용의 일부로 특정 직렬화 유형을 지시할 것입니다. MessageSet 인터페이스는 단순히 NIO 채널에 대량의 읽기와 쓰기를 위한 전문적인 방법을 지닌 메세지의 반복자입니다.

#### 5.4 메세지 형식

	/**
     * 한 메세지. N 바이트 메세지의 형식은 다음과 같습니다:
     * 
     * 매직 바이트가 0이면
     * 
     * 1. 포맷 변경을 가능하게 하는 1 바이트의 "매직" 식별자
     * 
     * 2. 페이로드의 4 바이트 CRC32
     * 
     * 3. N- 5 바이트 페이로드
     * 
     * 매직 바이트가 1 이면
     * 
     * 1. 포맷 변경을 가능하게 하는 1 바이트의 "매직" 식별자
     * 
     * 2. 버전의 독립된 메세지에 주석을 가능하게 하기 위한 1 바이트의 "속성" 식별자(예를 들어 압축 가능한, 사용된 코덱의 종류)
     * 
     * 3. 페이로드의 4 바이트 CRC32
     * 
     * 4. N-6 바이트 페이로드
     * 
     */

#### 5.5 로그

2개의 파티션을 가진 "my_topic"으로 부르는 토픽의 로그는 그 토픽의 메세지를 포함하는 데이터 파일을 저장한 2개의 디렉토리(즉 my_topic_0 와 my_topic_1)로 구성됩니다. 로그 파일의 형식은 "로그 항목"의 순서입니다 ; 각 로그 항목은 N 메세지 바이트에 의해 일어나는 메세지 길이를 저장하는 4 바이트의 정수 N입니다. 각 메세지는 유일하게 지금까지 그 파티션에서 그 토픽에 전송 된 모든 메세지의 스트림에 이 메세지의 시작 바이트 위치를 제공하는 64 비트의 정수 *오프셋*에 의해 식별됩니다. 각 메세지의 디스크에 기록되어 있는 형식은 아래에 제공됩니다. 각 로그 파일은 이것이 포함하는 첫 번째 메세지의 오프셋으로 지명합니다. 그래서 만든 첫 번째 파일은 00000000000.kafka가 되고, 각 추가 파일은 S가 설정에서 주어진 최대 로그 파일 크기인 이전 파일로 부터 거의 S바이트 정수 이름을 가집니다.  

메세지에 대한 정확한 바이너리 형식은 메세지 세트가 바람직할때 다시 복사하거나 변환없이 클라이언트와 브로커,프로듀서 간에 전송되도록 표준 인터페이스로 버전되고 유지됩니다. 이 형식은 다음과 같습니다:

	디스크에 기록되어 있는 메세지 형식
    
	메세지 길이 : 4 바이트(값: 1 + 4 + n)
    "매직" 값 : 1 바이트
	crc : 4 바이트
    페이로드 : n 바이트

메세지 오프셋의 사용은 메세지 id로 흔하지 않습니다. 우리의 원래 아이디어는 프로듀서에 의해 생성되는 GUID를 위해 사용하고 각 브로커에 오프셋하기 위해 GUID에서 매핑을 유지하기 위한 것입니다. 그러나 소비자가 각 서버에 ID를 유지해야하기 때문에 GUID의 글로벌 고유값은 값을 제공하지 않습니다. 게다가 오프셋을 위한 임의의 id 매핑 유지의 복잡성은 본질적으로 완전 지속적인 임의 접근 데이터 구조를 요구하는 디스크에 동기화시키는 무거운 인덱스 구조를 요구합니다. 따라서 검색구조를 단순화하기 위해 우리는 메세지를 고유하게 식별하기 위해 파티션 id 와 노드 id를 결합할 수 있는 간단한 파티션 당 원자 카운터를 사용하기로 결정합니다; 소비자 요청당 여러 탐색이 여전히 가능성이 있을지라도 이것은 검색구조를 더 간단하게 만듭니다. 우리가 카운터에 정착했지만, 오프셋 사용에 직접 점프하는 것은 자연스러워 보입니다- 결국 모두 파티션에 고유한 단조적으로 증가하는 정수입니다.(However once we settled on a counter, the jump to directly using the offset seemed natural-both after all are monotonically increasing integers unique to a partition.) 오프셋이 소비자 API에 숨어있기때문에 이 결정은 궁극적으로 구현 세부사항이고, 우리는 더 효율적인 접근을 해야합니다.

![kafka_log](C:\Users\Administrator\Desktop\번역1\kafka_log.png)

##### 쓰기

로그는 항상 마지막 파일로 가는 순차적인 첨부할 수 있습니다. 이것이 설정가능한 크기(1GB로)에 도달했을때 이 파일은 신선한 파일로 롤오버됩니다. 로그는 디스크에 파일을 플러시하기 위한 OS를 강요하기전에 쓸 수 있는 메세지 수를 주는 두 개의 설정 매개변수 M과 플러시가 강요한후 얼마간의 시간을 주는 S를 가집니다. 이것은 시스템 충돌의 경우에 대부분의 M 메시지 또는 데이터의 S 초에서 손실의 내구성 보장을 제공합니다.

##### 읽기

읽기는 메세지의 64-비트 타당한 오프셋과 S-바이트 최대 청크 크기를 제공함으로써 수행됩니다. 이것은 S-바이트 버퍼에 포함된 메세지를 통해 반복자를 반환할 수 있습니다. S는 어떤 단일 메세지보다 더 커지도록 예정했지만, 비정상적으로 큰 메세지의 경우에, 읽기는 버퍼 크기를 두배로 하는 각 시간에 메세지를 성공적으로 읽을때까지 여러번 다시 시도할 수 있습니다. 최대 메세지와 버퍼 크기는 서버가 어떤 크기보다 더 큰 메세지를 거부하고, 완전한 메세지를 얻기 위해 항상 읽어야하는 최대 클라이언트에 바운드를 제공하도록 지정할 수 있습니다. 이것은 읽기 버퍼가 부분 메세지로 끝나기 쉽고, 이것은 크기 범위를 제한함으로써 쉽게 감지할 수 있습니다.  

오프셋에서 읽기의 실제 프로세스는 글로벌 오프셋 값에서 지정한 파일 오프셋을 계산하는 데이터가 저장되는 로그 세그먼트 파일을 처음으로 배치하고 그 다음에 파일 오프셋에서 읽기를 요구합니다. 검색은 각 파일에 유지되는 인메모리 범위에 맞서 간단한 바이너리 검색 변화로 수행합니다.  

로그는 클라이언트가 "지금"과 같은 구독을 시작할 수 있도록 가징 최근에 작성된 메세지를 얻는 기능을 제공합니다. 이것은 또한 소비자가 SLA가 지정한 몇일 이내에 그 데이터를 소비하는 데 실패하는 경우에 유용합니다. 클라이언트가 존재하지 않는 오프셋을 소비하려고 시도하는 경우에 이것은 OutOfRangeException을 제공하고 스스로 리셋하거나 사용사례에 필요에 따라 실패할 수도 있습니다.  

다음은 소비자에게 보낸 결과의 형식입니다.

	MessageSetSend  (fetch result)
    총 길이 : 4 바이트
	에러 코드 : 2 바이트
    메세지 1 : x 바이트
	...
    메세지 n : x 바이트
	
    MultiMessageSetSend  (multiFetch result)
	총 길이 : 4 바이트
    에러 코드 : 2 바이트
	messageSetSend 1
    ...
	messageSetSend n

##### 삭제

데이터는 한번에 하나의 로그 세그먼트를 삭제합니다. 로그 관리자는 제거대상인 파일을 선택하기위해 접속할수있는 삭제 정책(pluggable delete policies)을 허용합니다. 마지막 N GB를 보유한 정책 또한 유용하지만 현재 정책은 N 일 전보다 더 많은 개정 시간을 가진 모든 로그를 삭제합니다. 여전히 세그먼트 목록을 개정하는 삭제를 허용하는동안 잠금 읽기를 막기 위해서 우리는 copy-on-write 방식을 삭제가 진행하는동안 로그세그먼트의 변경할수 없는 통계 스냅샷 뷰를 진행하기 위한 바이너리 검색을 허용하기 위해 한결같은 뷰를 제공하는 세그먼트 목록 구현에 사용합니다.

##### 보장

로그가 플러시를 디스크에 강요하기 전에 작성된 메세지의 최대 수를 제어하는 설정 매개변수 M을 제공합니다. 스타트업에 로그 복구 프로세스는 최신의 로그 세그먼트에서 모든 메세지를 통해 반복하고 각 메세지 항목이 유효한지 확인하는 걸 실행합니다. 이것의 크기와 오프셋의 합이 파일과 메세지와 함께 저장한 CRC와일치하는 메세지 페이로드의 CRC32의 길이보다 적을때 메세지 항목은 유효합니다. 경우에 손상은 로그가 마지막 유효한 오프셋을 자르는 걸 감지합니다.  

손상의 2종류는 처리되야 합니다: 충돌때문에 기록되지 않은 블록이 손실되는 끊음과, 터무니없는 블록을 파일에 추가하는 손상. 이 이유는 일반적으로 OS는 파일 아이노드와 실제 블록 데이터 사이의 쓰기 순서 보장을 하지않아서 작성된 데이터를 잃는것 뿐아니라 아이노드가 새 크기로 업데이트된 경우에 파일은 터무니없는 데이터를 얻을 수 있지만 그 데이터를 포함하는 블록이 작성되지 않기 전에 충돌이 일어납니다. CRC는 이 코너 케이스를 감지하고 이것을 변경된 로그로부터 방지합니다(물론 기록되지 않은 메세지가 있더라도 손실됩니다).  

#### 5.6 분산

##### 주키퍼 디렉토리

다음은 소비자와 브로커 사이의 조정에 사용하는 주키퍼 구조와 알고리즘을 제공합니다.

##### 표기법

경로의 요소를 [xyz]로 나타낼때, 이것은 xyz의 값이 고정되어 있지 않다는 걸 의미하고 실제로 각 xyz의 가능한 값에 대한 주키퍼 z노드가 있습니다. 예를 들어 /topics/[topic]은 각 토픽 이름의 서브 디렉토리를 포함하는 directory named/topics 입니다. 수치 범위는 또한 서브디렉토리 0,1,2,3,4를 가리키기 위해 [0....5]와 같이 주어집니다. 화살표 ->는 z노드의 항목을 가리키기 위해 사용됩니다. 예를들어 /hello -> 세계는 값 "world"을 포함하는 znode/hello 를 가리킵니다.  

##### 브로커 노드 등록소

	/brokers/ids/[0...N] --> host:port (ephemeral node)  

이것은 존재하는 모든 각각의 소비자(그것의 설정의 일부로 제공해야하는)에게 이것을 식별하는 고유 논리적인 브로커 id를 제공하는 브로커 노드의 목록입니다. 스타트업에서, 브로커 노드는 /brokers/ids 아래에 논리 브로커 id를 가진 z노드를 생성함으로써 스스로 등록합니다. 논리 브로커 id의 목적은 브로커가 소비자에게 영향을 주지않고 다른 물리적인 기계로 이동 시키도록 하는 것입니다. 이미 사용중인 브로커 id를 등록하려고 하면 오류가 발생합니다(두 서버가 같은 브로커 id로 설정되기 때문에).  

수명이 짧은 z노드를 사용하는 주키퍼에 브로커가 스스로 등록하고 있기 때문에, 이 등록은 다이나믹하고 브로커가 종료되거나 죽으면 사라질 것입니다(따라서 이것이 더 이상 사용할수 없다고 소비자에게 알림).

##### 브로커 토픽 등록소

	/brokers/topics/[topic]/[0...N] --> nPartions (ephemeral node)  

각 브로커는 이것을 유지하는 토픽아래에 스스로 등록하고 그 토픽에 대한 파티션 수를 저장합니다.

##### 소비자와 소비자 그룹

토픽의 소비자는 또한 데이터 소비의 균형을 맞추고 그들이 소비하는 브로커에 대한 각 파티션에 그들의 오프셋을 추적하기 위해 주키퍼에 스스로 등록합니다.  

여러 소비자는 그룹을 형성하고 공동으로 하나의 토픽을 소비할 수 있습니다. 동일한 그룹에 있는 각 소비자는 공유된 group_id이 주어집니다. 예를 들어 한 소비자가 세개의 기계를 통해서 실행되는 너의 foobar 프로세스라면, 너는 이 id "foobar" 소비자의 그룹을 지정할 수 있습니다. 이 그릅 id는 소비자 설정에서 제공하고, 그것이 속한 그룹의 소비자에게 전달하기 위한 방법입니다.  

그룹에 소비자가 가능한 공평하게 파티션을 나누고,각 파티션은 정확히 소비자 그룹에 한 소비자에 의해 소비됩니다.  

##### 소비자 id 등록소

그룹에 모든 소비자에 의해 공유되는 group_id 뿐만 아니라, 각 소비자는 식별하는 목적으로 일시적이고 고유한 consumer_id (호스트이름의 양식: uuid)를 제공합니다. 소비자 ids는 다음 디렉토리에 등록되어 있습니다.

	/consumers/[group_id]/ids/[consumer_id] --> {"topic1" : #streams, ... , "topicN" : #streams} (ephemeral node)

그룹에 소비자는 각 해당 그룹아래에 등록하고 그것의 consumer_id를 지닌 z노드를 생성합니다. z노드의 값은 < topic,#streams >의 맵을 포함합니다. 이 아이디는 단순히 그룹내에서 현재 활동적인 소비자의 각각을 식별하는 데 사용됩니다. 소비자 프로세스가 죽으면 이것이 사라지기 때문에 이것은 일시적인 노드입니다.

##### 소비자 오프셋 추적

소비자는 각 파티션에 그들이 소비하고있는 최대 오프셋을 추적합니다. 이 값은 주키퍼 디렉토리에 저장됩니다.

	/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id] --> offset_counter_value ((persistent node)

##### 파티션 소유자 등록소

각 브로커 파티션은 주어진 소비자 그룹내에 한 소비자에 의해 소비됩니다. 모든 소비를 시작하기 전에 소비자는 주어진 파티션의 소유권을 설립해야합니다. 그 소유권을 설립하기 위해, 소비자는 그것이 주장하는 특정 브로커 파티션 아래에 자신의 id를 일시적인 노드에 작성합니다.  

	/onsumers/[group_id]/owners/[topic]/[broker_id-prtition_id] --> consumer_node_id (ephemeral node)

##### 브로커 노드 등록

브로커 노드는 기본적으로 독립적이라서 그들은 그들만 가지고 있는 정보를 제공합니다. 브로커가 합류할때, 이것은 브로커 노드 등록 레지스터리 아래에 스스로 등록하고 그것의 호스트 이름과 포트에 대한 정보를 작성합니다. 브로커는 또한 기존 토픽과 그들의 논리 파티션의 목록을 브로커 토픽 등록소에 등록합니다. 그들이 브로커에 생성할때 새 토픽은 동적으로 등록됩니다.  

##### 소비자 등록 알고리즘

소비자가 시작할때, 이것은 다음과 같은 작업을 수행합니다:

1. 그 그룹아래에 소비자 id 레지스트리에 스스로 등록합니다.
2. 소비자 id 레지스트리 아래의 변화(새 소비자가 합류하거나 기존 소비자가 떠나는)에 감시를 등록합니다(각각의 변경은 변경된 소비자가 속한 그룹내에 모든 소비자 사이에서 재조정을 작동시킵니다.).
3. 브로커 id 레지스트리 아래의 변화(새 브로커가 합류하거나 기존 소비자가 떠나는)에 감시를 등록합니다(각 변경은 모드 소비자 그룹의 모든 소비자 사이에서 재조정을 작동시킵니다.).
4. 소비자가 토픽 필터를 사용하면서 메세지 스트림을 생성하면, 이것은 또한 브로커 토픽 레지스트리아래의 변화(새로운 토픽이 추가되는)에 감시를 등록합니다(각각의 변경은 토픽 필터에 의해 허용되는 토픽을 결정하는 데 사용가능한 토픽의 재평가를 작동합니다. 토픽을 허용하는 새로운 것은 소비자 그룹내 모든 소비자 사이에서 재균형이 작동될 것입니다.).
5. 해당 소비자 그룹내 균형을 맞추기 위해 스스로 강요합니다.

##### 소비자 재균형 알고리즘

소비자 재조정 알고리즘은 소비자가 어떤 파티션을 소비하는지 합의하기 위해 그룹에 모든 소비자를 허용합니다. 소비자 재조정은 같은 그룹내에 브로커 노드와 다른 소비자 각각의 추가 또는 제거 작동됩니다. 정해진 토픽과 정해진 소비자 그룹을 위해, 브로커 파티션은 그룹내 소비자 사이에 고르게 분할됩니다. 파티션은 항상 한 소비자에 의해 소비됩니다. 이 설계는 구현을 단순화합니다. 우리는 파티션이 동시에 여러 소비자에 의해 소비되도록 가능하게하고, 파티션에 대한 충돌이 있고 잠금의 몇몇 종류는 요구됩니다. 파티션보다 더 많은 소비자가 있다면, 일부 소비자는 결국 임의의 데이터를 얻을 수 없습니다. 재조정동안에, 우리는 각 소비자가 연결해야하는 브로커 노드의 수를 줄일 수 있도록 소비자에게 파티션을 할당하려고 노력합니다.  

각 소비자는 재조정하는 동안에 다음과 같은 작업을 수행합니다:  

1. Ci가 구독하는 각 항목 T에 대해
2. 토픽 T를 만드는 모든 파티션을 Pt라고 하자
3. 토픽 T를 소비하는 Ci로 같은 그룹에 있는 모든 소비자를 Cg라고 하자
4. 분류 Pt(같은 브로커에 파티션을 같이 클러스터하도록)
5. 분류 Cg
6. Cg 에서 Ci의 인덱스 위치를 i라고 하고 N = size(Pt)/size(Cg)라고 하자.
7. 소비자 Ci에 i x N to (i+1) x N -1에서 파티션을 할당
8. 파티션 소유 레지스트리에서 Ci에 의해 소유되는 현재 입력을 제거
9. 파티션 소유 레지스트리에 새롭게 할당된 파티션을 추가 (우리는 원래 파티션 소유주가 그 소유권을  해제할때까지 이것을 다시 시도할 필요가 있을지도 모릅니다)

재조정이 한 소비자에 작동될때, 재조정은 같은 시간에 동일한 그룹내의 다른 소비자에 작동될 수 있습니다.

### 6. 작동법

여긴 링크드인에 사용과 경험을 기반한 생산 시스템으로 실제 카프카 실행에 대한 몇가지 정보입니다. 너가 알고있는 모든 추가팁을 우리에게 보내주세요.

#### 6.1 기본 카프카 작동법

이 섹션은 카프카 클러스터에서 수행하는 가장 일반적인 작동법을 확인합니다. 이 섹션에서 모든 확인 도구는 카프카 분산의 bin/directory 아래 이용가능하고 그것을 인수없이 실행할때 각 도구는 모든 가능한 명령줄 옵션의 세부사항을 프린트합니다.  

##### 토픽 추가와 제거

너는 수동으로 토픽을 추가하거나 데이터가 존재하지않는 토픽에 처음 게시될때 자동으로 생성되는것 중 하나의 옵션을 가집니다. 토픽이 자동으로 생성되면 너는 자동생성된 토픽에 사용되는 기본 [토픽 설정](http://kafka.apache.org/documentation.html#topic-config)을 조정하기를 원할지도 모릅니다.  

토픽이 추가되고 토픽 도구를 사용해서 변경:

	> bin/kafka-topic.sh --zookeeper  zk_host:port/chroot --create --topic my_topic_name
	   --partitions 20 --replication-factor 3 --config x=y

복제 요소는 얼마나 많은 서버가 기록된 각 메세지를 복제하는 지를 제어합니다. 너가 3개의 복제 요소를 가지고 있으면 너의 데이터에 대한 접근을 잃기전에 2개의 서버까지 실패할 수 있습니다. 우리는 너가 솔직하게 데이터 소비 방해없이 기계를 바운스하도록 2개나 3개의 복제요소를 사용하기를 추천합니다.  

파티션 수는 얼마나 많은 로그가 토픽에 공유되는지를 제어합니다. 파티션 수의 일부 영향이 있습니다. 먼저 각 파티션은 단일 서버에 완전히 맞아야합니다.그래서 너가 20개의 파티션을 가지면 전체 데이터 세트(읽고 쓰는 로드)는 20개를 넘지 않는(복제본은 계산하지 않는) 서버에서 처리됩니다. 마지막으로 파티션 수는 소비자의 최대 유사성에 영향을 줍니다. 이것은 [개념 섹션](http://kafka.apache.org/documentation.html#intro_consumers)에서 더 자세히 설명됩니다.  

명령줄에 추가된 설정은 서버가 시간 데이터의 길이와 같은 것을 유지하기 위해 가지는 기본 설정을 오버라이드합니다. 토픽당 설정의 완전한 세트는 [여기](http://kafka.apache.org/documentation.html#topic-config)에 있습니다.

##### 토픽 개조

너는 설정이나 같은 토픽 도구를 사용하는 토픽의 분산을 바꿀 수 있습니다.  

파티션을 추가하기 위해 너가 할 수 있다

	> bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name
	    --partitions 40

파티션에 대한 하나의 사용사례는 의미상 파티션 데이터인걸 유의하고, 파티션을 추가하는 것은 기존의 데이터의 분할을 바꾸지 않아서 그들이 그 파티션에 의존하는 경우 이것은 소비자를 방해할지도 모릅니다. 데이터가 해시(키) % 파티션수 에 의해 분할되면 이 분할은 잠재적으로 파티션을 추가함으로써 셔플되지만 카프카는 자동으로 어떤 방법으로 데이터를 재분배하려고 하지않을 것입니다.  

설정을 추가:

	> bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --config x=y

설정을 제거:

	> bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --deleteConfig x

마지막으로 토픽을 삭제:

	>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name

경고: 삭제 토픽 기느성은 0.8.1베타입니다. 너가 [메일링 리스트]() 나 [JIRA](https://issues.apache.org/jira/browse/KAFKA/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel)에 발생하는 모든 버그를 보고하세요.  

카프카는 현재 토픽의 파티션 수를 줄이거나 복제요소를 변경하는 걸 지원하지 않습니다.

##### graceful 종료

카프카 클러스터는 자동으로 모든 브로커 종료나 고장을 감지하고 해당 시스템 파티션을 위한 새로운 리더를 뽑습니다. 이것은 서버가 고장나거나 그것이 유지나 설정변경을 위해 고의로 무너뜨리는지를 발생합니다.(This will occur whether a server fails or it is brought down intentionally for maintenance or configuration changes.) 나중에 카프카는 더 메커니즘을 서버를 멈추기 위해 지원하고 나서 이것을 그냥 죽입니다. 서버가 중지될때 이것은 장점을 가지는 두개의 최적화 기능을 가집니다:

1. 이것이 재시작할때 이것은 모든 로그 복구를 수행하기 위한 필요를 방지하기 위해 디스크에 대한 모든 로그를 동기화합니다(즉 로그의 끝에 있는 모든 메세지의 체크섬을 승인). 로그 복구는 이 스피드업이 의도적인 재시작해서 시간이 걸립니다.
2. 이것은 서버가 종료하기에 앞서 다른 복제본를 위한 리더가 되는 모든 파티션을 바꿉니다. 이것은 리더쉽 전송을 더 빠르고 각 파티션이 몇 mc를 이용할수없는 시간을 최소화할 수 있습니다.

로그를 동기화하는 것은 서버가 하드 킬외 다른것을 멈출때마다 자동으로 일어나지만 제어된 리더쉽 변경은 특별한 설정을 사용하는 걸 요구합니다:

	controlled.shutdown.enable = true

브로커에 호스트되는 *모든* 파티션들이 복제본을 가지고 있는 경우에만 제어된 종료가 성공한다는 걸 주의하세요(즉 복제요소는 1보다 크고 적어도 이 복제본 중 하나는 살아있습니다.). 이것은 마지막 복제 종료가 그 토픽 파티션을 이용할수없게 만들기 때문에 너가 원하는 것이 일반적입니다.

##### 균형 리더쉽

언제든지 브로커가 종료하거나 충도할때마다 그 브로커의 파티션에 대한 리더쉽은 다른 복제본에 전송됩니다. 이것은 기본적으로 브로커가 재시작할때 이것이 유일한 모든 파티션에 대한 팔로워라는 것을 의미합니다,즉 이것은 클라이언트 읽기와 쓰기를 위해 사용되는 것은 아니라는 의미입니다.  

이 불균형을 피하기위해, 카프카는 선호하는 복제의 개념을 가집니다. 파티션의 복제 목록이 1,5,9인 경우 노드1은 이것이 이전의 복제목록이기 때문에 노드5 나 노드9에 리더로서 적합합니다. 너는 명령을 실행함으로써 복구된 복제본에 리더쉽을 복원하는 카프카 클러스터 시도를 가집니다:

	> bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot

이 명령을 실행하는 것은 번거롭기때문에 너는 또한 카프카를 다음 구성을 설정함으로써 자동으로 이것을 하도록 설정할 수 있습니다:

	auto.leader.rebalance.enable = true

##### 클러스터간 데이터 미러링

우리는 하나의 클러스터 노드간에 발생한 복제를 지닌 혼동을 피하기 위해 카프카 클러스터 "미러링" 간에 데이터를 복제하는 프로세스를 가리킵니다. 카프카는 카프카 클러스터 간에 데이터를 미러링하는 도구가 들어있습니다. 도구는 하나 이상의 소스 클러스터에서 읽고 목적 클러스터에 씁니다,이와 같이:

![mirror-maker](C:\Users\Administrator\Desktop\번역1\mirror-maker.png)

미러링의 이런 종류의 일반적인 사용사례는 다른 데이터센터에 복제본을 제공하는 것입니다. 이 상황은 다음 섹션에서 더 자세하게 나옵니다.  

너는 처리량과 내결함성을 증가시키기 위해 이런 미러링 프로세스를 수행할 수 있습니다(한 프로세스가 죽으면, 다른 것은 추가 로드 오버가 걸립니다.).  

데이터는 소스 클러스터의 토픽에서 읽어지고 목적지 클러스터에 같은 이름을 지닌 토픽에 기록됩니다. 사실 미러 메이커는 같이 훅되는 카프카 소비자와 생산자보다 약간 더 있습니다(In fact the mirror maker is little more than a Kafka consumer and producer hooked together).  

소스와 목적지 클러스터는 완전히 독립적인 존재: 그들은 다른 파티션 수를 가지고 오프셋은 같지 않습니다. 이런 이유로 미러 클러스터는 실제로 내고장성 메커니즘으로 의미하지않습니다(소비자 위치가 달라서); 따라서 우리는 일반 클러스터의 복제를 사용하는걸 권장합니다. 그러나 미러 메이커 프로세스는 순서가 기준 키마다 저장되도록 유지하고 분할을 위한 메세지 키를 사용할 수 있습니다.  

여기에서 2개의 입력 클러스터에서 하나의 토픽(*my-topic*라고 불리는)을 미러하는 방법을 보여줍니다:

	> bin/kafka-run-class.sh kafka.tools.MirrorMaker
	  --consumer.config consumer-1.properties --consumer.config consumer-2.properties
      --producer.config producer.properties --whitelist my-topic

우리가 --whitelist 옵션을 지닌 토픽의 목록을 지정할 수 있습니다. 이 옵션은 [자바 스타일 정규 표현](http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html)을 사용하는 모든 정규 표현을 가능하게 합니다. 그래서 너는 --whitelist 'A|B'를 사용하는 이름이 A 와 B인 두 토픽을 미러할 수 있습니다. 또는 너는 --whitelist '*'을 사용하는 모든 토픽을 미러할 수 있습니다. 쉘이 파일 경로로 그것을 확장하지 않는 걸 보장하기 위해 어떤 정규 표현을 인용해서 확실하게 하세요. 편의를 위해서 우리는 토픽의 목록을 지정하기 위해 '|' 대신에 ','을 사용할 수 있습니다.  

가끔 이것은 너가 원하지 않는 것이 무엇인지에 대해 말하기 쉽습니다. 너가 미러를 하기 위해 원하는 것이 무엇인지 알려주기 위해 --whitelist를 사용하는 것 대신에 너는 배제하기 위한 것을 알려주기 위해 --blacklist을 사용할 수 있습니다. 이것은 또한 정규 표현 인수를 취합니다.  

설정 auto.create.topics.enable = true을 지닌 미러링을 조합하는 것은 심지어 새 토픽이 추가되더라도 자동으로 생성하고 소스 클러스터에 모든 데이터를 복제할 수 있는 복제 클러스터를 가지는 걸 가능하게 합니다.

##### 소비자 위치 확인

가끔 이것은 소비자의 위치를 보는데 유용합니다. 우리는 소비자 그룹에 있는 모든 소비자의 위치 뿐만 아니라 그들이 로그의 끝 뒤에 얼마나 멀리 있는지를 보여주는 도구를 가집니다. *my-group*라는 소비자 그룹에 이 도구를 실행하기 위해서 *my-topic*이라는 토픽을 소비하는 것은 다음과 같습니다:

	> bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zkconnect localhost: 2181 --group test
	Group     Topic  Pid Offset logSize Lag Owner
    my-group my-topic  0   0     0       0  test_jkreps-mn-1394154511599-60744496-0
	my-group my-topic  1   0     0       0  test_jkreps-mn-1394154521217-1a0be913-0

##### 클러스터 확장

카프카 클러스터에 서버를 추가하는 것은 간단하고, 그냥 그들에게 고유 브로커 id를 할당하고 너의 새 서버에 카프카를 시작합니다. 그러나 이 새 서버는 자동으로 어떤 데이터 파티션을 할당하지 않습니다,그래서 파티션들이 그들을 움직이지 않는 한 새 토픽이 생성될때까지 그들은 어떤 작업도 수행하지 않을 것입니다. 그래서 보통 너가 클러스터에 기계를 추가할때 너는 이 기계에 기존의 일부 데이터를 이동하길 원할 것입니다.  

데이터를 이동하는 프로세스는 수동으로 시작되지만 완전히 자동화되어있습니다. 카프카가 파티션의 팔로워로 새 서버를 추가하는 것이 발생하는 커버 아래 이것은 이동하고 이것을 그 파티션의 기존 데이터를 완전히 복제 할 수 있습니다. 새 서버가 완전히 이 파티션의 내용을 복제하고 동기화 복제에 합류할때 기존 복제들 중 하나는 그들의 파티션의 데이터를 삭제할 것입니다.  

파티션 재할당 도구는 브로커에 파티션을 이동하는데 사용됩니다. 이상적인 파티션 분배는 심지어 모든 브로커에 데이터 로드와 파티션 크기를 보장합니다. 0.8.1 버전에서, 파티션 재할당 도구는 자동으로 카프카 클러스터에 데이터 분배를 연구하는 능력을 가지지 않고 파티션이 로드 분배를 실현하기 위해서 주위에 이동합니다. 따라서, 관리자는 토픽이나 파티션이 주위에 이동해야 알 수 있습니다.  

파티션 재할당 도구는 3개의 상호 배타적인 모드에서 실행할 수 있습니다-

* --generate: 이 모드에서,토픽의 목록과 브로커의 목록을 고려해볼때, 도구는 새 브로커에 지정된 토픽의 모든 파티션을 이동하는 후보 재할당을 만듭니다. 토픽의 목록과 타겟 브로커를 고려해볼때 이 옵션은 단지 파티션 재할당 계획을 만드는 편리한 방법을 제공합니다.
* --execute: 이 모드에서, 도구는 재할당 계획을 제공하는 사용자에 기반한 파티션의 재할당을 시작합니다(--reassignment-json-file 옵션을 사용). 이것은 관리자에 의한 주문 재할당 계획 핸드 또는 --generate 옵션을 사용함으로써 제공할 수 있습니다.
* --verify: 이 모드에서, 도구는 마지막 --excute 동안 목록에 작성된 모든 파티션에 대한 재할당의 상태를 확인합니다. 그 상태는 성공적으로 완료,실패 하거나 진행할 수 있습니다.

**자동으로 새 기계에 데이터를 이동**

파티션 재할당 도구는 새로 추가된 브로커에 현재 브로커의 세트의 몇가지 토픽을 이동하는 데 사용됩니다. 이것은 보통 브로커의 새로운 세트에 전체 토픽을 움직이는 것이 한번에 한 파티션을 움직이는 것보다 더 쉽기 때문에 기존 클러스터를 확장할 때 유용합니다. 이것을 사용할때,유저는 브로커의 새로운 세트로 이동하고 새로운 브로커의 타겟 목록인 토픽의 목록을 제공해야 합니다. 그 다음에 도구는 고르게 브로커의 새로운 세트에 토픽의 특정 목록에 대한 모든 파티션을 분배합니다. 이것을 이동하는 동안에, 토픽의 복제요소는 일정하게 유지됩니다. 실질적으로 토픽의 입력 목록의 모든 파티션의 복제는 브로커의 이전 세트에서 새로 추가된 브로커에 이동됩니다.  

예를 들어, 다음 예제는 브로커 5,6의 새로운 세트의 토픽 foo1,foo2에 대한 모든 파티션을 이동합니다. 이 이동의 끝에, 토픽 foo1과 foo2에 대한 모든 파티션은 브로커 5,6 에*만* 존재합니다.  

도구가 json file 같은 토픽의 입력 목록을 받기때문에, 너는 먼저 너가 이동을 원하고 다음과 같은 json file를 만드는 토픽을 확인할 필요가 있습니다-

	> cat topics-to-move.json
	{"topics" : [{"topic": "foo1"},
                 {"topic": "foo2"}],
	"version":1
    }

json 파일이 준비되면, 후보 할당을 생성하는 파티션 재할당 도구를 사용합니다-

	> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate
	Current partition replica assignment

	{"version": 1,
     "partitions": [{"topic":"foo1","partition":2,"replicas":[1,2]},
                    {"topic":"foo1","partition":0,"replicas":[3,4]},
                    {"topic":"foo2","partition":2,"replicas":[1,2]},
                    {"topic":"foo2","partition":0,"replicas":[3,4]},
                    {"topic":"foo1","partition":1,"replicas":[2,3]},
                    {"topic":"foo2","partition":1,"replicas":[2,3]}]
	}

제안된 파티션 재할당 설정

	{"version":1,
     "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                   {"topic":"foo1","partition":0,"replicas":[5,6]},
                   {"topic":"foo2","partition":2,"replicas":[5,6]},
                   {"topic":"foo2","partition":0,"replicas":[5,6]},
                   {"topic":"foo1","partition":1,"replicas":[5,6]},
                   {"topic":"foo2","partition":1,"replicas":[5,6]}]
	}

이 도구는 브로커 5,6에 토픽 foo1,foo2에서 모든 파티션을 이동하는 후보 할당을 생성합니다. 그러나,파티션 이동이 시작하지 않은 이 시점에서, 이것은 그저 너한테 현재 할당과 제안된 새로운 할당을 알려줍니다. 너가 이것을 롤백하기를 원하는 경우에 현재 할당은 저장해야합니다. 새로운 할당은 다음과 같은 --execute 옵션을 지닌 도구를 입력하기 위해 json file(예를 들어 클러스터 reassignment.json 확장)에 저장되어야 합니다-

	> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
	Current partition replica assignment

	{"version":1,
     "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                   {"topic":"foo1","partition":0,"replicas":[3,4]},
                   {"topic":"foo2","partition":2,"replicas":[1,2]},
                   {"topic":"foo2","partition":0,"replicas":[3,4]},
                   {"topic":"foo1","partition":1,"replicas":[2,3]},
                   {"topic":"foo2","partition":1,"replicas":[2,3]}]
	}

롤백하는동안 --reassignment-json-file 옵션으로 사용할 이것을 저장  
성공적으로 파티션의 재할당을 시작했습니다.

	{"version":1,
     "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                   {"topic":"foo1","partition":0,"replicas":[5,6]},
                   {"topic":"foo2","partition":2,"replicas":[5,6]},
                   {"topic":"foo2","partition":0,"replicas":[5,6]},
                   {"topic":"foo1","partition":1,"replicas":[5,6]},
                   {"topic":"foo2","partition":1,"replicas":[5,6]}]
	}

마지막으로, --verify 옵션은 파티션 재할당의 상태를 확인하는 도구와 함께 사용됩니다. 같은 확장 클러스터 -reassignment.json(--execute 옵션과 같이 사용되는)은 --verify 옵션과 함께 사용됩니다.

	> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
	Status of partition reassignment:
    Reassignment of partition [foo1,0] completed successfully
    Reassignment of partition [foo1,1] is in progress
    Reassignment of partition [foo1,2] is in progress
    Reassignment of partition [foo2,0] completed successfully
    Reassignment of partition [foo2,1] completed successfully
    Reassignment of partition [foo2,2] completed successfully

**커스텀 파티션 할당과 이동**

파티션 재할당 도구는 또한 브로커의 특정세트에 파티션의 복제를 선택적으로 이동하는데 사용됩니다. 이 방식을 사용할때, 이것은 사용자가 재할당 계획을 알고 효과적으로 --generate 단계를 건너뛰고 바로 --execute 단계로 이동하는 후보 재할당을 만드는 도구를 요구하지 않는다고 추정합니다.  

예를 들어, 다음 예제에서는 브로커 5,6에 토픽 foo1의 파티션 0과 브로커 2,3에 토픽 foo2의 파티션 1을 이동합니다.  

첫번째 단계는 json file 에 커스텀 재할당 계획을 건네는 것입니다-

	> cat custom-reassignment.json
	{"version":1,"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},{"topic":"foo2","partition":1,"replicas":[2,3]}]}

그후, 재할당 프로세스를 시작하는 --execute 옵션을 지닌 json file을 사용-

	> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
	Current partition replica assignment

	{"version":1,
     "partitions": [{"topic":"foo1","partition":0,"replicas":[1,2]},
                    {"topic":"foo2","partition":1,"replicas":[3,4]}]
    }

롤백하는 동안에 --reassignment-json-file 옵션으로 사용할 이것을 저장  
성공적으로 파티션의 재할당을 시작했습니다.

	{"version":1,
     "partitions": [{"topic":"foo1","partition":0,"replicas":[5,6]},
                    {"topic":"foo2","partition":1,"replicas":[2,3]}]
    }

--verify 옵션은 파티션 재할당의 상태를 확인하는 도구와 함께 사용됩니다. 같은 확장 클러스터 -reassignment.json(--execute 옵션과 사용되는)은 --verify 옵션과 함께 사용됩니다.

	bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
    Status of partition reassignment:
    Reassignment of partition [foo1,0] completed successfully
    Reassignment of partition [foo2,1] completed successfully

##### 해체 브로커

파티션 재할당 도구는 자동으로 아직 해체 브로커를 위한 재할당 계획을 생성하는 기능을 가지고 있지않습니다. 따라서, 관리자는 브로커의 나머지로 해체되는 브로커에 호스트된 모든 파티션 복제를 이동하는 재할당 계획을 제시해야할 필요가 있습니다. 이것은 모든 복제본이 하나만 다른 브로커로 해체된 브로커에서 이동하지 않는 걸 보장할 필요가 있는 재할당으로 비교적 번거로울 수 있습니다. 이 처리를 수월하게 하기 위해서, 우리는 0.8.2에 브로커를 해체하기 위한 도구지원을 추가할 계획입니다.

##### 복제 요소 증가

기존 파티션의 복제 요소를 증가시키는 것은 간단합니다. 커스텀 재할당 json file에 추가 복제본을 지정하고 이것을 지정된 파티션의 복제 요소를 증가시키기 위해 --execute 옵션과 같이 사용합니다.  

예를 들어, 다음 예는 토픽 foo 1에서 3까지의 파티션 0의 복제요소를 증가시킵니다. 복제요소를 증가시키기전에, 파티션의 유일한 복제는 브로커 5에 존재합니다. 복제요소를 증가시키는 일부분으로, 우리는 더 복제본을 브로커 6과 7에 추가합니다.  

첫번째 단계는 json file에 커스텀 재할당 계획을 건네는 것입니다-

	> cat increase-replication-factor.json
	{"version":1,
     "partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}

그 후, 재할당 프로세스를 시작하는 --execute 옵션을 지닌 json file을 사용-

	> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
	Current partition replica assignment

	{"version":1,
     "partitions": [{"topic":"foo","partition":0,"replicas":[5]}]}

롤백하는 동안에 --reassignment-json-file 옵션으로 사용할 이것을 저장
성공적으로 파티션의 재할당을 시작했습니다

	{"version":1,
     "partitions": [{"topic":"foo","partition":0,"replicas":[5,6,7]}]}

--verify 옵션은 파티션 재할당의 상태를 확인하는 도구와 함께 사용됩니다. 같은 증가 -replication-factor.json(--execute 옵션과 사용되는)은 --verify 옵션과 함께 사용됩니다.

	bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify
    Status of partition reassignment:
    Reassignment of partition [foo,0] completed successfully

너는 또한 카프카 토픽도구를 지닌 복제요소의 증가를 확인할 수 있습니다-

	> bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe
	Topic:foo     PartitionCount:1      ReplicationFactor:3         Configs:
        Topic:foo     Partition:0       Leader:5     Replicas:5,6,7  Isr: 5,6,7

#### 6.2 데이터센터

일부 배치는 여러 데이터센터를 걸친 데이터 파이프라인을 관리할 필요가 있습니다. 이것을 위한 우리가 권장하는 방법은 각 데이터센터가 그들의 로컬 클러스터하고만 교류하고 클러스터간에 미러링을 하는 경우에 어플리케이션을 지닌 각 데이터센터에 로컬 카프카 클러스터를 배치하는 것입니다(이것을 실행하는 방법은 [미러 메이커 도구](http://kafka.apache.org/documentation.html#basic_ops_mirror_maker) 설명서를 참조하세요).  

이 배치 패턴은 데이터센터가 독립적인 존재로 수행하고 우리가 데이터간의 복제를 중심으로 조절하고 관리할 수 있습니다. 배치 패턴은 각 시설이 혼자 서고 데이터센터간의 링크를 사용할 수 없을지라도 운영할 수 있습니다: 배치 패턴이 발생할때 이것을 따라잡는 그시점에서 링크가 재복구될때까지 미러링은 뒤떨어집니다.  

모든 데이터의 글로벌 뷰가 필요한 어플리케이션을 위해서 너는 *모든* 데이터센터에 로컬 클러스터에서 미러된 데이터 집계를 가지는 클러스터를 제공하기 위해 미러링을 사용할 수 있습니다. 이런 집계 클러스터는 전체 데이터 세트를 요구하는 어플리케이션에서 읽기 위해 사용됩니다.  

이것은 유일하게 가능한 배치 패턴은 아닙니다. 이것은 읽거나 WAN을 통해 먼 카프카 클러스터에 쓰는 걸 가능하게 합니다,그러나 분명히 이것은 클러스터를 얻기 위해 필요한 어떤 지연이 추가될 것입니다.  

카프카는 자연적으로 생산자와 소비자 모두에 데이터를 배치해서 이것은 높은 지연 연결을 통해 높은 처리량을 얻을 수 있습니다. 이것을 허용하는 것은 socket.send.buffer.bytes 와 socket.receive.buffer.bytes 설정을 사용하는 생산자,소비자,그리고 브로커의 TCP 소켓 버퍼 크기를 증가시키기 위해 필요할지도 모릅니다. 이것을 설정하는 적절한 방법은 [여기](http://en.wikipedia.org/wiki/Bandwidth-delay_product)에 설명되있습니다.  

이것은 일반적으로 높은 지연 링크를 통해 여러 데이터센터를 걸친 단일 카프카 클러스터를 실행하는데 바람직하지 않습니다. 이것은 카프카 쓰기와 주키퍼 쓰기를 위해 매우 높은 복제 지연을 초래하고, 위치사이의 네트워크가 사용할 수 없을때 카프카와 주키퍼 모두 모든 위치에 사용할 수 없는 상태입니다.

#### 6.3 카프카 설정

##### 중요한 클라이언트 설정

가장 중요한 생산자 구성 제어

* 압축
* 동기 vs 비동기 생산
* 배치 크기 (비동기 프로듀서에 대한)

가장 중요한 소비자 설정은 페치크기입니다.  

모든 설정은 [설정](http://kafka.apache.org/documentation.html#configuration)섹션에 설명되어 있습니다.

##### 프로듀서 서버 설정

여기에 우리의 서버 생산 서버 설정이 있습니다:

	# 복제 설정
    num.replica.fetchers=4
	replica.fetch.max.bytes=1048576
    replica.fetch.wait.max.ms=500
	replica.high.watermark.checkpoint.interval.ms=5000
    replica.socket.timeout.ms=30000
	replica.socket.receive.buffer.bytes=65536
    replica.lag.time.max.mx=10000
	replica.lag.max.messages=4000

	controller.socket.timeout.ms=30000
    controller.message.queue.size=10

	# 로그 설정
    num.partitions=8
	message.max.bytes=1000000
    auto.create.topics.enable=true
	log.index.interval.bytes=4096
    log.index.size.max.bytes=10485760
	log.retention.hours=168
    log.flush.interval.ms=10000
	log.flush.interval.messages=20000
    log.flush.scheduler.interval.ms=2000
	log.roll.hours=168
    log.retention.check.interval.ms=300000
	log.segment.bytes=1073741824

	# ZK 설정
    zookeeper.connection.timeout.ms=6000
	zookeeper.sync.time.ms=2000

	# 소켓 서버 설정
    num.io.threads=8
	num.network.threads=8
    socket.request.max.bytes=104857600
	socket.receive.buffer.bytes=1048576
    socket.send.buffer.bytes=1048576
	queued.max.requests=16
    fetch.purgatory.purge.interval.requests=100
	producer.purgatory.purge.interval.requests=100

우리의 클라이언트 설정은 다양한 사용사례에 상당량을 변화시킵니다.

##### 자바 버전

우리는 현재 JDK 1.7 u51을 실행하고,우리는 G1 콜렉터로 전환했습니다. 이 작업(그리고 우리는 이것을 매우 추천합니다)을 수행한다면, u51에 있는지 확인하세요. 테스트에 u21을 사용했지만, 우리는 그 버전에 GC 구현을 가진 다수의 문제를 가집니다. 우리의 조정은 다음과 같습니다:

	-Xms4g -Xmx4g  -XX:PermSize=48m -XX:MaxPermSize=48m -XX:+UseG1GC
    -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35

참고로, 링크드인의 가장 바쁜 클러스터(절정) 중 하나의 통계가 있습니다: -15 브로커-15.5k 파티션(2개의 복제요소) -400k 메세지/초 내 -70MB/sec inbound, 400MB/sec+ 아웃바운드 조정은 매우 적극적으로 보이지만, 그 클러스터의 모든 브로커는 약 21ms의 90퍼센트 GC 일시정지 시간을 가지고, 그들은 초당 1보다 작은 GC를 하고 있습니다.  

#### 6.4 하드웨어와 OS

우리는 24GB의 메모리를 가진 듀얼 쿼드 코어 인텔 엑슨 기계를 사용하고 있습니다.  

너는 활발한 리더와 라이터를 버퍼링하기위해 충분한 메모리가 필요합니다. 너는 30초동안 버퍼링하고 쓰기_처리량 x 30으로 필요한 너의 메모리를 계산하도록 가정함으로써 필요한 메모리의 견적을 쉽게 산출할 수 있습니다.  

디스크 처리량은 중요합니다. 우리는 8x7200 rpm SATA 드라이브를 가지고 있습니다. 일반적으로 디스크 처리량은 성능 버틀넥이고, 더많은 디스크는 더 우수합니다. 플러시 동작을 설정하는 법에 따라 너는 더 비싼 디스크에서 이익을 보거나 손실을 볼 수 있습니다(너가 종종 플러시를 강요하는 경우에 더 높은 RPM SAS 드라이브가 더 좋을수 있습니다).  

##### OS

카프카는 모든 유닉스 시스템에 잘 작동되야하고 리눅스와 솔라리스에 테스트되고 있습니다.  

우리는 윈도우에서 실행되는 몇가지 문제를 봤고 우리가 그변경에 만족해야 하는데 윈도우는 현재 잘 지원되는 플랫폼이 아닙니다.  

성능을 도와주는 몇가지를 가지고 있지만 우리는 OS수준의 세부조정을 할 필요가 없습니다.  

중요한 두개의 설정:

* 우리가 많은 토픽과 많은 연결을 가지고 있기때문에 우리는 파일 디스크립터의 수를 올렸습니다.
* 우리는 데이터센터 간 고 성능 데이터 전송을 가능하게 하기위해 최대 소켓 버퍼 크기를 올렸습니다.[여기에 설명되있습니다.](http://www.psc.edu/index.php/networking/641-tcp-tune)  

##### 디스크와 파일시스템

우리는 좋은 처리량을 얻기 위해 여러 드라이브를 사용하고 어플리케이션 로그나 좋은 지연을 보장하는 다른 OS 파일시스템 활성을 가진 카프카 데이터를 사용하는 같은 드라이브를 공유하지 않는걸 추천합니다. 0.8에서 너는 드라이브들을 같이 단일 용량으로 하는RAID 나 포맷을 할 수 있고 그 소유 디렉토리로 각 드라이브를 시작합니다.(As of 0.8 you can either RAID these drives together into a single volume or format and mount each drive as its own directory.) 카프카가 복제를 가지기 때문에 RAID에서 제공하는 중복은 또한 어플리케이션 수준에세 제공될 수 있습니다. 이 선택은 몇가지 단점이 있습니다.  

너가 여러 데이터 디렉토리를 설정하면 파티션은 데이터 디렉토리에 라운드-로빈으로 할당됩니다. 각 파티션은 전부 데이터 디렉토리 중 하나에 있습니다. 데이터가 파티션간에 잘 균형되어 있지 않으면 이것은 디스크간에 로드 불균형을 초래합니다.  

RAID는 낮은 수준에 로드를 균형하기 때문에 잠재적으로 디스크간에 로드를 균형하는 것에 더 좋을수 있습니다(항상 그렇게 보이진 않음). 일반적으로 RAID의 주요 단점은 쓰기 처리량을 위한 큰 성능 히트라는 것이고 사용가능한 디스크 공간을 줄인다는 것입니다.  

RAID의 또다른 잠재적인 장점은 디스크 실패를 견디는 능력입니다. 그러나 우리의 경험에서 RAID 배열을 재구성하는 것은 효과적으로 서버를 비활성화하는 I/O에 너무 집중해서,이것은 실제 가용성 향상을 제공하지 않습니다.  

##### 어플리케이션 vs OS 플러시 관리

카프카는 항상 바로 모든 데이터를 파일시스템에 작성하고 and 플러시를 사용해서 OS 캐시와 디스크에 데이터를 강요할때 플러시정책을 설정하는 기능을 지원합니다. 이 플러시정책은 일정기간 후나 일정수의 메세지가 작성된 후에 데이터를 디스크에 강제로 제어할 수 있습니다. 이 설정에 몇가지 선택사항이 있습니다.  

카프카는 결국 데이터가 플러시된걸 알기위해서 fsync를 호출해야합니다. fsync로 알려져 있지않은 모든 로그 세그먼트에 대한 충돌에서 회복될때 카프카는 그것의 CRC를 체크함으로써 각 데이터의 완전성을 검사하고 또한 스타트업에 실행되는 복구 프로세스의 부분으로 동반한 오프셋 인덱스 파일을 재구축합니다.  

실패한 노드가 항상 그 복제본을 복구할때,카프카의 내구성은 동기화 데이터를 디스크에 요구하지 않는걸 주의.  

우리는 어플리케이션 fsync를 완전히 비활성화하는 기본 플러시 설정을 사용하는걸 추천합니다. 이것은 OS와 카프카의 고유 백그라운드 플러시에 의해 만들어진 백그라운드 플러시에 의지하는 걸 의미합니다. 이것은 대부분의 사용에 대한 전세계의 최고를 제공합니다: 조율하는 놉이 없음,훌륭한 처리량과 지연,그리고 모든 복구 보장. 우리는 일반적으로 복제가 제공하는 보장이 로컬 디스크에 동기화보다 더 강하다고 생각합니다,그러나 파라노이드는 여전히 어플리케이션 수준과 지원되지않은 fsync 정책 둘다를 가지는 걸 선호합니다.  

어플리케이션 수준 플러시 설정 사용의 단점은 디스크 사용 패턴에 덜 효율적(이것은 재주문 작성에 덜 자유로운 OS를 제공)이고 대부분의 리눅스 파일시스템 블록의 fsync가 파일에 기록하는 반면에 백그라운드 플러싱은 페이지-수준 잠금을 더 세분화하도록 지연을 도입할수 있습니다.  

일반적으로 너는 파일시스템의 저수준 조율을 할 필요가 없지만, 다음의 몇 섹션에서 우리는 그것이 유용한 이 몇가지 사용을 검토합니다.

##### 리눅스 OS 플러시 행동 이해

리눅스에서, 파일시스템에 기록된 데이터는 [페이지캐시](http://en.wikipedia.org/wiki/Page_cache)에 이것이 디스크에 작성되야 할때까지 보관됩니다(어플리케이션 수준의 fsync 나 OS의 고유 플러시 정책때문에). 데이터의 플러싱은 pdflush(또는 포스트 2.6.32 커널 "플러셔 스레드")라고 불리는 백그라운드 쓰레드의 세트에의해 처리됩니다.  

Pdflush는 얼마나 많은 캐시에 보관된 더티 데이터와 디스크에 다시 기록되기 전 긴시간을 제어하는 설정가능한 정책을 가집니다. 이 정책은 [여기](http://www.westnet.com/~gsmith/content/linux-pdflush.htm)에 설명되어 있습니다. Pdflush가 기록되고 있는 데이터의 속도를 따라잡을수 없을때, 이것은 결국 데이터의 축적을 늦추는 쓰기에서 초래된 지연을 막기위해 쓰기 프로세스를 야기합니다.  

너는 행해진 OS 메모리 사용의 현재 상태를 볼 수 있습니다

	> cat /proc/meminfo

이 값의 의미는 위의 링크에 설명되어 있습니다.  

페이지캐시를 사용하는것은 디스크에 작성되는 데이터를 저장하기 위한 처리중인 캐시를 통한 몇가지 장점을 가집니다:

* I/O 표는 연이은 작은 쓰기를 처리량을 향상시키는 더 큰 물리적 쓰기로 같이 배치할 수 있습니다.
* I/O 표는 처리량을 향상시키는 디스크 헤드의 이동을 최소화하는 쓰기를 재정비하는 걸 시도할 수 있습니다.
* 이것은 자동으로 기계에 모든 자유로운 메모리를 사용합니다.

##### EXt4 노트

EXt4는 카프카를 위한 최고의 파일시스템 이거나 아닐수도 있습니다. XFS와 같은 파일시스템은 아마 fsync가 더 좋은동안에 잠금을 처리합니다. 그러나 우리는 오직 EXt4만 시도합니다.  

그것은 이런 설정을 조율할 필요가 없지만, 도움이 되는 몇가지 놉을 가지는 성능을 활용하는 걸 원합니다:

* data = writeback : EXt4는 몇가지 쓰기에 강한 순서를 놓게 명령된 디스크에 디폴트합니다. 카프카는 모든 플러시되지않은 로그에 매우 파라노이드한 데이터 복구를 하도록 이 명령을 요구하지 않습니다. 이 설정은 순서 제약을 제거하고 크게 지연을 줄일 것 같습니다.
* 저널링 비활성화 : 저널링은 트레이드오프입니다 : 이것은 서버 충돌후 재부팅을 빠르게 하지만 성능을 쓰는 변화를 더하는 다량의 추가 잠금을 도입합니다. 재부팅 시간을 신경쓰지 않고 쓰기 지연 스파이크의 주요 원인을 줄이길 원하는 사람들은 완전히 저널링을 신경끌수있습니다.
* commit=num_secs : 이것은 ext4가 메타데이터 저널에 커밋하는 빈도를 조율합니다. 이것을 더 낮은 값으로 설정하는 것은 충돌동안 플러시되지 않은 데이터의 손실을 줄입니다. 이것을 더 높은 값으로 설정하는 것은 처리량을 향상시킵니다.
* nobh : 이 설정은 data = writeback 모드를 사용할때 추가 주문한 보장을 제어합니다. 우리가 주문한 쓰기에 의지하지 않고 처리량과 지연을 향상시킬때 이것은 카프카에게 안전해야 합니다.
* delalloc: 지연된 할당은 파일시스템이 물리적 쓰기가 발생할때까지 모든 블록을 할당하는 걸 막는 것을 의미합니다. 이것은 ext4가 더 작은 페이지 대신에 큰 크기로 할당하는 걸 허용하고 데이터가 순서대로 작성됬다는 걸 보장하는데 도움을 줍니다. 이 특징은 처리량을 위해 훌륭합니다. 이것은 약간의 지연 변화를 더하는 파일시스템에 일부 잠금을 포함하는 것처럼 보입니다.

#### 6.6 모니터링

카프카는 서버와 클라이언트에 기록하는 메트릭을 위한 야머 메트릭을 사용합니다. 이것은 너의 모니터링 시스템에 연결시키는 장착형 통계 리포터들을 사용하는 통계를 알리도록 구성할 수 있습니다.  

jconsole을 작동시키고 실행중인 카프카 클라이언트나 서버에 이것을 가리키는 이용가능한 메트릭을 보는 가장 쉬운 방법; 이것은 JMX를 지닌 모든 메트릭을 둘러봅니다.  

우리는 다음 메트릭스에 그래핑하고 알립니다:(We pay particular we do graphing and alerting on the following metrics:)

묘사 | Mbean 이름 | 정규값
----|------------|------
Message in rate| kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec| 
Byte in rate| kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec | 
Request rate | kafka.network:type=RequestMetrics,name=RequestsPerSec,request={ProducelFetchConsumerlFetchFollower} |
Byte out rate | kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec | 
Log flush rate and time | kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs | 
# of under replicaated partitions (ISR < all replicas) | kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions | 0 
Is controller active on broker | kafka.controller:type=KafkaController,name=ActiveControllerCount | 클러스터에 하나의 브로커만 1을 가져야합니다.
Leader election rate | kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs | 브로커가 실패할때 0이외의 값 
Unclean leader election rate | kafka.controller:type=ControllerStats,name=UncleeanLeaderElectionsPerSec | 0 
Partition counts | kafka.server:type=ReplicaManager,name=PartitionCount | 주로 브로커간에 
Leader replica counts | kafka.server:type=ReplicaManager,name=LeaderCount | 주로 브로커간에 
ISR shrink rate | kafka.server:type=ReplicaManager,name=IsrShrinksPerSec | 브로커가 다운되면, 파티션의 일부를 위한 ISR은 수축됩니다. 그 브로커가 다시 접속할때, ISR은 복제본이 완전히 잡힐때 확장됩니다. 그 이외에는 ISR 수축속도와 확장속도 둘다 예상되는 값은 0입니다. 
ISR expansion rate | kafka.server:type=ReplicaManager,name=IsrExpansPerSec | 위를 보세요. 
Max lag in messages btw follower and leader replicas | kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica | <replica.lag.max.messages 
Lag in messages per follower replica | kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+) | <replica.lag.max.messages 
Requests waiting in the producer purgatory | kafka.server:type=ProducerRequestPurgatory,name=PurgatorySize | ack=1을 사용될때 0이외의 값 
Requests waiting in the fetch purgatory | kafka.server:type=FetchRequestPurgatory,name=PurgatorySize | 소비자의 fetch.wait.max.ms에 의지하는 크기 
Request total time | kafka.network:type=RequestMetrics,name=TotalTimeMs,request={Produce l FetchConsumer l FetchFollower} | 큐,로컬,원격과 응답전송시간으로 분할 
Time the request waiting in the request queue | kafka.network:type=RequestMetrics,name=QueueTimeMs,request={Produce l FetchConsumer l FetchFollower} | 
Time the request being processed at the leader | kafka.network:type=RequestMetrics,name=LocalTimeMs,request={Produce l FetchConsumer l FetchFollower} | 
Time the request waits for the follower | kafka.network:type=RequestMetrics,name=RemoteTimeMs,request={Produce l FetchConsumer l FetchFollower} | ack=1 일때 생산요청을위한 0이외의 값 
Time to send the response | kafka.network:type=RequestMetrics,name=ResponseSentTimeMs,request={Produce l FetchConsumer l FetchFollower} | 
Number of messages the consumer lags behind the producer by | kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=([-.\w]+) | 
The average fraction of time the network processors are idle | kafka.network:type=SocketServer,name=NetworkProcessorAvgldlePercent | 0과1사이, 이상적으로>0.3 
The average fraction of time the request handler threads are idle | kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgldlePercent | 0과1사이, 이상적으로>0.3 

##### 새로운 프로듀서 모니터링

다음 메트릭은 새로운 프로듀서 인스턴스에 사용할 수 있습니다.

Metric/Attribute 이름 | 묘사 | Mbean 이름
---------------------|------|-----------
waiting-threads | 그들의 기록을 엔큐하는 버퍼메모리를 기다리는 걸 차단하는 유저 스레드의 수 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
buffer-total-bytes | 클라이언트가 사용하는 버퍼메모리의 최대양(이것은 현재 사용되고 있는지 여부). | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
buffer-available-bytes | 사용되지 않는 버퍼메모리의 총량(할당되지 않거나 자유 목록 중). | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
bufferpool-wait-time | appender가 공간 할당을 기다리는 시간의 일부. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
batch-size-avg | 요청에 따라 파티션마다 전송된 바이트 수의 평균. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
batch-size-max | 요청에 따라 파티션마다 전송된 바이트 수의 최대. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
compression-rate-avg | 레코드 배치의 평균 압축률. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-queue-time-avg | 레코드 계산기기장치에서 전송된 ms 레코드 배치의 평균시간. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-queue-time-max | 레코드 계산기기장치에서 전송된 ms 레코드 배치의 최대시간. | kafka.producer:type=producer-metrics,client-id=([-.\w]+)
request-latency-avg | ms의 평균 요청 지연 시간 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
request-latency-max | ms의 최대 요청 지연 시간| kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-send-rate | 초당 전송된 레코드의 평균 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
records-per-request-avg | 요청당 기록의 평균 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-retry-rate | 재시도된 레코드 전송의 평균 초당 수.(The average per-second number of retried record sends) | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-error-rate | 에러를 야기하는 레코드 전송의 평균 초당 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-size-max | 최대 레코드 크기 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
record-size-avg | 평균 레코드 크기 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
requests-in-flight | 응답을 기다리는 요청의 현재 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
metadata-age | 현재 사용되는 프로듀서 메타데이터의 초의 나이 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
connection-close-rate | 윈도우에 초당 끝낸 연결. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
connection-creation-rate | 윈도우에 초당 설립된 새 연결. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
network-io-rate | 초당 모든 연결에 네트워크 작업(읽기 또는 쓰기)의 평균 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
outgoing-byte-rate | 모든 서버에 초당 전송된 발신 바이트의 평균 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
request-rate | 초당 전송된 요청의 평균 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
request-size-avg | 윈도우에 있는 모든 요청의 평균 크기. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
request-size-max | 윈도우에서 전송한 모든 요청의 최대 크기. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
incoming-byte-rate | Bytes/second가 모든 소켓을 읽어냄 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
response-rate | 초당 받은 응답 전송.(Responses received sent per second) | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
select-rate | 초당 수행하는 새로운 I/O를 확인하는 I/O 층의 횟수 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
io-wait-time-ns-avg | I/O 스레드가 나노세컨드에서 읽기 또는 쓰기에 대한 준비가 된 소켓을 기다리는데 걸리는 시간의 평균 길이. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
io-wait-ratio | I/O 스레드가 대기하는데 소비한 시간의 일부. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
io-time-ns-avg | 나노초에서 선택 콜당 I/O를 위한 시간의 평균 길이. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
io-ratio | I/O스레드가 I/O가 하는동안 소비한 시간의 일부 | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
connection-count | 활성 연결의 현재 수. | kafka.producer:type=producer-metrics,client-id=([-.\w]+) 
outgoing-byte-rate | 노드에 대해 초당 전송된 발신 바이트의 평균 수. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
request-rate | 노드에 대해 초당 전송된 요청의 평균 수. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
request-size-avg | 노드에 대한 윈도우에 모든 요청의 평균 크기. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
request-size-max | 노드에 대한 윈도우에 전송된 모든 요청의 최대 크기. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
incoming-byte-rate | 노드에 대해 초당 받은 응답의 평균 수. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
request-latency-avg |  노드에 대한 ms의 평균 요청 지연 시간. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
request-latency-max | 노드에 대한 ms의 최대 요청 지연 시간. | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
response-rate | 노드에 대해 초당 전송된 받은 응답 | kafka.producer:type=producer-node-metrics,client-id=([-.\w]+),node-id=([0-9]+) 
record-send-rate | 토픽에 대해 초당 전송된 기록의 평균 수. | kafka.producer:type=producer-topic-metrics,client-id=([-.\w]+),topic=([-.\w]+) 
byte-rate | 토픽에 대해 초당 전송된 바이트의 평균 수. | kafka.producer:type=producer-topic-metrics,client-id=([-.\w]+),topic=([-.\w]+) 
compression-rate | 토픽에 대해 배치한 기록의 평균 압축률. | kafka.producer:type=producer-topic-metrics,client-id=([-.\w]+),topic=([-.\w]+) 
record-retry-rate | 토픽에 대해 전송된 재시도한 기록의 평균 초당 수.(The average per-second number of retried record sends for a topic.) | kafka.producer:type=producer-topic-metrics,client-id=([-.\w]+),topic=([-.\w]+) 
record-error-rate | 토픽에 대한 에러를 야기하는 기록 전송의 평균 초당 수. | kafka.producer:type=producer-topic-metrics,client-id=([-.\w]+),topic=([-.\w]+) 

우리는 모니터 GC 시간과 다른 통계와 CPU활용,I/O서비스시간,등등과 같은 다양한 서버 통계를 추천합니다. 클라이언트 입장에서,우리는 message/byte 률(토픽당과 전세계적으로)을 감시,rate/size/time 요청을 추천하고, 소비자 입장에서, 모든 파티션과 분(min) 사이에 메세지의 최대 래그는 요청 속도를 가져옵니다. 소비자 유지를 위해서, 최대 래그는 한계점보다 더 적어야하고 분 페치률은 0보다 커야할 필요가 있습니다.

##### 감사

우리가 하는 마지막 경고는 데이터 전송의 정확성에 있습니다. 우리는 모든 소비자에 의해 사용되고 있는지 전송된 모든 메세지를 감사하고 발생한 이것을 위한 래그를 측정합니다. 중요한 토픽을 위해서 확실한 완성도가 일정 시간에 달성되지 않으면 우리는 경고합니다. 이것의 자세한 내용은 KAFKA-260에 설명되고 있습니다.

#### 6.7 주키퍼

##### 안정된 버전

링크드인에서,우리는 주키퍼3.3*을 실행합니다. 버전3.3.3은 일시적인 노드삭제와 세션 만료에 관하여 심각한 문제를 가지고 있다고 알려져 있습니다. 생산에 이러한 문제를 실행한후,우리는 3.3.4로 업그레이드하고 올해보다 순조롭게 그것을 실행하고 있습니다.

##### 주키퍼 조작

조작상,우리는 안전한 주키퍼 설치를 위해 다음을 수행:

* physical/hardware/network 층에 불필요한 중복: 괜찮은 하드웨어 같은 랙에 그들 모두를 같이 넣지 않고(하지만 괜찮지 않은 것에 가지않음), 쓸모없는 전원과 네트워크 경로,등등을 유지합니다.
* I/O 분리: 쓰기 타입 트랙의 많은 양을 너가 할 경우, 너는 아마 분명히 어플리케이션 로그와 스냅샷보다 다른 디스크 그룹의 트랙잭션 로그를 원할 것입니다(주키퍼 서비스에 쓰기는 느려질 수 있는 디스크에 동기 쓰기를 가집니다).
* 어플리케이션 분리: 너가 정말로 같은 박스에 설치하는 걸 원하는 다른 앱의 어플리케이션 패턴을 이해하지않는한, 그것은 혼자 주키퍼를 실행하는 것이 좋은 생각일 수 있습니다(그러나 이것은 하드웨어의 기능을 가진 행동을 균형맞출수있습니다).
* 가상화 사용 주의: 이것은 너의 클러스터 레이아웃과 읽기/쓰기 패턴과 SLAs에 관하여 작동할 수 있지만,가상화 층에서 도입된 작은 오버헤드는 늘어나고 이것이 시간에 민감해서 주키퍼를 던질 수 있습니다.
* 주키퍼 설정과 모니터링: 너에게 '충분한' 힙 공간을 제공하는 이것은 자바입니다(우리는 보통 3-5G에서 그들을 실행하지만,우리가 가진 데이터 세트 크기 때문에 대부분 여기 있습니다). 불행히도 우리는 이것을 위한 좋은 공식을 가지고 있지 않습니다. 모니터링까지, JMZ와 4 문자 명령 둘다 매우 유용하고, 그들은 어떤 경우에 중복됩니다(그리고 그런 경우에 우리는 4 문자 명령을 더 선호하고,그들은 더 예측가능하거나, 아주 최소한으로 LI 모니터링 인프라로 더 나은 작업을 합니다).
* 클러스터를 더 구축하지 마세요: 특히 큰 쓰기 사용 패턴에 대규모 클러스터는 클러스터간 소통(쓰기와 다음 클러스터 멤버 업데이트에 쿼럼)의 많은 양을 의미하지만, 이것을 더 적게 구축하지마세요(그리고 클러스터가 넘쳐나는 위험).
* 3-5 노드 클러스터에서 실행을 시도: 주키퍼 쓰기는 쿼럼과 클러스터에 기계의 수를 홀수로 가지는 것을 의미하는 걸 본질적으로 사용합니다. 5 노드 클러스터는 3 노드 클러스터에 비해 더 느린 쓰기를 야기하지만,더 많은 내고장성을 허용한다는 걸 주의하세요.

전반적으로, 우리는 가능한 단순하게 그리고 로드(+ 표준 성장 용량 계획)를 다룰수 있을 만한 주키퍼 시스템을 유지합니다. 우리는 공식적인 출시에 비해 구성이나 어플리케이션 레이아웃을 가진 공상을 아무것도 할 수 없을뿐만 아니라 가능한 스스로 포함되는 그것을 유지하지 않게합니다. 이러한 이유로, 우리는 이것을 쓰는 더 좋은 방법의 바램을 위해 '엉망'될 수 있는 OS 표준 계층에 물건을 넣으려는 경향을 가지고 있기때문에, OS 패키지된 버전을 생략하려는 경향이 있습니다.

![feather-small](C:\Users\Administrator\Desktop\번역1\feather-small.png)